{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./output/train.csv')    \n",
    "test = pd.read_csv('./output/test.csv')    \n",
    "test2 = pd.read_csv('./output/test2.csv')\n",
    "\n",
    "x_train = train.drop(columns=['hash','trajectory_id','x_exit','y_exit'])\n",
    "x_test = test2.drop(columns=['hash','trajectory_id','x_exit','y_exit'])\n",
    "y_train=pd.DataFrame()\n",
    "y_train=train[['x_exit','y_exit']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def within_measure(x, y):\n",
    "    #  3750901.5068 ‚â§ ùë• ‚â§ 3770901.5068\n",
    "    #  ‚àí19268905.6133 ‚â§ ùë¶ ‚â§ ‚àí19208905.6133\n",
    "    if 3750901.5068 <= x and x <= 3770901.5068 and -19268905.6133 <= y and y <= -19208905.6133:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 814262 entries, 0 to 814261\n",
      "Data columns (total 13 columns):\n",
      "hash             814262 non-null object\n",
      "trajectory_id    814262 non-null object\n",
      "time_entry       814262 non-null int64\n",
      "time_exit        814262 non-null int64\n",
      "vmax             814262 non-null float64\n",
      "vmin             814262 non-null float64\n",
      "vmean            814262 non-null float64\n",
      "x_entry          814262 non-null float64\n",
      "y_entry          814262 non-null float64\n",
      "x_exit           814262 non-null float64\n",
      "y_exit           814262 non-null float64\n",
      "x_req            814262 non-null float64\n",
      "y_req            814262 non-null float64\n",
      "dtypes: float64(9), int64(2), object(2)\n",
      "memory usage: 80.8+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hash</th>\n",
       "      <th>trajectory_id</th>\n",
       "      <th>time_entry</th>\n",
       "      <th>time_exit</th>\n",
       "      <th>vmax</th>\n",
       "      <th>vmin</th>\n",
       "      <th>vmean</th>\n",
       "      <th>x_entry</th>\n",
       "      <th>y_entry</th>\n",
       "      <th>x_exit</th>\n",
       "      <th>y_exit</th>\n",
       "      <th>x_req</th>\n",
       "      <th>y_req</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00032f51796fd5437b238e3a9823d13d_31</td>\n",
       "      <td>traj_00032f51796fd5437b238e3a9823d13d_31_5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.773118e+06</td>\n",
       "      <td>-1.914490e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.773385e+06</td>\n",
       "      <td>-1.911344e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000479418b5561ab694a2870cc04fd43_25</td>\n",
       "      <td>traj_000479418b5561ab694a2870cc04fd43_25_10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.769978e+06</td>\n",
       "      <td>-1.934136e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.771380e+06</td>\n",
       "      <td>-1.933274e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000506a39775e5bca661ac80e3f466eb_29</td>\n",
       "      <td>traj_000506a39775e5bca661ac80e3f466eb_29_5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.757468e+06</td>\n",
       "      <td>-1.923860e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.755349e+06</td>\n",
       "      <td>-1.916135e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0005401ceddaf27a9b7f0d42ef1fbe95_1</td>\n",
       "      <td>traj_0005401ceddaf27a9b7f0d42ef1fbe95_1_4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.760505e+06</td>\n",
       "      <td>-1.935500e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.751349e+06</td>\n",
       "      <td>-1.916284e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00063a4f6c12e1e4de7d876580620667_3</td>\n",
       "      <td>traj_00063a4f6c12e1e4de7d876580620667_3_4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.766319e+06</td>\n",
       "      <td>-1.917013e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.776264e+06</td>\n",
       "      <td>-1.918289e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  hash  \\\n",
       "0  00032f51796fd5437b238e3a9823d13d_31   \n",
       "1  000479418b5561ab694a2870cc04fd43_25   \n",
       "2  000506a39775e5bca661ac80e3f466eb_29   \n",
       "3   0005401ceddaf27a9b7f0d42ef1fbe95_1   \n",
       "4   00063a4f6c12e1e4de7d876580620667_3   \n",
       "\n",
       "                                 trajectory_id  time_entry  time_exit  vmax  \\\n",
       "0   traj_00032f51796fd5437b238e3a9823d13d_31_5           0          0   0.0   \n",
       "1  traj_000479418b5561ab694a2870cc04fd43_25_10           0          0   0.0   \n",
       "2   traj_000506a39775e5bca661ac80e3f466eb_29_5           0          0   0.0   \n",
       "3    traj_0005401ceddaf27a9b7f0d42ef1fbe95_1_4           0          0   0.0   \n",
       "4    traj_00063a4f6c12e1e4de7d876580620667_3_4           0          0   0.0   \n",
       "\n",
       "   vmin  vmean       x_entry       y_entry  x_exit  y_exit         x_req  \\\n",
       "0   0.0    0.0  3.773118e+06 -1.914490e+07     NaN     NaN  3.773385e+06   \n",
       "1   0.0    0.0  3.769978e+06 -1.934136e+07     NaN     NaN  3.771380e+06   \n",
       "2   0.0    0.0  3.757468e+06 -1.923860e+07     NaN     NaN  3.755349e+06   \n",
       "3   0.0    0.0  3.760505e+06 -1.935500e+07     NaN     NaN  3.751349e+06   \n",
       "4   0.0    0.0  3.766319e+06 -1.917013e+07     NaN     NaN  3.776264e+06   \n",
       "\n",
       "          y_req  \n",
       "0 -1.911344e+07  \n",
       "1 -1.933274e+07  \n",
       "2 -1.916135e+07  \n",
       "3 -1.916284e+07  \n",
       "4 -1.918289e+07  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\J.K. Paw≈Çowski\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "data=train[['x_entry','y_entry']]\n",
    "clusters=50\n",
    "kmeans = KMeans(n_clusters=clusters)\n",
    "\n",
    "kmeans.fit(data)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=train[['x_entry','y_entry']]\n",
    "\n",
    "train_c=kmeans.predict(data)\n",
    "data['c']=train_c\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(16)\n",
    "\n",
    "# Plot\n",
    "for c in range(clusters):\n",
    "    ax.scatter(data[data.c==c].x_entry.values,data[data.c==c].y_entry.values,s=.05, alpha=0.5)\n",
    "    \n",
    "# Create a Rectangle patch\n",
    "rect = patches.Rectangle((3750901.5068,-19268905.6133),3770901.5068-3750901.5068,19268905.6133-19208905.6133,linewidth=2,edgecolor='y',facecolor='none')\n",
    "\n",
    "# Add the patch to the Axes\n",
    "ax.add_patch(rect)    \n",
    "\n",
    "ax.set(xlabel='x', ylabel='y',\n",
    "       title='Entry map')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\J.K. Paw≈Çowski\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data=test2[['x_entry','y_entry']]#[:10000]\n",
    "\n",
    "\n",
    "test2_c=kmeans.predict(data)\n",
    "data['c']=test2_c\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(16)\n",
    "\n",
    "\n",
    "\n",
    "# Plot\n",
    "for c in range(clusters):\n",
    "    ax.scatter(data[data.c==c].x_entry.values,data[data.c==c].y_entry.values,s=.05, alpha=0.5)\n",
    "    \n",
    "# Create a Rectangle patch\n",
    "rect = patches.Rectangle((3750901.5068,-19268905.6133),3770901.5068-3750901.5068,19268905.6133-19208905.6133,linewidth=2,edgecolor='y',facecolor='none')\n",
    "\n",
    "# Add the patch to the Axes\n",
    "ax.add_patch(rect)    \n",
    "\n",
    "ax.set(xlabel='x', ylabel='y',\n",
    "       title='Entry map')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     c  target\n",
       "0   28       0\n",
       "1   23       0\n",
       "2   41       1\n",
       "3   30       0\n",
       "4   18       0\n",
       "5   18       0\n",
       "6   41       1\n",
       "7   31       0\n",
       "8   36       1\n",
       "9   25       0\n",
       "10  12       1\n",
       "11  41       1\n",
       "12  18       0\n",
       "13  16       0\n",
       "14  14       0\n",
       "15  22       1\n",
       "16   1       0\n",
       "17  49       0\n",
       "18  33       0\n",
       "19  14       0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "params = {}\n",
    "params['learning_rate'] = 0.01\n",
    "params['num_boost_round']=100\n",
    "params['boosting_type'] = 'gbdt'\n",
    "params['objective'] = 'regression'\n",
    "params['metric'] = 'l2'\n",
    "#params['num_leaves'] = 200\n",
    "params['max_depth'] = 10\n",
    "#params['verbose']=1\n",
    "\n",
    "pred = pd.DataFrame({'c':test2_c,'target':test2_c})\n",
    "\n",
    "for c in range(clusters):\n",
    "    xc_train=x_train[train_c==c]\n",
    "    yc_train=y_train[train_c==c]\n",
    "    xc_test=x_test[test2_c==c]\n",
    "    \n",
    "    \n",
    "    d_train = lgb.Dataset(xc_train, label=yc_train.x_exit.values)\n",
    "    clf = lgb.train(params, d_train)\n",
    "    #Prediction\n",
    "    predx=clf.predict(xc_test)\n",
    "\n",
    "    d_train = lgb.Dataset(xc_train, label=yc_train.y_exit.values)\n",
    "    clf = lgb.train(params, d_train)\n",
    "    #Prediction\n",
    "    predy=clf.predict(xc_test)\n",
    "    \n",
    "    target = [within_measure(x, y) for x,y in zip(predx,predy)]\n",
    "    \n",
    "    def add(row,c):\n",
    "        if row['c']==c:\n",
    "            return target.pop(0)\n",
    "        else:\n",
    "            return row.target\n",
    "        \n",
    "    pred['target'] = pred.apply(lambda x: add(x,c), axis=1)\n",
    "    \n",
    "city_lgbm=pred.target.values    \n",
    "pred[:20]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 13066703044461683628231020754663311230537853729712978383549527605115984874516332948267930568871616526155776.00000000\n",
      "Iteration 2, loss = 15352169760153051275174384560266064490183893981140567009090911468205402437904473962147384330835912342634496.00000000\n",
      "Iteration 3, loss = 15350733408973521968715298087512393831710264531050764096028451990772264346693992739738606400960210787106816.00000000\n",
      "Iteration 4, loss = 15349205672988074571569232062464151947549735716923306814800519407970529308256932923194151087277297924833280.00000000\n",
      "Iteration 5, loss = 15347678086345035336443523819888849590027979100808205935945272074826530539360052294878261502654207993315328.00000000\n",
      "Iteration 6, loss = 15346150651730700642337797527359101604929717126272542271305002771269164262499555397465243134962760247410688.00000000\n",
      "Iteration 7, loss = 15344623369130000497099130656808694031052097213680857057703370900916224310631118518099446819250609913331712.00000000\n",
      "Iteration 8, loss = 15343096238527816019711168652505342464495744958321826296722587884915489249339633438774029070484463815753728.00000000\n",
      "Iteration 9, loss = 15341569259909020181013651620772417427578532317971481784071290480669735099648194523956947350284204045762560.00000000\n",
      "Iteration 10, loss = 15340042433258492062954248671391548247955396478540338469863296443390489791001244201260058410279831244636160.00000000\n",
      "Iteration 11, loss = 15338515758561108710444652579658277984835586216560750252744029862353030616702775543913919238764639870255104.00000000\n",
      "Iteration 12, loss = 15336989235801741057288627117409890892091285080430587876953733829023882961635433562005187534021805830307840.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 13, loss = 15335974683679876345370003824759262205610735364881019145275094454057604987318627685282364755816152353996800.00000000\n",
      "Iteration 14, loss = 15335658852389724698473498186616533022978980862257814933705632263661681898770125479131458135246800355852288.00000000\n",
      "Iteration 15, loss = 15335353593294646720311226367040324449580532573794401996662610302171681967186178743388071154006631200587776.00000000\n",
      "Iteration 16, loss = 15335048340587628008422896865586338280314981004505629780483430453354091766878698879687918023215936513769472.00000000\n",
      "Iteration 17, loss = 15334743093956743354860338831322546232253263304724170084451208896957751019259397542742138283066971205402624.00000000\n",
      "Iteration 18, loss = 15334437853401821648601540167417701755957553086684494585220877694337606288530528964521551813276415870107648.00000000\n",
      "Iteration 19, loss = 15334132618922775297099518490970095308263248747125678069651509210234880220652770906630268790365904621797376.00000000\n",
      "Iteration 20, loss = 15333827390519465781907883056925860634863538448332769037892334160984529558075567271139905307439416989450240.00000000\n",
      "Iteration 21, loss = 15333522168191781066047935468550252971245559674506909659181700920092769313074203567078974380978112886210560.00000000\n",
      "Iteration 22, loss = 15333216951939592816253166653219837405330942634823953691010808533575809408800370468425590920769502758043648.00000000\n",
      "Iteration 23, loss = 15332911741762778810364996541769437830378382766594238045276037045258611676827106712301769126611215601106944.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 24, loss = 15332708881553051757810498543582877538306019961680142384787923468960501880386252902033955817887318094643200.00000000\n",
      "Iteration 25, loss = 15332645728304418061731345263620500880769003031503339462294908410150986511664830984329059892925789405970432.00000000\n",
      "Iteration 26, loss = 15332584688036973932575500434872552503711591769420400924096217459992395500351363639717038348682887119765504.00000000\n",
      "Iteration 27, loss = 15332523648074891681452076742886675477575188340569882755450018683485415123098364823846039793380407017209856.00000000\n",
      "Iteration 28, loss = 15332462608355805414909617561645675069163451223173033200015835776380569303908400798842509910421838204239872.00000000\n",
      "Iteration 29, loss = 15332401568879735503307886236010413962933264511011462772477605398040364404185965108519446333174242514829312.00000000\n",
      "Iteration 30, loss = 15332340529646651391107237748689598132199302063412755700809422559421040881824317437157352611587027198017536.00000000\n",
      "Iteration 31, loss = 15332279490656589744955246120432780408983955249183810911442373247375110187351546163619624485720903554957312.00000000\n",
      "Iteration 32, loss = 15332218451909515935240313664976494229710521108895890529413765140986311506380012263424165978851866467893248.00000000\n",
      "Iteration 33, loss = 15332157413405448295286227392695516010390195326952444017939141233680900564173759926002674961010271587401728.00000000\n",
      "Iteration 34, loss = 15332096375144372565841152962187241871903159037706344016739745863905122907749643670686053088839175629701120.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 35, loss = 15332055805110709961451582910813479137317028583156119591637785975916662148145127245473407683002673613766656.00000000\n",
      "Iteration 36, loss = 15332043174981288247604645160639514501501938431399743798067159932726493721940107254909359344179167310643200.00000000\n",
      "Iteration 37, loss = 15332030967389808551571628777891605143071869829049169958865466947645805099837054494927258711641829821906944.00000000\n",
      "Iteration 38, loss = 15332018759820528473608705624511849305754086629897735022217945335824549136351065782349978922528478997774336.00000000\n",
      "Iteration 39, loss = 15332006552260960983180945300791421417478639345818193486871422908046326290527598759809970722830210612330496.00000000\n",
      "Iteration 40, loss = 15331994344711112191396276810188580283582593204945028507231080662119888470788001490451133402557143215767552.00000000\n",
      "Iteration 41, loss = 15331982137170984135290676487189412172511636616656401134765312263981486313272723328654766725045982991482880.00000000\n",
      "Iteration 42, loss = 15331969929640576814864144331793917084265769580952311369474117713631119817981764274420870690296729939476480.00000000\n",
      "Iteration 43, loss = 15331957722119880044936798671571663676616550050941953954015528681387535804212877555842946481625853142761472.00000000\n",
      "Iteration 44, loss = 15331945514608922344012308189327859707803615757919583608947056490358243177932354134259190785747239168901120.00000000\n",
      "Iteration 45, loss = 15331933307107669082479075198799038614250263742456462459306008819626981124752554985187507625936882900140032.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 46, loss = 15331925193181264630884740045952978700920635388073688968627760638949624408083402501443586011773877352398848.00000000\n",
      "Iteration 47, loss = 15331922667176193092092757207158807737045233656269508872785399034171590297041542087946718307803593099444224.00000000\n",
      "Iteration 48, loss = 15331920225676369402526750317336725384550853858839441030632501880263471221443766936452812071215137885782016.00000000\n",
      "Iteration 49, loss = 15331917784179442378119091066729316761825392197154388376535397687703756737564973715167169299422875463188480.00000000\n",
      "Iteration 50, loss = 15331915342682896279439006365020040338443663089185452347027909025222911211950209763184582271594669335904256.00000000\n",
      "Iteration 51, loss = 15331912901186743328702354219125413725079796991201599250920397888438438461442171206792849567750756604313600.00000000\n",
      "Iteration 52, loss = 15331910459690979451837181960073264384842417084446506985276076945477837213759959337229371661217724901621760.00000000\n",
      "Iteration 53, loss = 15331908018195592426627631580947074707057392912651209241284584200723603652060878028206349971975337127444480.00000000\n",
      "Iteration 54, loss = 15331905576700612808613348099038138718410050616488121790971824643219497318451667595443280950074186032742400.00000000\n",
      "Iteration 55, loss = 15331903135206010042254686497055162392215064055284828862311893283921758670825587723220668145463678866554880.00000000\n",
      "Iteration 56, loss = 15331900693711798386803481116400749607592252094688457815583545784384142162165783892207609901500758912663552.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 57, loss = 15331899070929725420740145634520193173196433512539926969874241264656172612811596675129692881313055417303040.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 58, loss = 15331898565729554038468756277103856863247216966862134048327067908121079025521167435412161409248017256546304.00000000\n",
      "Iteration 59, loss = 15331898077430332003872526452766493868046335066653639845272819077696437147209447012535273874493400680497152.00000000\n",
      "Iteration 60, loss = 15331897589131615154198427580978525447376178692229086406380199399461953031729166476220727647241917586997248.00000000\n",
      "Iteration 61, loss = 15331897100832922748956044723023592248054283230342465585108303712462476549934278192481778580030908694265856.00000000\n",
      "Iteration 62, loss = 15331896612534238491857567203013004122515141405968488969709982689208002612701187326268028566166724535123968.00000000\n",
      "Iteration 63, loss = 15331896124235576642154829362349364949878572084754283920463991991252285673013039358248575949006308393353216.00000000\n",
      "Iteration 64, loss = 15331895635936920903560020525143984582579067991674562025623182291105320641746239453373022621856010801774592.00000000\n",
      "Iteration 65, loss = 15331895147638287572360951367285553168182136401754611696934702916257112608024382446691766691409481227567104.00000000\n",
      "Iteration 66, loss = 15331894659339664426341763881857553096013646858725466625588191871090157755004772211917009577646482570346496.00000000\n",
      "Iteration 67, loss = 15331894171041067761790268744748674513639106637612415223330798483094461171811003584099149387260664297291776.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 68, loss = 15331893846484776205550572251332173840879522847623636562879915001698405684823307145314071688760177075421184.00000000\n",
      "Iteration 69, loss = 15331893745444776966115087333009590396155520179792448063826851384494897908980950192728921323738515797704704.00000000\n",
      "Iteration 70, loss = 15331893647784957818441947915769587525841880076039946261424083076019477421460178102481660882162749156622336.00000000\n",
      "Iteration 71, loss = 15331893550125242559603601557319984346258348850573658083909391730292839377102323085680688370758997868806144.00000000\n",
      "Iteration 72, loss = 15331893452465525263729278864384294898229129215729208854926306718629950696604018714498416096018540397592576.00000000\n",
      "Iteration 73, loss = 15331893354805812041926908840420777987091286399641081728880009038839563288386613052078743347951495293173760.00000000\n",
      "Iteration 74, loss = 15331893257146098820124538816457261075953443583552954602833711359049175880169207389659070599884450188754944.00000000\n",
      "Iteration 75, loss = 15331893159486393746466074130438089238598354404977471682660988343003791016513599144764596905164229817925632.00000000\n",
      "Iteration 76, loss = 15331893061826682561699680440960658595906199998267505608083084329149654244436642836726223920433890896904192.00000000\n",
      "Iteration 77, loss = 15331892964166963228789381413538882879431291954044895327631605651550514927797889111162651882356727242293248.00000000\n",
      "Iteration 78, loss = 15331892866507252044022987724061452236739137547334929253053701637696378155720932803124278897626388321271808.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 79, loss = 15331892801596002695733344297116931683348249790601082147424457071536669857341370674644982316607798083846144.00000000\n",
      "Iteration 80, loss = 15331892781388002847846247313452414994403449257034844447613844348095968302172899284127952243603465828302848.00000000\n",
      "Iteration 81, loss = 15331892761856042277569181565182152449853822691289401769482720551898885222493463833088579776627042393522176.00000000\n",
      "Iteration 82, loss = 15331892742324091892471997489342321247532638172434764348693565085383055323516275153955706126334149875728384.00000000\n",
      "Iteration 83, loss = 15331892722792139470338837079016403776765765244201965876436015952930974788398637120441532712704551174537216.00000000\n",
      "Iteration 84, loss = 15331892703260193159313605672148745111335957544103650558583647818287646161702347150071258589085071023538176.00000000\n",
      "Iteration 85, loss = 15331892683728254996432279603225431519688903481517979446604854347389320079567854597226183518812415606128640.00000000\n",
      "Iteration 86, loss = 15331892664196310722443024530843859122704784190797825180220879878682242089012013981237209158529641638526976.00000000\n",
      "Iteration 87, loss = 15331892644664362374381816789490114188829288081321348810900118078102662826175274656485635271573455304130560.00000000\n",
      "Iteration 88, loss = 15331892625132411989284632713650282986508103562466711390110962611586832927198085977352761621280562786336768.00000000\n",
      "Iteration 89, loss = 15331892605600463641223424972296538052632607452990235020790200811007253664361346652601187734324376451940352.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 90, loss = 15331892592618217438230253688982589225156669038524155492307460496460563149738243064791667992126729534570496.00000000\n",
      "Iteration 91, loss = 15331892588576617876060029559146903141056846613686540162639016684959672965932638657564521930193204320141312.00000000\n",
      "Iteration 92, loss = 15331892584670222502747054274315112602633819845532393944663362060222255332172032600346567815459189739749376.00000000\n",
      "Iteration 93, loss = 15331892580763827129434078989483322064210793077378247726687707435484837698411426543128613700725175159357440.00000000\n",
      "Iteration 94, loss = 15331892576857437867229032708109790331124831537358584663117233808556171973072168549054558876001279129157632.00000000\n",
      "Iteration 95, loss = 15331892572951048605023986426736258598038869997338921599546760181627506247732910554980504051277383098957824.00000000\n",
      "Iteration 96, loss = 15331892569044657305782963810876640596507220047941097484507892888762589886253203206525149463216780885360640.00000000\n",
      "Iteration 97, loss = 15331892565138263969505964860530936326529881689165112318000631929961422888633046503688495111819472488366080.00000000\n",
      "Iteration 98, loss = 15331892561231874707300918579157404593443920149145449254430158303032757163293788509614440287095576458166272.00000000\n",
      "Iteration 99, loss = 15331892557325483408059895963297786591912270199747625139391291010167840801814081161159085699034974244569088.00000000\n",
      "Iteration 100, loss = 15331892553419088034746920678465996053489243431593478921415636385430423168053475103941131584300959664177152.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 18253223302586342332602857752260464414570957617005627118925961224947235691574176941270567326321468956999680.00000000\n",
      "Iteration 2, loss = 22714102456256658085248840929734674906573843878885624675145699245121418744134396786753863191571476523253760.00000000\n",
      "Iteration 3, loss = 22714613823998903292683435367064528532371307085933639523007875884166510850474059158532177556073699126280192.00000000\n",
      "Iteration 4, loss = 22712886507319841910836422963724182248291840811252407386257657340327872999165874869910595863807669802369024.00000000\n",
      "Iteration 5, loss = 22711158493705722318093454969269669414750440558668820184707319445637772914614165119824884591826137537052672.00000000\n",
      "Iteration 6, loss = 22709430611253758131766148603338640994973243367168425383509873658497947193339488488487124866703297030914048.00000000\n",
      "Iteration 7, loss = 22707702860260276975331881556900567783253161477839380089891903725288435186509426823573889279087647100239872.00000000\n",
      "Iteration 8, loss = 22705975240715460335384721607019635871372061687945416226195939833281170697158092007220318546055383769677824.00000000\n",
      "Iteration 9, loss = 22704247752609253402345481730374024211411955306883583744430846921143013736027470813330780837624785789779968.00000000\n",
      "Iteration 10, loss = 22702520395931666551786217607196672345716882744152086243594087237500844670353929356011236750588729779814400.00000000\n",
      "Iteration 11, loss = 22700793170672722381494842924637037427305014865518093631493485026599047684216529875655445461760329459433472.00000000\n",
      "Iteration 12, loss = 22699066076822406822611697349095023777172131167941876889505778545829495511166246233793770407891987247136768.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 13, loss = 22697883483968475916509895626809711748763077014019145673386048170401755531375734190100957895371402074128384.00000000\n",
      "Iteration 14, loss = 22697516379898406901563855905515968529209372801581383431552699013023223676377334615963851551073378383167488.00000000\n",
      "Iteration 15, loss = 22697170993647494041964832352825933435204992921198901993266770321706130621712310138978674414992346161610752.00000000\n",
      "Iteration 16, loss = 22696825620685429818300988922238456003021800599815000797267816673763976125264514596755595882103448730075136.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17, loss = 22696480252981741745443319191939136512658939332075105392120599540106445678813827980403398035842486717382656.00000000\n",
      "Iteration 18, loss = 22696134890533341676851700081021192000452780500687061751740321361377574893439029047871639657762886093111296.00000000\n",
      "Iteration 19, loss = 22695789533340201094022462906679414708163686374356615155569470814469854863173826837822124061150760289697792.00000000\n",
      "Iteration 20, loss = 22695444181402193700725074930776455992158975571637780412567640611335746147310361378614265919130325936504832.00000000\n",
      "Iteration 21, loss = 22695098834719274682168056794618417946633503086211014390430170101377734750758746873859470438294046998790144.00000000\n",
      "Iteration 22, loss = 22694753493291313668048923091095779391063210717874009795179864664675779960530224643154552765092727739121664.00000000\n",
      "Iteration 23, loss = 22694408157118286213935957806375505104099837554088834009196000309994874142939402433923915739485893956730880.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 24, loss = 22694171683784582921813069400013083260062291542994056974614677262027792273683907473212538862897939517276160.00000000\n",
      "Iteration 25, loss = 22694098274616371425122532715889102729184820838592696241586834684522421971506144014591265686067462866468864.00000000\n",
      "Iteration 26, loss = 22694029207396670519131583826741056835205099943964442468332836735944895320427815709839246662957898791911424.00000000\n",
      "Iteration 27, loss = 22693960141993561630188155608577657974126471124554989282119034748832167271555636269352775574361977196118016.00000000\n",
      "Iteration 28, loss = 22693891076801245224075820010621317872884444757251142045281783844935267036382648205765814568121913560793088.00000000\n",
      "Iteration 29, loss = 22693822011819089819641913342185293313315613934822974802619047584016497411369551660875437009858791032750080.00000000\n",
      "Iteration 30, loss = 22693752947047136157605962292991309664333746844833708583498699284800871119325333722307638166306733279936512.00000000\n",
      "Iteration 31, loss = 22693683882485363867608203518178504241481959393501732873236802287925881798845500846249420404098678468378624.00000000\n",
      "Iteration 32, loss = 22693614818133793320008400362607739729217135674608658186517293252754035811334546576513781356601688432050176.00000000\n",
      "Iteration 33, loss = 22693545753992383774087026136557290758625507500591263493972298860560320433983483825474725757081639503003648.00000000\n",
      "Iteration 34, loss = 22693476690061180044635560198721055235512219877769091927906479761942249661882198389520848398946067715981312.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 35, loss = 22693429397203710295959557045439169105801659445658118130623575738928845601462268615252771526020096176685056.00000000\n",
      "Iteration 36, loss = 22693414715835846865197406891567301702198549350068701699865370610004228362701931850502162597873706810212352.00000000\n",
      "Iteration 37, loss = 22693400902793096845466341479456090364843455381205815542410289832979009621705712877777548430861201741709312.00000000\n",
      "Iteration 38, loss = 22693387090080004431857438619657874130135027898902210618038954851266738761909412139556234569683768237359104.00000000\n",
      "Iteration 39, loss = 22693373277375459421205235263477640293535166167362377655047442138062137147438602385268727669325480268398592.00000000\n",
      "Iteration 40, loss = 22693359464679306998775529989972832453171551073846076741837833082210156431619132681936245716196667896627200.00000000\n",
      "Iteration 41, loss = 22693345651991571609000038812976485830392443530891240496030851674945804248136395282134385870337805322813440.00000000\n",
      "Iteration 42, loss = 22693331839312228807447045718655565203849582625959936300005773925034072963304997933287550971708418346188800.00000000\n",
      "Iteration 43, loss = 22693318026641315260764124727759623405565359727859063080193685819327474027653029014259136760369218267906048.00000000\n",
      "Iteration 44, loss = 22693304213978790228231749150566935066626006649025399807226714039100994718371501437423147969586081420017664.00000000\n",
      "Iteration 45, loss = 22693290401324674080209682331938362871488407483240556995788795243717141396864908746592582232726069636497408.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 46, loss = 22693280942825558463156817593310407042990306694134314170121237553223063418315023184577117927315618857484288.00000000\n",
      "Iteration 47, loss = 22693278006570609989528818501925568707509673937667292249199205449390456075463313068974472376523634549719040.00000000\n",
      "Iteration 48, loss = 22693275243978107755004168500890949255171944225047478485713489539767923841724083090309085109692446340546560.00000000\n",
      "Iteration 49, loss = 22693272481450204005361037722624074752505052712670928887927707800525564893914779251421792776487746028961792.00000000\n",
      "Iteration 50, loss = 22693269718922642477761931138019693348713813975825435936832061938573312817700966948592860683849684528136192.00000000\n",
      "Iteration 51, loss = 22693266956395423172206848747077805043798228014510999632426551953911167613082646181822288831778261838069760.00000000\n",
      "Iteration 52, loss = 22693264193868537940551885211854064763975541191214975768837603182794126735498019533584878166926653225172992.00000000\n",
      "Iteration 53, loss = 22693261431341999005012898539264990119919883962206330654875577620839694001789783130168427269315095789830144.00000000\n",
      "Iteration 54, loss = 22693258668815781921158172715477545890282995414947131672919751276812861778272544718997338978903115331272704.00000000\n",
      "Iteration 55, loss = 22693255906289911133419423754324767296413136461975311440590848141948637698631696552647210455731186050269184.00000000\n",
      "Iteration 56, loss = 22693253143764366271436888310945791653853423009509259443204931556884515401462745087305044066432246112845824.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 57, loss = 22693251252067447961328568340379219291705474624945670272000787643879106942033547322635413723493173481701376.00000000\n",
      "Iteration 58, loss = 22693250664817207081027869079187563905244407361064268407597892821278319318692387970080677615907969637023744.00000000\n",
      "Iteration 59, loss = 22693250112299342189347555438639555769831645144526553713039573411464011347764740541313126323594061215170560.00000000\n",
      "Iteration 60, loss = 22693249559794359513181581088101109284952383835479328504602797382498726329038810219885278372610056598323200.00000000\n",
      "Iteration 61, loss = 22693249007289384985159512075507007873855876163944747502039596017278443854874677315982629474972876715065344.00000000\n",
      "Iteration 62, loss = 22693248454784430827497206407773769147216252586191777014160331311420667742115037955892978210702758665781248.00000000\n",
      "Iteration 63, loss = 22693247902279476669834900740040530420576629008438806526281066605562891629355398595803326946432640616497152.00000000\n",
      "Iteration 64, loss = 22693247349774546956604311086140326915285266343223768656022525890940123150281151488289272842202996767981568.00000000\n",
      "Iteration 65, loss = 22693246797269613169301768763267950873102526859252408682827197844444853398926005672012619211299940552671232.00000000\n",
      "Iteration 66, loss = 22693246244764707900502895123200782589159425106575303430189381121057092553537150817074162267110770904924160.00000000\n",
      "Iteration 67, loss = 22693245692259818927991832159022304452781830628923486589298713725159336797271890797186103429615250724356096.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 68, loss = 22693245313920540377026547024391041432149762875923879010826998104868787930233237930327245149201475261431808.00000000\n",
      "Iteration 69, loss = 22693245196470525608356419057724525157366839336949439882028075261703140838268375471669614046406415900213248.00000000\n",
      "Iteration 70, loss = 22693245085966962407793042735148137618823591258657069990164700976234282297557002886946342651959823896150016.00000000\n",
      "Iteration 71, loss = 22693244975465994391063516547845656080087376728141879669034857093548734199778107783998969748476909540343808.00000000\n",
      "Iteration 72, loss = 22693244864965014152118132353626656930677031741357723039094651215245682285156516554763798264973758084153344.00000000\n",
      "Iteration 73, loss = 22693244754464042061316653497352002855049440392086210615028020000687632915096722743053825834817431361552384.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 74, loss = 22693244643963082192731032647993866390095979499083664499771750781747087361879625057631651984681341739335680.00000000\n",
      "Iteration 75, loss = 22693244533462101953785648453774867240685634512299507869831544903444035447258033828396480501178190283145216.00000000\n",
      "Iteration 76, loss = 22693244422961138011128074935444558238840796800540639651638488352630988621760037434211707124368688294133760.00000000\n",
      "Iteration 77, loss = 22693244312460169994398548748142076700104582270025449330508644469945440523981142331264334220885773938327552.00000000\n",
      "Iteration 78, loss = 22693244201959201977669022560839595161368367739510259009378800587259892426202247228316961317402859582521344.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 79, loss = 22693244126291349526733527669091080586755055643915395176033887328699783670619235621955269282658834383372288.00000000\n",
      "Iteration 80, loss = 22693244102801350647071454744729949868689847754876829453210890091939155524507161838986342588773234877923328.00000000\n",
      "Iteration 81, loss = 22693244080700638821773170014009106868359473502969619895425572701219884070821067063794208215218598950469632.00000000\n",
      "Iteration 82, loss = 22693244058600444403612874242754176053233955233115317410612246458308274196809108301452213729187333605949440.00000000\n",
      "Iteration 83, loss = 22693244036500241837308673133554900164325683325748370719925345551651661778235352121585020189809243527839744.00000000\n",
      "Iteration 84, loss = 22693244014400055567292282700244314422982918693406712440985593972485054448785190776768224757124802916909056.00000000\n",
      "Iteration 85, loss = 22693243992299861149131986928989383607857400423552409956172267729573444574773232014426230271093537572388864.00000000\n",
      "Iteration 86, loss = 22693243970199674879115596495678797866514635791210751677232516150406837245323070669609434838409096961458176.00000000\n",
      "Iteration 87, loss = 22693243948099488609099206062368212125171871158869093398292764571240229915872909324792639405724656350527488.00000000\n",
      "Iteration 88, loss = 22693243925999281968723052284196763699372222432745824604669076332711116225018254436162846339673153905623040.00000000\n",
      "Iteration 89, loss = 22693243903899091624634709181914005421138080981647844222792537421672007623287194382583451380315300927897600.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 90, loss = 22693243888765523578890781804947606028350244653782664717885627169083486635539131286568672689370543308144640.00000000\n",
      "Iteration 91, loss = 22693243884067519728886414551103207347845826257218629470384240389858859734035817821212287823920011040260096.00000000\n",
      "Iteration 92, loss = 22693243879647375734197976537370169733023200679334658717652461978966004934386239382668821138539718908051456.00000000\n",
      "Iteration 93, loss = 22693243875227345813524213254857963151159126026627706847150728860503185758601824789478141200014973046095872.00000000\n",
      "Iteration 94, loss = 22693243870807307744706544634401411495512297736408110770775421078295364038255612778762262208143402450550784.00000000\n",
      "Iteration 95, loss = 22693243866387281898104734020861377450539599902457481003210475291705046134752096894334181796292068955389952.00000000\n",
      "Iteration 96, loss = 22693243861967243829287065400404825794892771612237884926835167509497224414405884883618302804420498359844864.00000000\n",
      "Iteration 97, loss = 22693243857547205760469396779948274139245943322018288850459859727289402694059672872902423812548927764299776.00000000\n",
      "Iteration 98, loss = 22693243853127171765723680828463895020490491850555014877021339276954082245994359570949144347350769535549440.00000000\n",
      "Iteration 99, loss = 22693243848707133696906012208007343364843663560335418800646031494746260525648147560233265355479198940004352.00000000\n",
      "Iteration 100, loss = 22693243844287091554016390918578619172305458451359500621333936380665937533021036840754786836934215977664512.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 27263748589118369557889864298991328369243623130799455510291206932659875332263027260158203370865096508047360.00000000\n",
      "Iteration 2, loss = 38475353235119793283904307494254577687153816007293449266372180623299797961819350649848477059282199046520832.00000000\n",
      "Iteration 3, loss = 38551926249485361526425594739288297264038808866767885216017557752598789029225559747271004800404895823822848.00000000\n",
      "Iteration 4, loss = 38550343812786049494366734442529340590110758178947555274641107815206734670662559329420279894668640060964864.00000000\n",
      "Iteration 5, loss = 38548358130756601963826830906675995213652896699436701630551236092557402211978413383701499601003438640463872.00000000\n",
      "Iteration 6, loss = 38546370472449279794856183941401735811210641952254507687248767819341603462480279475923123324995323402649600.00000000\n",
      "Iteration 7, loss = 38544382905918641688810715810420789044626539973186324399159988692246496874688819987883711436148591411855360.00000000\n",
      "Iteration 8, loss = 38542395441817644320824402991981724150051057530880708730662450901303865443603884122552401053241360187916288.00000000\n",
      "Iteration 9, loss = 38540408080196023961295428298366871497412398002517893334055901946009241174436908453744213805291803561164800.00000000\n",
      "Iteration 10, loss = 38538420821048736909146387542026630415186059767771114773597624968206048983435291533360935670615411439697920.00000000\n",
      "Iteration 11, loss = 38536433664370478722694905721192357870799424805908995025590513869897632360868914552497982920429282256748544.00000000\n",
      "Iteration 12, loss = 38534446610155985700978134523817136200593643283763377095705335871812347519816645789876767092684638113497088.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 13, loss = 38533014887463451115442847169060277240623665228418072885767317476014380426679019711393138356527854658781184.00000000\n",
      "Iteration 14, loss = 38532560308741474254518689729340723780378393919952991157995822223546159935186335416332790085851495721336832.00000000\n",
      "Iteration 15, loss = 38532162636282402098240341232371513915605409714737597990765164226760696508199418222806932281759765224226816.00000000\n",
      "Iteration 16, loss = 38531765261181929919538188464910939001779667435038037027646881601441545108459790751764686046553567360188416.00000000\n",
      "Iteration 17, loss = 38531367891690876301443931384985163275257844347476481403660786284479143842354512735981536312551889784799232.00000000\n",
      "Iteration 18, loss = 38530970526305590915810411081098615940466427506203823415212005436848921820349951492749377510065220624580608.00000000\n",
      "Iteration 19, loss = 38530573165018284137064124478457406461092939449132202247163160518328446441367775867977914639529114567966720.00000000\n",
      "Iteration 20, loss = 38530175807828866335622112859673739025527090163622531634904930227722689715228214268889958114128499545473024.00000000\n",
      "Iteration 21, loss = 38529778454737313067052660210914578412420618737136878960816590573796644008245874442909910773822901356331008.00000000\n",
      "Iteration 22, loss = 38529381105743559146204523828625164031511496069574090577909544246590288963926377049836180191837722131824640.00000000\n",
      "Iteration 23, loss = 38528983760847571980502082361028115587668707610883589662689492591123614404022532419567970154785662937595904.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 24, loss = 38528697454300740984498896006060498531848341019906315290954550577242338588496875778410803030122526085742592.00000000\n",
      "Iteration 25, loss = 38528606548519941609367172755761141804130657589619048705391097032832198905626554270942280572013706051846144.00000000\n",
      "Iteration 26, loss = 38528527021918421439936247085052353714818163577653419936163761292665144850655058962861372864990890069852160.00000000\n",
      "Iteration 27, loss = 38528447554125681759367141137974613591832967012048252369025398602388325617645693815062047931407395985555456.00000000\n",
      "Iteration 28, loss = 38528368086799105539766324324822889653966126182330747034967123427452083309628382701425764023013735290372096.00000000\n",
      "Iteration 29, loss = 38528288619637985530307299809542233948694648684366486169715970258537510873469431854481457375243114863656960.00000000\n",
      "Iteration 30, loss = 38528209152640781731791958720651427531078097028265714863166327647839127386989129361966506905545660056993792.00000000\n",
      "Iteration 31, loss = 38528129685807453403500774368428745032202703026465212085950322276631920127378488136254917347187247202435072.00000000\n",
      "Iteration 32, loss = 38528050219138082026872800132317637189896003054091419896803700782365914540255482352598679233636123635875840.00000000\n",
      "Iteration 33, loss = 38527970752632569824181171956985963118764953460992607825243567200101080090878543000695403924730392554242048.00000000\n",
      "Iteration 34, loss = 38527891286290973832433227208044138335288829709757285312384944176052434591180252003221484793897827092660224.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 35, loss = 38527834026501151730881501042785686640337514669574726118627567713302713739850795982351430396971036935454720.00000000\n",
      "Iteration 36, loss = 38527815845743504537234986709932421891221982906925151348653961310503136249489546970303106613192073217572864.00000000\n",
      "Iteration 37, loss = 38527799940738787894575226565808585967947169334257292125403510602912279370383385560676500058325631622971392.00000000\n",
      "Iteration 38, loss = 38527784047469321437563766033456434622640232422229923435832473342806191996142650973633689520926196041252864.00000000\n",
      "Iteration 39, loss = 38527768154266873464173710093342515140481964051701148056413045385346033642685675531353092761160194233204736.00000000\n",
      "Iteration 40, loss = 38527752261071269931664138026478457635836751191793505610796334973688012721138530810239700812726968640339968.00000000\n",
      "Iteration 41, loss = 38527736367882217506854457666867839452525462892051804687533654213012037627276509779386347755140828853436416.00000000\n",
      "Iteration 42, loss = 38527720474699740634176385028343695811896360065013977904245727094553115994785004691368630748442249073262592.00000000\n",
      "Iteration 43, loss = 38527704581523806721054298759128646418818428160629448437438254963331237645416825876085753579243930365460480.00000000\n",
      "Iteration 44, loss = 38527688688354432063776009535111381420857174453923504698858387146836407668295568168588114354239522197209088.00000000\n",
      "Iteration 45, loss = 38527672795191608514197612018347555744229845307383502482632548981323623518859434151350514020082199834918912.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 46, loss = 38527661343294430877049868917698984839338268871175295301922780159241662288514637168666480918678719327895552.00000000\n",
      "Iteration 47, loss = 38527657707158862022602342016511462415172987678412850813086110222392731078091168814616721857683214739439616.00000000\n",
      "Iteration 48, loss = 38527654526170512465290480314466441269142047103422163566633020365150492576984025059639057399562234656456704.00000000\n",
      "Iteration 49, loss = 38527651347528209093779140900032443948669433719001808263291414624020894486836524830073628772605848597823488.00000000\n",
      "Iteration 50, loss = 38527648168898225715852672457448198187720320253699492240794700465335143774126719898609168806048459726520320.00000000\n",
      "Iteration 51, loss = 38527644990268568263682417532637755378081352288902944453240972856449494843888811668152670973364060198797312.00000000\n",
      "Iteration 52, loss = 38527641811639155255829322746157664781924993449485722841894485159913922250504825963452144741084402678759424.00000000\n",
      "Iteration 53, loss = 38527638633010035581156820125673996841947765560523692641996685358198441261345547289658784429290435567943680.00000000\n",
      "Iteration 54, loss = 38527635454381176647089288319409371263018654071966277030053274796323041698163785976671793824594859931992064.00000000\n",
      "Iteration 55, loss = 38527632275752586601770632665308133118920412621326120211937828138032726105521339442016371980344500504494080.00000000\n",
      "Iteration 56, loss = 38527629097124224704481326473648557040739273021040001158282472064602481760609220598066523629805233617502208.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 57, loss = 38527626806747225472079473898878019920804295350078977278339342759941850338517689041564233960225132859359232.00000000\n",
      "Iteration 58, loss = 38527626079520721182354087796877526954921211197472274979915393620698254429655442201639171338368522014162944.00000000\n",
      "Iteration 59, loss = 38527625443323569492844094949728869418298154428278309024184124263431968563564329205246298239602379053858816.00000000\n",
      "Iteration 60, loss = 38527624807595563484971744924136525071281284724599784651261269352177190932083125057239319690963921976426496.00000000\n",
      "Iteration 61, loss = 38527624171870010068414901619792047932873259912227166246284388228168179213702943584317256199719709709434880.00000000\n",
      "Iteration 62, loss = 38527623536144424059282436963670190499334220549803971017813208449179157317075572441294396495088198508085248.00000000\n",
      "Iteration 63, loss = 38527622900418870642725593659325713360926195737431352612836327325170145598695390968372333003843986241093632.00000000\n",
      "Iteration 64, loss = 38527622264693349818744371706758616517649185475109311031353744856141144058562399165551065725987072908460032.00000000\n",
      "Iteration 65, loss = 38527621628967820846619244416247174600589421575274625243997587723367139973867609945204599394783334842236928.00000000\n",
      "Iteration 66, loss = 38527620993242300022638022463680077757312411312952583662515005254338138433734618142383332116926421509603328.00000000\n",
      "Iteration 67, loss = 38527620357516819939376327200834706282949169238193763110400296104034149616410613427188060105803631844917248.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 68, loss = 38527619899441499944706228997735180582033159351625471551972701947803048268697921807634552894686494082465792.00000000\n",
      "Iteration 69, loss = 38527619753996226790450429926345855239717904888647121392258065976687337738435583659235217151694376007630848.00000000\n",
      "Iteration 70, loss = 38527619626756799711805993492093861761906394989813385883461241970732081583042080026966722153279877309005824.00000000\n",
      "Iteration 71, loss = 38527619499611214806519334162864083040068528324102969420623820315971131145869434032415724550245835360698368.00000000\n",
      "Iteration 72, loss = 38527619372466126938010900448239353818978633546663849516074453149655335926966430506901869201368102161350656.00000000\n",
      "Iteration 73, loss = 38527619245321014625070750719781589376540477856686796993904361992104533074378034728812416692449894761234432.00000000\n",
      "Iteration 74, loss = 38527619118175918608418411667212515081667829441735032883481420162043735310913233785773362290225336828297216.00000000\n",
      "Iteration 75, loss = 38527618991030838888053883290532130934360688301808557184805627659472942636572027677784705994694428362539008.00000000\n",
      "Iteration 76, loss = 38527618863885718426969828224130021418139778974318860456761961838177137239421834482170054432429396228833280.00000000\n",
      "Iteration 77, loss = 38527618736740622410317489171560947123267130559367096346339020008116339475957033539131000030204838295896064.00000000\n",
      "Iteration 78, loss = 38527618609595526393665150118991872828394482144415332235916078178055541712492232596091945627980280362958848.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 79, loss = 38527618517980460765102349410783098673454729439599145083055844414060320934037334788676204375087487863750656.00000000\n",
      "Iteration 80, loss = 38527618488891424060167781339982792767313736549531292304034781480076184426020821477551775143852078662680576.00000000\n",
      "Iteration 81, loss = 38527618463443556570355485796609953234073492572292362455197280939124138792978075069653514061532193336852480.00000000\n",
      "Iteration 82, loss = 38527618438014421663381562187286438327383861236622461909707932347932943107507591552187876623562370533294080.00000000\n",
      "Iteration 83, loss = 38527618412585408978566218647128099527435534463642224452322203712916785590464069297600224985794918733578240.00000000\n",
      "Iteration 84, loss = 38527618387156379997463064431081070579921700415636698583189325750410622984296952207962175241333817466683392.00000000\n",
      "Iteration 85, loss = 38527618361727375460791626228867076853756127280169105331677171779139468011815227370899722656913190400557056.00000000\n",
      "Iteration 86, loss = 38527618336298354627832377350764392980025046869676223668417868480378307950209907698786871965798913867251712.00000000\n",
      "Iteration 87, loss = 38527618310869325646729223134717364032511212821670697799284990517872145344042790609148822221337812600356864.00000000\n",
      "Iteration 88, loss = 38527618285440312961913879594559025232562886048690460341899261882855987826999268354561170583570360800641024.00000000\n",
      "Iteration 89, loss = 38527618260011308425242441392345031506397312913222867090387107911584832854517543517498717999149733734514688.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 90, loss = 38527618241688292040272319115525538645896260917254571977465631293287787681001844989005490127232445341237248.00000000\n",
      "Iteration 91, loss = 38527618235870476551141500163421132390885308701728357215787844042745957834836744909255405227638538767433728.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 92, loss = 38527618230780901423550259987157695469480709178778042404845629001806548199315836144170713200505196755550208.00000000\n",
      "Iteration 93, loss = 38527618225695066294011569927348647414360029274131418089874184619823306517659942023152386659564407461249024.00000000\n",
      "Iteration 94, loss = 38527618220609263757048501219316979654370363919535370598397038892820075014251237572234856332010917101305856.00000000\n",
      "Iteration 95, loss = 38527618215523453071941527173340966820597944927426678901046318502071840966280735703792126951110602007773184.00000000\n",
      "Iteration 96, loss = 38527618210437658683122363803253644134391033210343275615442747438813612007433828670399795676903936381419520.00000000\n",
      "Iteration 97, loss = 38527618205351847998015389757277631300618614218234583918092027048065377959463326801957066296003621287886848.00000000\n",
      "Iteration 98, loss = 38527618200266045461052321049245963540628948863638536426614881321062146456054622351039535968450130927943680.00000000\n",
      "Iteration 99, loss = 38527618195180234775945347003269950706856529871529844729264160930313912408084120482596806587549815834411008.00000000\n",
      "Iteration 100, loss = 38527618190094440387126183633182628020649618154446441443660589867055683449237213449204475313343150208057344.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 15474038978237440772079442528786218172433366036326828092237021399582428710576606675456452000313426101403648.00000000\n",
      "Iteration 2, loss = 19522342266418449690010808224329269183097389659732004882545469327433470048708585407077382537761061750177792.00000000\n",
      "Iteration 3, loss = 19524047832695182253225787066544991472375178715125875495198449356765358334327513626413355285111484150972416.00000000\n",
      "Iteration 4, loss = 19522657250596915768746643739919720800636898985657733981557690708342178580811080607368713621618625882357760.00000000\n",
      "Iteration 5, loss = 19521265021498697632370324435249063157718088552361102058554736964544343518043730999101893482188894512349184.00000000\n",
      "Iteration 6, loss = 19519872890700575717676647545482133697002377147642347380514841279649379336557323902461346251149082879852544.00000000\n",
      "Iteration 7, loss = 19518480859179691738081550032368870593871482562819926718258829281852434976221763774367947386414892505366528.00000000\n",
      "Iteration 8, loss = 19517088926929523104388808871461042285231117969022153269990182643278973515318217885899854683853124150493184.00000000\n",
      "Iteration 9, loss = 19515697093943001301760543396039297264542502824030178440382880565139287546487426533946889365093321426337792.00000000\n",
      "Iteration 10, loss = 19514305360213025222783251587606903730135842035574576810616603593663659484122940345298076438377729009647616.00000000\n",
      "Iteration 11, loss = 19512913725732546722978816124305372859929239155218110300049267589424888282269993160656234758702952345501696.00000000\n",
      "Iteration 12, loss = 19511522190494452472717876980721455241578768634422387181050191103035752538479439480522590754290999010263040.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 13, loss = 19510568411822691935202089333872083503073310930302979895007312853186900943994556509466887553883330211479552.00000000\n",
      "Iteration 14, loss = 19510271782427021692946377827089557346396279829961891876770808682458612362935135466109199691648304532160512.00000000\n",
      "Iteration 15, loss = 19509993495736903205527646517291835262878176613862345479954821569179173880292153389446691960641093074157568.00000000\n",
      "Iteration 16, loss = 19509715223357439118277142233274130956727042773639050592246154445701270911228848868845769211361495097540608.00000000\n",
      "Iteration 17, loss = 19509436954952831109432262642475527111694998082806519672287466732472164984148052452185246796837323320328192.00000000\n",
      "Iteration 18, loss = 19509158690517200293165306418051052993525293075991958182294638537472520197712927395034007727334532457693184.00000000\n",
      "Iteration 19, loss = 19508880430050461113965267511585085327499014559312601960595135891379809834024600813377461942711462806945792.00000000\n",
      "Iteration 20, loss = 19508602173552564682968713895411553670919640707692585771947510811724018625712288202064415122887165966548992.00000000\n",
      "Iteration 21, loss = 19508323921023470259456118879808732654873403333568688586983889979780133849967002473468872001127518268555264.00000000\n",
      "Iteration 22, loss = 19508045672463108584204287092249689152206896518083434655778888753715633878013465578626640623984509477453824.00000000\n",
      "Iteration 23, loss = 19507767427871430768349786505068352720223598436160958743091059151060503442480893012386526671377191191707648.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 24, loss = 19507576706394632970784913362755034036898440985857369061386533703096931999639438445523015735041157553455104.00000000\n",
      "Iteration 25, loss = 19507517389344106547422345145645940767630946547181623843433509212275836253390113091565568560681481081454592.00000000\n",
      "Iteration 26, loss = 19507461739584607940770959283073630114580098041738972770051178639275625637057537007176032280059517465526272.00000000\n",
      "Iteration 27, loss = 19507406092051805582066658168317875054203671260410648106595444977463283962756931247845597932986862621687808.00000000\n",
      "Iteration 28, loss = 19507350444678926843792424887222883127932887939750152123789636638815884402854239255596983624043253939044352.00000000\n",
      "Iteration 29, loss = 19507294797464778022866127430942101026594340184155108661155065384690554179046139362988528037918867946733568.00000000\n",
      "Iteration 30, loss = 19507239150409375415575576475364218897753535268650806130438880542577298380456226405070629281307354111934464.00000000\n",
      "Iteration 31, loss = 19507183503512719021920772020489236741410473193237244531641082112476117007084500381843287354208712434647040.00000000\n",
      "Iteration 32, loss = 19507127856774800693757808728372809483782400320401779658888095430642007514369163875781303203276118181281792.00000000\n",
      "Iteration 33, loss = 19507072210195616357014733930042764587977939831388089409243133165202468630029318178122077301836158985043968.00000000\n",
      "Iteration 34, loss = 19507016563775170085763500294471274590888468544952495885642982648030001626345861997628209176562247212728320.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 35, loss = 19506978420849955146137503426999016734135150083568101255550799868344166091172877100082163281930657441251328.00000000\n",
      "Iteration 36, loss = 19506966557792966788705010811454095110854315445046166575652755153729720870993875060594568643736884188545024.00000000\n",
      "Iteration 37, loss = 19506955428144182094725257837548550235570838033078416960890567685096837718872190489884045416165993361702912.00000000\n",
      "Iteration 38, loss = 19506944298915324219122953832382900365170200291498783349609038310787592375823114206594534997227985170333696.00000000\n",
      "Iteration 39, loss = 19506933169693045969724210217275897574343124841379345981239049910567901766425452574903260155850949352488960.00000000\n",
      "Iteration 40, loss = 19506922040477131420715535536702397407846640288635033400130873895195198459791574030392445978344030468046848.00000000\n",
      "Iteration 41, loss = 19506910911267547979521308438885019570549732083215268782790211609689472277674288902961296251319929582649344.00000000\n",
      "Iteration 42, loss = 19506899782064328238717150275601144357583414775170628952711361709030733398320786862710607188165945630654464.00000000\n",
      "Iteration 43, loss = 19506888652867451827943297701989909084490804270719503395210387533856475460326574365827381155515016778088448.00000000\n",
      "Iteration 44, loss = 19506877523676930969415608724967831361946031026130858419097651079784202280534347538599416733387380125335552.00000000\n",
      "Iteration 45, loss = 19506866394492753440918225337618393579274964585135727715562790351196410042101410254738915341762798572011520.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 46, loss = 19506858765962511609642766842611170241212165347898719812858112278648356639891524582708459352240908847284224.00000000\n",
      "Iteration 47, loss = 19506856393365244449316905382585412870607356594621914702831685202308880374924805647011138738665607187660800.00000000\n",
      "Iteration 48, loss = 19506854167447604615322582844839863117883595457281563334472130163775587762349408334592546325214490343243776.00000000\n",
      "Iteration 49, loss = 19506851941612941404788269263332372231831501929953482479660163372475707694838012390038513669153048007540736.00000000\n",
      "Iteration 50, loss = 19506849715778583749650405854737821612632669809349559345107246471613423048394019602679445513597533181444096.00000000\n",
      "Iteration 51, loss = 19506847489944478686973607922417968280699200451637606592635144146846217283365746758601548011793585096622080.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 52, loss = 19506845264110622142685922797400639699139717038061302119307069066301589127472295149042221637067791386279936.00000000\n",
      "Iteration 53, loss = 19506843038277022264931255817630180941736973206133290130996595893724541125275462191526665442766976784007168.00000000\n",
      "Iteration 54, loss = 19506840812443679053709606983106592008490968955853570627703724629115073276775247886054879428891141289803776.00000000\n",
      "Iteration 55, loss = 19506838586610580286805118286913355288727573830953177300618093276855681765128956106339065015420047803285504.00000000\n",
      "Iteration 56, loss = 19506836360777738186433647735966988393120918287701076458550063832563870407179282978667020782373933424836608.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 57, loss = 19506834835073892263476168883322017168986666659921403725636359828328447521791147801322233704116280969134080.00000000\n",
      "Iteration 58, loss = 19506834360554994534825340639121199726849502987628377544208866480469725807912387889401345019654667468013568.00000000\n",
      "Iteration 59, loss = 19506833915371957086289577475821663218026519738421488464126150230212220468017512961934609548443293061218304.00000000\n",
      "Iteration 60, loss = 19506833470205464443953603008514799025084797458638659410336788714182381860852294319384551897959540207910912.00000000\n",
      "Iteration 61, loss = 19506833025039000320121297224013142590382712910150085077104938521260052159653366638172690934189673922166784.00000000\n",
      "Iteration 62, loss = 19506832579872544344432896777455831229463381999174154949746662992082725003016236374486029023766632370012160.00000000\n",
      "Iteration 63, loss = 19506832134706088368744496330898519868544051088198224822388387462905397846379106110799367113343590817857536.00000000\n",
      "Iteration 64, loss = 19506831689539652763415859229202071192081604271003905209714048593090577051146469390925702836287611099676672.00000000\n",
      "Iteration 65, loss = 19506831244373229380303080134422140126293287910078551905850071718893260072756528797339837139251868481880064.00000000\n",
      "Iteration 66, loss = 19506830799206801923118348370670036523613594730396876499049307512823441822085689494991371915542713497288704.00000000\n",
      "Iteration 67, loss = 19506830354040398910365332620750968142282162463253133709869267297988631205100242445218503851874032713465856.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 68, loss = 19506830048899714466470452364843162664795949967828698904371703000089573091465308552011616591029479451656192.00000000\n",
      "Iteration 69, loss = 19506829953995960179986393263630468905095053509659290706294285788127336636831128563955555919512313425559552.00000000\n",
      "Iteration 70, loss = 19506829864959370616195832374448120765652514862345730143199606798314841166888107897017646742633052958097408.00000000\n",
      "Iteration 71, loss = 19506829775926093272902791359642045118899329863922039267713028620845880061315737454073152751238046694768640.00000000\n",
      "Iteration 72, loss = 19506829686892811855537797675863796935254768046742026289289663111504417683462468302366059233169628064645120.00000000\n",
      "Iteration 73, loss = 19506829597859542660388661999002066362284336685830979619676659597780459122451895276946764295121446534905856.00000000\n",
      "Iteration 74, loss = 19506829508826257168951715646251645641748398049894644538316506756566495472317727416477071250379615537987584.00000000\n",
      "Iteration 75, loss = 19506829419792987973802579969389915068777966688983597868703503242842536911307154391057776312331434008248320.00000000\n",
      "Iteration 76, loss = 19506829330759702482365633616639494348242028053047262787343350401628573261172986530588083267589603011330048.00000000\n",
      "Iteration 77, loss = 19506829241726433287216497939777763775271596692136216117730346887904614700162413505168788329541421481590784.00000000\n",
      "Iteration 78, loss = 19506829152693147795779551587027343054735658056199881036370194046690651050028245644699095284799590484672512.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 79, loss = 19506829091665008462557403934462478437103589465861200813508608787987338663932719640800158116626632412233728.00000000\n",
      "Iteration 80, loss = 19506829072684265753404497452164284758946163811739963379766700009339893917567681060714145035670023940603904.00000000\n",
      "Iteration 81, loss = 19506829054876949470275166341916684145814206809779780108322479144126395332491436410831603010963536793829376.00000000\n",
      "Iteration 82, loss = 19506829037070285038658262267216689435302540808831133306764231378513100312358985162964985254003028334215168.00000000\n",
      "Iteration 83, loss = 19506829019263620607041358192516694724790874807882486505205983612899805292226533915098367497042519874600960.00000000\n",
      "Iteration 84, loss = 19506829001456968397640312124733217624953339263202806012458097842904014088936778793519548320102248515371008.00000000\n",
      "Iteration 85, loss = 19506828983650299891951455381061050377550296443497837107963062745418217796523428836890331036468327688962048.00000000\n",
      "Iteration 86, loss = 19506828965843643608478456644305400740821384080061834512278389643549925320952775006548912332854643962937344.00000000\n",
      "Iteration 87, loss = 19506828948036983250933505238577578567201094897869509813656929209809131573101222467444894102567547870117888.00000000\n",
      "Iteration 88, loss = 19506828930230326967460506501821928930472182534433507217972256107940839097530568637103475398953864144093184.00000000\n",
      "Iteration 89, loss = 19506828912423662535843602427121934219960516533484860416414008342327544077398117389236857641993355684478976.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 90, loss = 19506828900218032224756001295225657774299276724163331110079618891463380836810472963199510492354716649914368.00000000\n",
      "Iteration 91, loss = 19506828896421881238482248397382715516532965502085290361569164736610391124168926021924748160159347535511552.00000000\n",
      "Iteration 92, loss = 19506828892860413092970038972566588349636921919185667183756175765320689880416598641433120323209955266002944.00000000\n",
      "Iteration 93, loss = 19506828889299087539976172961776499973939066992757317608730743409568533166495726067632475919829995834310656.00000000\n",
      "Iteration 94, loss = 19506828885737766061054259619958584135132588885085290136642098385688877724855752202594431043123448769413120.00000000\n",
      "Iteration 95, loss = 19506828882176420137700630264307633074977849864875330046932729370574214649530386084980789006376427503747072.00000000\n",
      "Iteration 96, loss = 19506828878615090510634811584545372162388618119690658368970509682949556663328614802417545076323055705260032.00000000\n",
      "Iteration 97, loss = 19506828875053764957640945573755283786690763193262308793945077327197399949407742228616900672943096273567744.00000000\n",
      "Iteration 98, loss = 19506828871492435330575126893993022874101531448077637115982857639572741963205970946053656742889724475080704.00000000\n",
      "Iteration 99, loss = 19506828867931101629437355545258589424620922884136643335083850620075582704723300954727813286162940309798912.00000000\n",
      "Iteration 100, loss = 19506828864369767928299584196524155975140314320195649554184843600578423446240630963401969829436156144517120.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 19696559390525857764415690853239495826188512851675567475331708991562170661499720541906157622840377428934656.00000000\n",
      "Iteration 2, loss = 23313350940825612572295141298120121818519389764477550759996402959026069753277729138307711002900534984179712.00000000\n",
      "Iteration 3, loss = 23311327958250780356430817273720788439446049557435389049104261958340418032375920727106608356481643692687360.00000000\n",
      "Iteration 4, loss = 23309084416534633373668466264784647486282432963051002143390232211630234496014135702481509620954103308877824.00000000\n",
      "Iteration 5, loss = 23306841080813598757068771977774692908536240611012438738482874531217206826220907392207652811306365001662464.00000000\n",
      "Iteration 6, loss = 23304597960997436902087884318253141680507284922933915967077873593578527264172733817085919755759183225421824.00000000\n",
      "Iteration 7, loss = 23302355057065805967466127108162517103551109848499173865796081359315343311342362125456873773964610573959168.00000000\n",
      "Iteration 8, loss = 23300112368997924112172935920450708494754562911709165354085317946798665062865478919300329304846164027244544.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, loss = 23297869896773046161825320348815158003226883005371742277824489461252014064405859179459496527388071866400768.00000000\n",
      "Iteration 10, loss = 23295627640370353608745141945454202114032526284680958630030330034193888960571101129050794140453139770245120.00000000\n",
      "Iteration 11, loss = 23293385599769089056333552297148765365606601186175699947771383775230307480182283622629633743005358919516160.00000000\n",
      "Iteration 12, loss = 23291143774948515478351466335540634980841100240176462282800131453329793713464979058564424567375782328926208.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 13, loss = 23289642808091239901441815390651706828487075352460341090388463948039112516181730666600091008949165899120640.00000000\n",
      "Iteration 14, loss = 23289176633692879529726375935020183628236742630194765403751209170533511903923467452957209825824134892879872.00000000\n",
      "Iteration 15, loss = 23288728333246734626462975147192095145220048966816627651550446756458551247561144784737175875872194671149056.00000000\n",
      "Iteration 16, loss = 23288280042233638885401975162045509439610228229047120295622879285614411972136509116784808171486557883072512.00000000\n",
      "Iteration 17, loss = 23287831759849839021361077804521485896131592075880165451879738291587592140547802219652486948508441426001920.00000000\n",
      "Iteration 18, loss = 23287383486095176145534128984705295576020444575819201105786317831350542133839974451598830666674762994941952.00000000\n",
      "Iteration 19, loss = 23286935220969442480251542585016139097816567972291800007566463979405697065687191665731263465641491883360256.00000000\n",
      "Iteration 20, loss = 23286486964472466914491306508622769912082135877532433833875108797108003500291708094020605225125308685877248.00000000\n",
      "Iteration 21, loss = 23286038716604127226094840686360011912075843728851439496608632328282423269226562473588870144923842398650368.00000000\n",
      "Iteration 22, loss = 23285590477364223785536464338591410790120227407189033951864455311176394030727718075068681418039887048736768.00000000\n",
      "Iteration 23, loss = 23285142246752589555866118037457892533668837343536010979234297139017363621278327839193458450863535597551616.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 24, loss = 23284842125356210592058561808497195794462156998675385323040824208139129025830799226335391347557580973015040.00000000\n",
      "Iteration 25, loss = 23284748909071555427719196918523202523416794533528181349716917540235496384360057196542321901218437376507904.00000000\n",
      "Iteration 26, loss = 23284659265400745157110734831304592249026203038785773946440175003603767775791774862708579565581795166519296.00000000\n",
      "Iteration 27, loss = 23284569622235743141641983646222954649741790597701412454121046433622496657640881531893872314110727625375744.00000000\n",
      "Iteration 28, loss = 23284479979415863909427726431821262192770448642175034942930375446969002998981404898410268620748076001263616.00000000\n",
      "Iteration 29, loss = 23284390336941091164180152512210824730546669897181353001121012716153281710689750127207370378800190827003904.00000000\n",
      "Iteration 30, loss = 23284300694811457498474883239169022558201468912770943452187256896155342971013106888385973801654371036954624.00000000\n",
      "Iteration 31, loss = 23284211053026922171592391922974130306821077501380585266761234668250174057142488094320083622576492963168256.00000000\n",
      "Iteration 32, loss = 23284121411587493331676583901570493050188249300522922650716520696182777513639691162534898894913381339234304.00000000\n",
      "Iteration 33, loss = 23284031770493187275015269850846800935868491585223244015800264307443158429628310928080817725358685632331776.00000000\n",
      "Iteration 34, loss = 23283942129743963260888923081081328594948036167918328332644592183306304082299360303331844847178282174513152.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 35, loss = 23283882108343270265520325083872331277337963475901369582561965162625601990650955817101957035436433536450560.00000000\n",
      "Iteration 36, loss = 23283863465830059437983391908385273129210604221969291057249902221403129831503732540181897566926798252408832.00000000\n",
      "Iteration 37, loss = 23283845537752555486378493891916612719069857703266096507389347457457350090587774687454338415070067315179520.00000000\n",
      "Iteration 38, loss = 23283827609720998918255796543609823369876873118363578878616321551580919133647454259324241085958009089163264.00000000\n",
      "Iteration 39, loss = 23283809681703229009620930997134898861103043204855057587931626702248793575268364283830942019673400096587776.00000000\n",
      "Iteration 40, loss = 23283791753699298723409281949130082172336266606572719973513498223803489955102187974888235062970601105784832.00000000\n",
      "Iteration 41, loss = 23283773825709142874469606696040612713314514223415412388373338806284987916654545992294527789075014248038400.00000000\n",
      "Iteration 42, loss = 23283755897732810351665337265532560926734307880459000067752596432163302727296222841201014518067587924885504.00000000\n",
      "Iteration 43, loss = 23283737969770288932780615650689409201921517121434516702840909105820930570184522395319896669928085035941888.00000000\n",
      "Iteration 44, loss = 23283720041821554173383725837678122317527881033804029676017552836022863811634052402075577084616031380439040.00000000\n",
      "Iteration 45, loss = 23283702113886622369762478502387390421118906892592827399029676950259107540768407696518453868825076425555968.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 46, loss = 23283690109721613784813621921363745478392688952847006960273720397510920762811517463325020475771462424199168.00000000\n",
      "Iteration 47, loss = 23283686381248715603818280918303591185331095478217000376108260344023714999367365741927632919318542862516224.00000000\n",
      "Iteration 48, loss = 23283682795659489318334454050344984023170325697748867726065010103369764245156126723138988511118232607784960.00000000\n",
      "Iteration 49, loss = 23283679210077241918105549131717932555938046446860497406738459360310492908124375814683333294473306672463872.00000000\n",
      "Iteration 50, loss = 23283675624495540443518301855362001032150260909319288880941411088166392056733051880416014652065637887639552.00000000\n",
      "Iteration 51, loss = 23283672038914388968644664890249361988698345903881564251610652618809962963263053629099632110568638620106752.00000000\n",
      "Iteration 52, loss = 23283668453333791567556590905352187962473678249303645621682971284113706899995279769496785196655721236660224.00000000\n",
      "Iteration 53, loss = 23283664867753727869894316555809616269019373851803922476474430424715117505525236757794476276959823903326208.00000000\n",
      "Iteration 54, loss = 23283661282174246764521273869287717351031954536458260051226478023084210047223709099143899671561895021641728.00000000\n",
      "Iteration 55, loss = 23283657696595315658861841494009110913380405753216081522444815424240974346843507123444259167074635657248768.00000000\n",
      "Iteration 56, loss = 23283654111016922330700161423057279345390597045808420581319080632567906587541934704407756183477808709763072.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 57, loss = 23283651710188532463160811383351862117883912290015877018011149001689709453927894977031733699169885121150976.00000000\n",
      "Iteration 58, loss = 23283650964495135937456798252258735972527421761925814394021098166766637771612049657411158733838252526010368.00000000\n",
      "Iteration 59, loss = 23283650247378347494624555210048570609718414591222141365815082006362677650434926906671747071281358421622784.00000000\n",
      "Iteration 60, loss = 23283649530262858680745213569961444515258612603785219174444224713286623386864492251201584417543009324761088.00000000\n",
      "Iteration 61, loss = 23283648813147394311297587943707353642147071528886229600694091411445576756979449848307018923845134428667904.00000000\n",
      "Iteration 62, loss = 23283648096031938089993867655397607842818284091499884232817532773349532671656204862937652483494084266164224.00000000\n",
      "Iteration 63, loss = 23283647378916526683481626725781759949294641660433081997245634785851002581422845673956880836550570138402816.00000000\n",
      "Iteration 64, loss = 23283646661801127499185243803082429666445129685635246070484098793969976308032182611263907769627293111025664.00000000\n",
      "Iteration 65, loss = 23283645944685752759320576894216134604943878623375342761343286793323957668326911801146531862744490284417024.00000000\n",
      "Iteration 66, loss = 23283645227570390241671767992266357154116758017384405761012836788295442845464337117316954535881924558192640.00000000\n",
      "Iteration 67, loss = 23283644510455068464742485780038305072203405598956689790050260101991940745410749521113372475753482499915776.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 68, loss = 23283644030289568120771752139283944235166097945573824765432601445457356790135125277687507341852676974444544.00000000\n",
      "Iteration 69, loss = 23283643881150928741536085668992609867630292663767768849415107130823254922024763559636867710185791650004992.00000000\n",
      "Iteration 70, loss = 23283643737727609349245992148888998641847433325936462011379704818343974857229786871857420928404489076998144.00000000\n",
      "Iteration 71, loss = 23283643594304558845704774780948774850895444026022413967172266409449778762974124962409542907068402712444928.00000000\n",
      "Iteration 72, loss = 23283643450881500194019652075064205986160701088595721717091253336810580124156665635436465832385491614302208.00000000\n",
      "Iteration 73, loss = 23283643307458433394190624031235292047643204513656385261136665600426378940777408890938189704355755782569984.00000000\n",
      "Iteration 74, loss = 23283643164035382890649406663295068256691215213742337216929227191532182846521746981490311683019669418016768.00000000\n",
      "Iteration 75, loss = 23283643020612320164892331288438326855065095457559322863911426787020482935423388945754635081663345953079296.00000000\n",
      "Iteration 76, loss = 23283642877189269661351113920498103064113106157645274819703988378126286841167727036306757060327259588526080.00000000\n",
      "Iteration 77, loss = 23283642733766223231881849221530051810052493676487548878433337301104592019192963835621478565664585590767616.00000000\n",
      "Iteration 78, loss = 23283642590343164580196726515645482945317750739060856628352324228465393380375504508648401490981674492624896.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 79, loss = 23283642494310073474360875659233390359071318209648192249889724627277979388338356819240947422883020594479104.00000000\n",
      "Iteration 80, loss = 23283642464482338265184227561025212919159678879525601281400008566980656724610666799858140348537501269360640.00000000\n",
      "Iteration 81, loss = 23283642435797669497839865654237883629733454829451753390268783306237799184914593011787131560173145914605568.00000000\n",
      "Iteration 82, loss = 23283642407113061841574793782033142393677883060722737043189368023582460729431999855155115671909976061771776.00000000\n",
      "Iteration 83, loss = 23283642378428441963093863902911883546948180835724754387299590745309618457106710572235301203626569108553728.00000000\n",
      "Iteration 84, loss = 23283642349743826158684886692762797237109855429483093834346600798909277457062319998078086262016574522130432.00000000\n",
      "Iteration 85, loss = 23283642321059218502419814820558056001054283660754077487267185516253939001579726841446070373753404669296640.00000000\n",
      "Iteration 86, loss = 23283642292374598623938884941436797154324581435756094831377408237981096729254437558526255905469997716078592.00000000\n",
      "Iteration 87, loss = 23283642263689978745457955062315538307594879210758112175487630959708254456929148275606441437186590762860544.00000000\n",
      "Iteration 88, loss = 23283642235005367015120930521138624534647930623272773725471428345180414729165656410211826022250008543232000.00000000\n",
      "Iteration 89, loss = 23283642206320755284783905979961710761700982035787435275455225730652575001402164544817210607313426323603456.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 90, loss = 23283642187114137878431126342473726751829970893656166820350063276789592457450914748688239699028378017333248.00000000\n",
      "Iteration 91, loss = 23283642181148592466224577790420960278604193755134177467826834997479128433617736228316718094828639099027456.00000000\n",
      "Iteration 92, loss = 23283642175411659527570095942857928928097224308870672310187947411705057180134701212455036242490450501435392.00000000\n",
      "Iteration 93, loss = 23283642169674734737059519433239242651373008500119811358422634489675988471213463614118553443499086637432832.00000000\n",
      "Iteration 94, loss = 23283642163937818094692848261564901448431546328881594612530896231391922306854023433307269697854547507019776.00000000\n",
      "Iteration 95, loss = 23283642158200893304182271751946215171707330520130733660765583309362853597932785834970786898863183643017216.00000000\n",
      "Iteration 96, loss = 23283642152463968513671695242327528894983114711379872709000270387333784889011548236634304099871819779014656.00000000\n",
      "Iteration 97, loss = 23283642146727060019448929408597532765824406177654300168982106792794721269213905473348219407574105382191104.00000000\n",
      "Iteration 98, loss = 23283642140990135228938352898978846489100190368903439217216793870765652560292667875011736608582741518188544.00000000\n",
      "Iteration 99, loss = 23283642135253210438427776389360160212375974560152578265451480948736583851371430276675253809591377654185984.00000000\n",
      "Iteration 100, loss = 23283642129516293796061105217685819009434512388914361519559742690452517687011990095863970063946838523772928.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 25678030217241534751087001668245334977400079615534887270454064986356059947279232333717810759495462628098048.00000000\n",
      "Iteration 2, loss = 30335173134807885240581824036249898298247079386272826383945257695087708720006229458420310327857698574434304.00000000\n",
      "Iteration 3, loss = 30332508119218416003598249752801158146717592684439728486559798128357852832521610951712167059064237678657536.00000000\n",
      "Iteration 4, loss = 30329587530602173297121632423066158342275069376828098462024967332624482549053275063382045416727208949448704.00000000\n",
      "Iteration 5, loss = 30326667212841030294752857899840920030892036066873278841799449883600717956125055012473917913077467339292672.00000000\n",
      "Iteration 6, loss = 30323747176265476802500200844945544959352378141573057040606692930970139743323102733441785939630886196609024.00000000\n",
      "Iteration 7, loss = 30320827420848868389793206180371641858051700934580879851857546028574427193569862918884745052270586683719680.00000000\n",
      "Iteration 8, loss = 30317907946564132848506388586032703083791043810136373260600188883642626000293415841328940506171391449497600.00000000\n",
      "Iteration 9, loss = 30314988753384193896442310072870048456480069313722841148946013871531280584640943064537918029834710776020992.00000000\n",
      "Iteration 10, loss = 30312069841281983399547437989769342869811193628336231604879988031341937912321423569800422405108779678957568.00000000\n",
      "Iteration 11, loss = 30309151210230465816343861037393631512607847487023069539881377057156155127291026008505994627229132108333056.00000000\n",
      "Iteration 12, loss = 30306232860202520049842661867990336298974548428947115703756912673732962655607046148029585631289642311483392.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 13, loss = 30304290414617731885217349540859934107227984113135747985566404364158361277370437404043226756454367251922944.00000000\n",
      "Iteration 14, loss = 30303685770141980397099291045232070640731571993285343420372667417589728492171007243837003943271065158942720.00000000\n",
      "Iteration 15, loss = 30303102184070211390572458190992344839629886081808635834220785487495555423337764975040112209315794553667584.00000000\n",
      "Iteration 16, loss = 30302518610089130903132576756745112167775252407537967398822298889387150734096896813060121588016058602618880.00000000\n",
      "Iteration 17, loss = 30301935047346496045216682460633897928725085791721591076445615210340869800090212835989845848259084612009984.00000000\n",
      "Iteration 18, loss = 30301351495842062372507615164328349908996777108980180690883494538006636284463790518073313389640130574155776.00000000\n",
      "Iteration 19, loss = 30300767955575585440688214729498115895107717233934410065928696960034373850363707333554552611754454481371136.00000000\n",
      "Iteration 20, loss = 30300184426546869694304753045478914116271818866280818260615430546544021428306825261828786234278262727507968.00000000\n",
      "Iteration 21, loss = 30299600908755699207543738656745600117246110611934333819293966708293011587405512738478239343520699872444416.00000000\n",
      "Iteration 22, loss = 30299017402201866202735585445717374516570374714322529492188151519783779441333935616610336079137735209648128.00000000\n",
      "Iteration 23, loss = 30298433906885126235563133274063885100762002048066079103090745068666248653238171370469104840724626731433984.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 24, loss = 30298045510860695610021883741422647967777488506179358317651423266958441569506300444053402526310499862183936.00000000\n",
      "Iteration 25, loss = 30297924606072115871006048282838808148267715280537430422005193631496890658701431810298491222456394613522432.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 26, loss = 30297807910228981163741137224824898814362165571879984069056136608988016188828423743442759877352450305294336.00000000\n",
      "Iteration 27, loss = 30297691215005700207786999606973787355874082362605211136552474660057714168570359071054613785246464351404032.00000000\n",
      "Iteration 28, loss = 30297574520231858647435444992280428143402513894515867881616902889850266360215148962358451723972546094170112.00000000\n",
      "Iteration 29, loss = 30297457825907533890053574091216099377883619723982074260048380603943196937099868883843664700325530502692864.00000000\n",
      "Iteration 30, loss = 30297341132032611861626712172559970026358848925826811389616862509906470275359354990157465967449871306719232.00000000\n",
      "Iteration 31, loss = 30297224438607178117665865284727663363547114693932843431994882577062613092892480165314445585487228208939008.00000000\n",
      "Iteration 32, loss = 30297107745631187843379554069025281483643272021980627254877780154814111394609358612926008761030065174609920.00000000\n",
      "Iteration 33, loss = 30296991053104665483199494539285859607995581822508095475886279234395972814195382585567752654118856404500480.00000000\n",
      "Iteration 34, loss = 30296874361027582518622018012704189978364406364220993374462868492700688445684261121901480578039715331047424.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 35, loss = 30296796685545867714084971171697796966922585132172595638626624988722326119384684071695261241181850203848704.00000000\n",
      "Iteration 36, loss = 30296772505552290671653602750145098435884093535685043097267756285550104265377295915152505107463820812484608.00000000\n",
      "Iteration 37, loss = 30296749167238426840673505846584722586551268855366153741106180790301261028724825464232612269728694743859200.00000000\n",
      "Iteration 38, loss = 30296725828976605204816802393556332987665926968305807299465982634383669908272460746759073158163142111395840.00000000\n",
      "Iteration 39, loss = 30296702490732778744775037790614038837991993527920189529615429359304198452549692633687643363060013611352064.00000000\n",
      "Iteration 40, loss = 30296679152506894497612827341119597157941569890377113093376285650720330121904837911104529037664948475396096.00000000\n",
      "Iteration 41, loss = 30296655814299001352193603072739078390211177880752443225989999491102080183708681084160924502058895105064960.00000000\n",
      "Iteration 42, loss = 30296632476109091160373459647528137461018063861533535721582996216704446093399424735331630702895028766769152.00000000\n",
      "Iteration 43, loss = 30296609137937143551792633720625911685905343738938780065471339168164921489572575320803650006806287626534912.00000000\n",
      "Iteration 44, loss = 30296585799783199267170651981754126433786785700531397287022901664208519095037119928202977680526795352309760.00000000\n",
      "Iteration 45, loss = 30296562461647237936147751086051919020205505652529776871553747045472732548388565013716616090689490110119936.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 46, loss = 30296546926699810453254297988701208871522620587205603869837052948343564494538252294213271189886748166455296.00000000\n",
      "Iteration 47, loss = 30296542090739654506171255058414861827439961068994268817082660448184661753530693617404200116321829053792256.00000000\n",
      "Iteration 48, loss = 30296537423111066462915690489408109205738006953477992267845489624918568560765085012440329960043291141472256.00000000\n",
      "Iteration 49, loss = 30296532755490023600916468856864894906865921174670250357538457429524831632223885035810783202924456533032960.00000000\n",
      "Iteration 50, loss = 30296528087869701849652869632396219637767532315731520667042782975563819897401756510161352666999610847264768.00000000\n",
      "Iteration 51, loss = 30296523420250088986909034809085565787768709920392836887548104267418029539456003309204239772248516983783424.00000000\n",
      "Iteration 52, loss = 30296518752631180938613011717960760819978077169897876916117633973214959286105726724176844991997762575794176.00000000\n",
      "Iteration 53, loss = 30296514085013002149196516372854839955743894976784573370372096084189616771036319007654765486287821824065536.00000000\n",
      "Iteration 54, loss = 30296509417395548544587596104795630658174786522296604147374703268469500721966881450875401728445282361802752.00000000\n",
      "Iteration 55, loss = 30296504749778820124786250913783132927270751806433969247125455526054611138897414053838753718470144189005824.00000000\n",
      "Iteration 56, loss = 30296500082162808741648575461873001689249037191684024463750778193199945477266119399019622403015582572085248.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 57, loss = 30296496975179283612336639548691281131596746019118426459927305903243473213450867774802060946057326804074496.00000000\n",
      "Iteration 58, loss = 30296496007988800570262045172059575741503405242878558565355613514762176131990865369228066867241042363547648.00000000\n",
      "Iteration 59, loss = 30296495074464455923858981701880370149557002340655851945205510191141886252100608501231333324925302390915072.00000000\n",
      "Iteration 60, loss = 30296494140941480165632015006351136953113210540557371911815950376682023858592317777468040744876117661319168.00000000\n",
      "Iteration 61, loss = 30296493207418545148124575000543629125583186928022112907794263880947174187893014141330743431561056599670784.00000000\n",
      "Iteration 62, loss = 30296492273895638649120803677541329056292801046781108624330088708319833423160001466531642804959882105585664.00000000\n",
      "Iteration 63, loss = 30296491340372760668620701037344236745242052896834359061423424858800001564393279753070738865072594179063808.00000000\n",
      "Iteration 64, loss = 30296490406849915280696219748924524729322319296938186322011059664260179883873747709710631138572605186899968.00000000\n",
      "Iteration 65, loss = 30296489473327106559419312481254365545424977065848912509029780456572869653882304045213919152133327495888896.00000000\n",
      "Iteration 66, loss = 30296488539804318208502168558445069045984518928541249210732437908248065785295353924530204799061111638851584.00000000\n",
      "Iteration 67, loss = 30296487606281558376088693318440980304783698522527840632992606683030770822674694765184687132702782349377536.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 68, loss = 30296486984885098609357856807929422914114124777145311629022509603764052961221746707849666347050555676819456.00000000\n",
      "Iteration 69, loss = 30296486791447054149063932095446890308305079901978260967699048974035809830125249698896141472706977083686912.00000000\n",
      "Iteration 70, loss = 30296486604742244701233828368404768228529900875376022346546123354650270429448319473230747853675649644363776.00000000\n",
      "Iteration 71, loss = 30296486418037700068080648124553861046694215067934720416284374306977313727029805317134323468416126046699520.00000000\n",
      "Iteration 72, loss = 30296486231333163583071373218647298938641282898006062691896199923049359569173088578563098136503427182624768.00000000\n",
      "Iteration 73, loss = 30296486044628614875846240305824219219914220271808438658697663543503901594473675713704074224570491218165760.00000000\n",
      "Iteration 74, loss = 30296485857924074316765012730945484574969911283123458831372701827703446164336060266370249365984379987296256.00000000\n",
      "Iteration 75, loss = 30296485671219541905827690494011095003808355931951123209921314775647993278760242236561623560745093490016256.00000000\n",
      "Iteration 76, loss = 30296485484515005420818415588104532895755423762022465485533140391720039120903525497990398228832394625941504.00000000\n",
      "Iteration 77, loss = 30296485297810468935809140682197970787702491592093807761144966007792084963046808759419172896919695761866752.00000000\n",
      "Iteration 78, loss = 30296485111105932450799865776291408679649559422165150036756791623864130805190092020847947565006996897792000.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 79, loss = 30296484986826660867813461819049959885972528766870254750646708867373293594303995953193941041243613397254144.00000000\n",
      "Iteration 80, loss = 30296484948139041383167599937225804768893140063070407150746369678559141660154359908620477297024025524961280.00000000\n",
      "Iteration 81, loss = 30296484910798077863972798124228511338181553530247430585341069621933033271106614379982358762548395090378752.00000000\n",
      "Iteration 82, loss = 30296484873457183604001191683758151034623372916281929769861154207139446510834146900308432181520774891307008.00000000\n",
      "Iteration 83, loss = 30296484836116281195885679905343445657282438664803784748507664128600857205999882003109306547146329958645760.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 84, loss = 30296484798775378787770168126928740279941504413325639727154174050062267901165617105910180912771885025984512.00000000\n",
      "Iteration 85, loss = 30296484761434476379654656348514034902600570161847494705800683971523678596331352208711055278397440093323264.00000000\n",
      "Iteration 86, loss = 30296484724093561749323286563182811914585505454100383375636831897367585474654391185224131064002758060277760.00000000\n",
      "Iteration 87, loss = 30296484686752667489351680122712451611027324840134882560156916482573998714381923705550204482975137861206016.00000000\n",
      "Iteration 88, loss = 30296484649411769155308121013269918770577767407413059641740213735907910681828557517113678375274105295339520.00000000\n",
      "Iteration 89, loss = 30296484612070850450904798558966523245671325880909626208639574329879316287870697784864154634206010895499264.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 90, loss = 30296484587215023023182405382734572230419006753642373030800354168939657242747410049166510205497855816237056.00000000\n",
      "Iteration 91, loss = 30296484579477488533666156067042092611085549284115966043184639268308323547987146197469058687303066088112128.00000000\n",
      "Iteration 92, loss = 30296484572009290940940852501676026880673579795043784206579434458736100343440518641226315548399845161041920.00000000\n",
      "Iteration 93, loss = 30296484564541093348215548936309961150261610305971602369974229649163877138893891084983572409496624233971712.00000000\n",
      "Iteration 94, loss = 30296484557072907977706103377860413030523771273168386842179386835209157751189959655028627850613640407285760.00000000\n",
      "Iteration 95, loss = 30296484549604726681268610488383037447677309059121493417321331353126939635766926933836282818404068947394560.00000000\n",
      "Iteration 96, loss = 30296484542136529088543306923016971717265339570049311580716126543554716431220299377593539679500848020324352.00000000\n",
      "Iteration 97, loss = 30296484534668347792105814033539596134418877356002418155858071061472498315797266656401194647291276560433152.00000000\n",
      "Iteration 98, loss = 30296484527200154273452463137145702940898284685686558422189653583772776383531537808921051035061468000157696.00000000\n",
      "Iteration 99, loss = 30296484519731972977014970247668327358051822471639664997331598101690558268108505087728706002851896540266496.00000000\n",
      "Iteration 100, loss = 30296484512263779458361619351274434164531229801323805263663180623990836335842776240248562390622087979991040.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 8864440412569099600722908483754444396730060350409261850119707065821438307522113913076672092249886325473280.00000000\n",
      "Iteration 2, loss = 12004987233424604705714998025332878690794453916495463856566257638726789649554669987164696071689119241076736.00000000\n",
      "Iteration 3, loss = 12015977907673099134093089068673788058807655847671850328777638916940834238426576750405518827978434843508736.00000000\n",
      "Iteration 4, loss = 12015317854933084031371296852041507135013488684699637489615148253702883874395609329541189499864552004648960.00000000\n",
      "Iteration 5, loss = 12014625908396180198120967594893964228959173434005460919606751346986212162987686520399592313130506854072320.00000000\n",
      "Iteration 6, loss = 12013933914253792449761154194832829759028523975236172173398478579436702781398207886558710859634707244515328.00000000\n",
      "Iteration 7, loss = 12013241959727947220867326018197728203668642621134938489564904251907686193251602194465470665102129828462592.00000000\n",
      "Iteration 8, loss = 12012550045055266610450496968769602214045162738889498436714263519273056473144874375900380921323038045110272.00000000\n",
      "Iteration 9, loss = 12011858170234102656405812447304660617596161141567561376916080639106048983454496736391933088902129525981184.00000000\n",
      "Iteration 10, loss = 12011166335262153508080014484525420070693735231847139150885513103443444885472698825071394597361417032040448.00000000\n",
      "Iteration 11, loss = 12010474540137135648143632121529173645721178096809693062553261397748281065755754380501730746253268974829568.00000000\n",
      "Iteration 12, loss = 12009782784856749262979383723524524267496275548511396004102876679993589321736342306195508728436404298711040.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 13, loss = 12009291309711835990320971886521130324157086116197098488840922415395104694492474045494901979587922037309440.00000000\n",
      "Iteration 14, loss = 12009136935319093798225804145771576450724941265694804114944841020936495748753857991245663149422969962364928.00000000\n",
      "Iteration 15, loss = 12008998553057688066378263925149014056752370665052529629188046844687279871659870310514240761878630535004160.00000000\n",
      "Iteration 16, loss = 12008860216186727958585685363495201948656479419586738463321481771245423354507123253601546283516224377716736.00000000\n",
      "Iteration 17, loss = 12008721881029277699294197138916508447845412076877899377902041621241174188261395949661175848987756058378240.00000000\n",
      "Iteration 18, loss = 12008583547465696054505745880108128933701820629392976479764544960865919935910757521814129403528956098904064.00000000\n",
      "Iteration 19, loss = 12008445215495636728104354724435397770458675482844591019282068580957052453578817725239447179899773321740288.00000000\n",
      "Iteration 20, loss = 12008306885119085460838189330495711078996157771585615636175856819960817288282431079268030834743264443105280.00000000\n",
      "Iteration 21, loss = 12008168556336017808275533684456033637966006583078117712825185686642206806336205331324283208018955262230528.00000000\n",
      "Iteration 22, loss = 12008030229146423585236506113885934105139779870431291991888086851319967827037893709501705483043314862129152.00000000\n",
      "Iteration 23, loss = 12007891903550276310253414270466290990723528311729044804275442656822842080561654606843400736439162858635264.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 24, loss = 12007793622981674274222496325158533274412125914736041840201605439998938264001004245060069142974971961671680.00000000\n",
      "Iteration 25, loss = 12007762751858090806486099234467737561496334313426588495068669217283094318205511481737588709609711022899200.00000000\n",
      "Iteration 26, loss = 12007735078465027437525355526420656041731060542435803359469062310927855979030494685442063628775680659947520.00000000\n",
      "Iteration 27, loss = 12007707413893875956398974446855422103055896926822114432953751815441338616161008892528306858275754133684224.00000000\n",
      "Iteration 28, loss = 12007679749410445355521485341623166241060706362957784889874877532715965369462070821526258656354203248623616.00000000\n",
      "Iteration 29, loss = 12007652084990818795494745009585010634917874341853909439822407705233017313917810681595397686743728736174080.00000000\n",
      "Iteration 30, loss = 12007624420634933128203487081672280962811060172787495487276138688968724729174298486915431286006438911016960.00000000\n",
      "Iteration 31, loss = 12007596756342772057359900881996287077174756580733254620488921156433082526107939402435961347448684305973248.00000000\n",
      "Iteration 32, loss = 12007569092114353916287773420931805394020159250094636302676298101052346429982777617588685741100820571619328.00000000\n",
      "Iteration 33, loss = 12007541427949670556843199360534490839564514543358996327964694859081513896237015714848405413616022974365696.00000000\n",
      "Iteration 34, loss = 12007513763848719941990202366318257145362134051148173644885717764584334288730204339833820601657585330814976.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 35, loss = 12007494108313366603993568806889343058938036993628640056917266203930420211413376461517499467937233198120960.00000000\n",
      "Iteration 36, loss = 12007487934238836498909478378919583880069925306424752049618881107274152785304277733321954669937670532104192.00000000\n",
      "Iteration 37, loss = 12007482399682582465107813213054054303282556670226473574501291518770066165840643995595733999701182589698048.00000000\n",
      "Iteration 38, loss = 12007476866880466962355229709254267483035336984207318716942493952222524718546403145865617123903460345905152.00000000\n",
      "Iteration 39, loss = 12007471334085701085405261031253737214831898334593237557348043083667278465993432903865046366941647799844864.00000000\n",
      "Iteration 40, loss = 12007465801293491688605592133291473845967413454571275990587642965106580569704402410395678597546095417491456.00000000\n",
      "Iteration 41, loss = 12007460268503836734920246680881391107996193934763272965192899930604180393538862311076214052380097015447552.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 42, loss = 12007454735716730113241295670565230195581174547034745326758632982351326029075464542762753441433534043521024.00000000\n",
      "Iteration 43, loss = 12007449202932173860604715436829077377168043700763854126753235786284268112454658459836596528043112685109248.00000000\n",
      "Iteration 44, loss = 12007443670150163902938553310700760115865424577194277262239921010530505371395545353535143785535420573417472.00000000\n",
      "Iteration 45, loss = 12007438137370702277278785626666364680119005585704175784687082321026288442038574578239694977247163891843072.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 46, loss = 12007434206286766634669884940159549854206589163850962803933934780081230400886616581874102918142559545982976.00000000\n",
      "Iteration 47, loss = 12007432971477871092004839389211741694281187537612183665116608472251103911674661873698075659827911745077248.00000000\n",
      "Iteration 48, loss = 12007431864571518949360395528178894137115200682974224541286178398045816378334542542302502397891643990605824.00000000\n",
      "Iteration 49, loss = 12007430758015573661293057294733772070378018240844016907597375345802456452811465173329618789479631885959168.00000000\n",
      "Iteration 50, loss = 12007429651460693743041341997511768400735873903492039191878459578218179228743400145776511406164953698140160.00000000\n",
      "Iteration 51, loss = 12007428544905923824732348762538423227160903672560758255452801771191436356259600254813591243032409413779456.00000000\n",
      "Iteration 52, loss = 12007427438351259832294124920841564012761730729293851995383614592849726563079166791678258773408586666082304.00000000\n",
      "Iteration 53, loss = 12007426331796685469438859796532500609972847798666031999923748715703044760078504921320115890599835987869696.00000000\n",
      "Iteration 54, loss = 12007425225242221106526316734472095703251138974458908783757140799113897308662108187552160227973219213115392.00000000\n",
      "Iteration 55, loss = 12007424118687852484304661393257745413476785391025354986605035181528529755846831109705293442171793058037760.00000000\n",
      "Iteration 56, loss = 12007423012133589787953775445319881082878229095256175865809400192628195282334920459686014349879088439623680.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 57, loss = 12007422225917727881172446431598901245727421350446282846603173752684222609096625620399248445590116069539840.00000000\n",
      "Iteration 58, loss = 12007421978956188328070254256973084782955297968070266671522803605221272121371078754004895162323833676890112.00000000\n",
      "Iteration 59, loss = 12007421757575115084623874663019666057064738624948664628897224453009276193098552391835597600929738678861824.00000000\n",
      "Iteration 60, loss = 12007421536264105693583519718003450520627019843676427841671785178137910315581569974471660005544821633581056.00000000\n",
      "Iteration 61, loss = 12007421314953293895032869218137603023421076772085813046880531499082856143688174932093799453820404377845760.00000000\n",
      "Iteration 62, loss = 12007421093642488207590147721730014331552198928629681406494458817836553880216127952859838192106105672302592.00000000\n",
      "Iteration 63, loss = 12007420872331680483111449890836339371237632675795388714639992470654000980603631619244577167055100783362048.00000000\n",
      "Iteration 64, loss = 12007420651020880906776657397887009484705820060473740228659100787216450625552932703154515195350920628011008.00000000\n",
      "Iteration 65, loss = 12007420429710085404513817573909852135065384263908413845614996435651401542783132495827052750320152839454720.00000000\n",
      "Iteration 66, loss = 12007420208399293976322930418904867322316325286099409565507679415958853732294230997262189831962797417693184.00000000\n",
      "Iteration 67, loss = 12007419987088508659239972267358141314904331536424888439805543394075057830226677561841226203615560546123776.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 68, loss = 12007419829845368870459327816391325642605184537513486659458596761066273473826208264084669236145065006465024.00000000\n",
      "Iteration 69, loss = 12007419780453076441312309523560417990237991772312307415602314592689188210948513984103676780850775521755136.00000000\n",
      "Iteration 70, loss = 12007419736176863014844619405461386006127292949314883637958234961808539406978278324298597126573980232187904.00000000\n",
      "Iteration 71, loss = 12007419691914670507002039555094139733689915876199977117267757970141019157720948870979788518845845266759680.00000000\n",
      "Iteration 72, loss = 12007419647652516702843010059962532561720618581270130574476760631262260995132157150905675414515127785881600.00000000\n",
      "Iteration 73, loss = 12007419603390356787576051561372666584414256058205800877280582294574750924122017367687663020174291754811392.00000000\n",
      "Iteration 74, loss = 12007419559128198909345069397268886875553581944519632231552797623823491489252326938850950389170161907138560.00000000\n",
      "Iteration 75, loss = 12007419514866043068150063567651193435138596240211624637293406619008482690523085864395537521502738242863104.00000000\n",
      "Iteration 76, loss = 12007419470603891301027010407005672531614987354659939145970802946065975164074743498702724180508726945382400.00000000\n",
      "Iteration 77, loss = 12007419426341729348724075573929720285862936422217448397306230943442214456924154361103412022831184730914816.00000000\n",
      "Iteration 78, loss = 12007419382079571470493093409825940577002262308531279751578446272690955022054463932266699391827054883241984.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 79, loss = 12007419350630946364587331387913098218366396681878424867564808078399949041370999168849207667004344432066560.00000000\n",
      "Iteration 80, loss = 12007419340752491138015489864524654717406059583843246701142981510222533006620179279863088797284216428560384.00000000\n",
      "Iteration 81, loss = 12007419331897253341608295043671455364853572001751348469138310382293404772563210598417192298436952210800640.00000000\n",
      "Iteration 82, loss = 12007419323044814432632583806700788856676958905252734954706536250772650595483654836877170624223983981035520.00000000\n",
      "Iteration 83, loss = 12007419314192385708836754242160553690728787855644926697616730448933149599106345847243647766694546668257280.00000000\n",
      "Iteration 84, loss = 12007419305339954948004948343134232256334928396658957389058530981157397966588587503228825145828403172081664.00000000\n",
      "Iteration 85, loss = 12007419296487524187173142444107910821941068937672988080500331513381646334070829159214002524962259675906048.00000000\n",
      "Iteration 86, loss = 12007419287635093426341336545081589387547209478687018771942132045605894701553070815199179904096116179730432.00000000\n",
      "Iteration 87, loss = 12007419278782658591437577977083095416261973200944727360447145245957641796754413762421757756556560316760064.00000000\n",
      "Iteration 88, loss = 12007419269930227830605772078056773981868113741958758051888945778181890164236655418406935135690416820584448.00000000\n",
      "Iteration 89, loss = 12007419261077797069773966179030452547474254282972788743330746310406138531718897074392112514824273324408832.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 90, loss = 12007419254788069604149642173264580553612255066388424504765946272424436572213664896451054453855683814096896.00000000\n",
      "Iteration 91, loss = 12007419252812378966242469135484109107109325328657021081775259691976203492491590789530090632578999450075136.00000000\n",
      "Iteration 92, loss = 12007419251041330999553834904416251982909690130363009225080646733203127718452107182364651380142205369843712.00000000\n",
      "Iteration 93, loss = 12007419249270843217758692657022118681274367511063286522194291906898976883036196030056647045299611723890688.00000000\n",
      "Iteration 94, loss = 12007419247500351361891597740655812842747668073007241716371149748722324775339386168986043183783605711142912.00000000\n",
      "Iteration 95, loss = 12007419245729867654168408162233852078003722272463841116421582254290675212204373725440638375614424431984640.00000000\n",
      "Iteration 96, loss = 12007419243959379872373265914839718776368399653164118413535227427986524376788462573132634040771830786031616.00000000\n",
      "Iteration 97, loss = 12007419242188894127614100001931671743178765443242556762117266267618624177513000775205929469265943323475968.00000000\n",
      "Iteration 98, loss = 12007419240418406345818957754537538441543442823942834059230911441314473342097089622897925134423349677522944.00000000\n",
      "Iteration 99, loss = 12007419238647916526987839172657318871462431795264950304876162949074071870540729116208621036244049848172544.00000000\n",
      "Iteration 100, loss = 12007419236877434856300625928721444375164174404099710756394989120578672943546166027044515991411574752411648.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 12684508969981443379720826758896303827116902756002357778770278620148348107420644055718072147790848201850880.00000000\n",
      "Iteration 2, loss = 15104141757905985588512060898013505106066092529631678397003319745210646770229042494328033356750601588834304.00000000\n",
      "Iteration 3, loss = 15102951728586893026343305436572483682969674723712139190703137228098087220319839437136501025747389269409792.00000000\n",
      "Iteration 4, loss = 15101559234350357764635724436589728432748598170253080895679614281973425019170395467492109673270993211097088.00000000\n",
      "Iteration 5, loss = 15100166856003394380430907019692754395944078746789239087044735037322103424724181123159842026531751521681408.00000000\n",
      "Iteration 6, loss = 15098774606034282111712745934678016199712599529378839568096005929465303345619008007425707019834780538634240.00000000\n",
      "Iteration 7, loss = 15097382484431974112381579263499680063085916460254500225734576586115824992198027310501088078222447697985536.00000000\n",
      "Iteration 8, loss = 15095990491184606684911234959191318558374733321006257308035736594549963482484199115696163528717940893417472.00000000\n",
      "Iteration 9, loss = 15094598626280377242854831009369092311260406174568978607126585520131533018713964137760104598445633520533504.00000000\n",
      "Iteration 10, loss = 15093206889707420051650219032580487625607950393154539319614019284200578082769833105621789851092007289618432.00000000\n",
      "Iteration 11, loss = 15091815281453932524850517016441665127098722039697807237625137452120912876888246734030390513781435596275712.00000000\n",
      "Iteration 12, loss = 15090423801508063187145410920902714998717555352057784918045591606786336335934861232583883493557343434571776.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 13, loss = 15089494545078759628206884708405417947525225448966312353857027211147182601273219397554585263445723272183808.00000000\n",
      "Iteration 14, loss = 15089205437946945439957329847518133413457440739028190208074530844358805546200879351702805778716894693425152.00000000\n",
      "Iteration 15, loss = 15088927180111533223935529001586826586314157596239940923614245688471575154982929333346451840949285319344128.00000000\n",
      "Iteration 16, loss = 15088648928076540209806650596583193488002810579482526860749840687545741970492592978440203907304776305475584.00000000\n",
      "Iteration 17, loss = 15088370681172775634913099939335014488532658050312608034541331792012326221431563777436208709761647413035008.00000000\n",
      "Iteration 18, loss = 15088092439400113203024344291704940944271018627284199253948311713823788467091981758693880921444115271385088.00000000\n",
      "Iteration 19, loss = 15087814202758455136413519598360831969824848660245570048487884488040098172732277911910831902190283077451776.00000000\n",
      "Iteration 20, loss = 15087535971247705694389738138456632948246792908423150999145547815657475439751332581165972775174960211558400.00000000\n",
      "Iteration 21, loss = 15087257744867767099226135856660202994143807721665211635438405731735889733407576756156914900236249870630912.00000000\n",
      "Iteration 22, loss = 15086979523618553795411706704555918832796979906088987795693924266952814335802137552869068217232492351979520.00000000\n",
      "Iteration 23, loss = 15086701307499955783003728619895121968139135355273782700618845460750714895350749834712245505981553752145920.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 24, loss = 15086515498918351464363716062759935718235836619556162003066575732748130746851093589307175832780088981585920.00000000\n",
      "Iteration 25, loss = 15086457688538429763623627840582309612663481660572233640843787958463618527964103717751001942129697976483840.00000000\n",
      "Iteration 26, loss = 15086402046733452755723617086097879682121145995749353019838861481448753923197469535491007568981885760569344.00000000\n",
      "Iteration 27, loss = 15086346405267487423149096844034589556826652682001301974781394779009758356574188906662826743823939269558272.00000000\n",
      "Iteration 28, loss = 15086290764006751428154299740676759066193715808601025318917377487886103304082912839404502328477263880781824.00000000\n",
      "Iteration 29, loss = 15086235122951236622595320438080043136439581738035878846373234944332786221161843916190835269595034860650496.00000000\n",
      "Iteration 30, loss = 15086179482100924673148371925869665351553054785902413093933424154923551382546937947590127697146896558587904.00000000\n",
      "Iteration 31, loss = 15086123841455829839065288545448229590653953817847755421876700781212153241221340414271477954489792258375680.00000000\n",
      "Iteration 32, loss = 15086068201015941935166188624385304511513836786981100572861096493517338616482804544328387224940191043026944.00000000\n",
      "Iteration 33, loss = 15086012560781283368846811842027839067035276196462220113038941617137864505876273235955152905201860929912832.00000000\n",
      "Iteration 34, loss = 15085956920751819510495560512112366693641569086862376167447543831157470095014107464669679018550796801277952.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 35, loss = 15085919760743431090765444514856245046650868365070845187922357475540318863670863672562237111842911543099392.00000000\n",
      "Iteration 36, loss = 15085908199109257446488637223185396742449818993246351443911611168147638285813488103975372668580244215562240.00000000\n",
      "Iteration 37, loss = 15085897071138697063827908618368628458236538235529820374081467275218843259735106693362672966418782417321984.00000000\n",
      "Iteration 38, loss = 15085885943203114625916819474139175654938934910216704068038961173649721401312589364048209518839195735425024.00000000\n",
      "Iteration 39, loss = 15085874815275741442990358308837384687765621378892625179622928795170663188900970191371792318185528144953344.00000000\n",
      "Iteration 40, loss = 15085863687356571403940596119004996751379532413423100554428189141972916714078901112189522074447661095714816.00000000\n",
      "Iteration 41, loss = 15085852559445620805055343580530701993346175288833418604201891541546487065969976961551796894319244054888448.00000000\n",
      "Iteration 42, loss = 15085841431542869275974837348553637729208665911341968814260099334528867883169704195645619144433215188500480.00000000\n",
      "Iteration 43, loss = 15085830303648324964842982761018149032749757918461395390476387184665061710239880231996187878136399230140416.00000000\n",
      "Iteration 44, loss = 15085819175761987871659779817924235903969451310191698332850755091955068547180505070603503095428796179808256.00000000\n",
      "Iteration 45, loss = 15085808047883866144569133857216243416650499724045521847256777720143890938553376128992763849657230771093504.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 46, loss = 15085800615950502499125463983154117681858856485696237695401721287121794298394275901840138728094220133859328.00000000\n",
      "Iteration 47, loss = 15085798303641335798107242056440588757852496511873402752565194157119525683396231078888133164029097747152896.00000000\n",
      "Iteration 48, loss = 15085796078062848194920777110656131333129077972700993511472261842763396002655192772211037968800573282058240.00000000\n",
      "Iteration 49, loss = 15085793852490031699892427374135845261202191142328951558387295494929037336925157063072483902962063395258368.00000000\n",
      "Iteration 50, loss = 15085791626917551315800172827819793482813891859353483097587284026576033634369264826848390787680073769025536.00000000\n",
      "Iteration 51, loss = 15085789401345392783392179130305372118844361258127460768793471776150630442004370582869660279597661119578112.00000000\n",
      "Iteration 52, loss = 15085787175773564250812351619536926243076352976163528777879433407397830304392271748661491432061650180505600.00000000\n",
      "Iteration 53, loss = 15085784950202061643988737626542283318618490194705365021908381588445131949252069615461284718398628585013248.00000000\n",
      "Iteration 54, loss = 15085782724630899222173171492724047224590591779400096861159071980846289829566909663938138481965539616882688.00000000\n",
      "Iteration 55, loss = 15085780499060044392790031866304837665861643180197147472137205929621293767089602223991256509375084341755904.00000000\n",
      "Iteration 56, loss = 15085778273489539933594821771492466279791100994037898935679050419431407120769583737627933830698092610977792.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 57, loss = 15085776787105601314193523943905030640635755413735805386186017552462591620448887142777951156912529839489024.00000000\n",
      "Iteration 58, loss = 15085776324644473603252081824542608245420948427566226626270278006779358256500934535869788063934527290998784.00000000\n",
      "Iteration 59, loss = 15085775879529401860066718789511418426991744080702819789142225719524327742698768540469656321924961937653760.00000000\n",
      "Iteration 60, loss = 15085775434415411782984789366592037153223085113642931281731210044418385019475209721539698911706379968315392.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 61, loss = 15085774989301433928118717950589173490128556602852009083130556364929946113094347028897540081508035099361280.00000000\n",
      "Iteration 62, loss = 15085774544187470332504480875988913706153846957708214244808658346995261659696629816924479594666633514188800.00000000\n",
      "Iteration 63, loss = 15085774099073527107250007146249516606636021406346029921170696988423083567703406148764416741192293762990080.00000000\n",
      "Iteration 64, loss = 15085773653959583881995533416510119507118195854983845597532735629850905475710182480604353887717954011791360.00000000\n",
      "Iteration 65, loss = 15085773208845663064136799366117671360502942806781432840047104596577484381261901710638588430947382277963776.00000000\n",
      "Iteration 66, loss = 15085772763731748357385994319183482019224754986713503236966654561112815195234969003816722264186929094328320.00000000\n",
      "Iteration 67, loss = 15085772318617851983958976282624069093957762851049023097101747519074401734472080486426553967456831561269248.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 68, loss = 15085772021341181593350953583505151028598346115170680951782616103608675276097823979819423800893995170660352.00000000\n",
      "Iteration 69, loss = 15085771928848990680774262845896133113132087677365503074762160515388289417695872482919887159022399778717696.00000000\n",
      "Iteration 70, loss = 15085771839825994258053781982367454311768304810520638960258414318176288912971393602395298727983501121945600.00000000\n",
      "Iteration 71, loss = 15085771750803213761146792574363919965647493337760846301404396710206855839134546286288485210635457905295360.00000000\n",
      "Iteration 72, loss = 15085771661780441412383708504304730693309435502513697848423953765982425309859496387706870746634239422234624.00000000\n",
      "Iteration 73, loss = 15085771572757669063620624434245541420971377667266549395443510821757994780584446489125256282633020939173888.00000000\n",
      "Iteration 74, loss = 15085771483734896714857540364186352148633319832019400942463067877533564251309396590543641818631802456113152.00000000\n",
      "Iteration 75, loss = 15085771394712118254986527290668904070958196768637769335077443935500381813612998628818128064620465422860288.00000000\n",
      "Iteration 76, loss = 15085771305689347943259419555095801067065827342768781933565394657212201920478398084617813363955953123196928.00000000\n",
      "Iteration 77, loss = 15085771216666575594496335485036611794727769507521633480584951712987771391203348186036198899954734640136192.00000000\n",
      "Iteration 78, loss = 15085771127643799171661298746005249985498334853518162924667721436890839589647399578691984909280103790280704.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 79, loss = 15085771068188471612054818476536942431452654416352609860302754884793696333621986211390718118644996299030528.00000000\n",
      "Iteration 80, loss = 15085771049690048096198509937314959981168359276314333855471098161890623742152831263556169086294961741103104.00000000\n",
      "Iteration 81, loss = 15085771031885430885737822021131665058573544700417543779648484662209218043171981168895813482724167595851776.00000000\n",
      "Iteration 82, loss = 15085771014080897193752163818877907142251954909025356814030011465914088425949554603868748175958326969892864.00000000\n",
      "Iteration 83, loss = 15085770996276334983262836933818941467690727386338915127854026946511449902760837077503486182478599776370688.00000000\n",
      "Iteration 84, loss = 15085770978471789069061320724648665940695007138677761853425191754598816468695714386188622295692522050027520.00000000\n",
      "Iteration 85, loss = 15085770960667243154859804515478390413699286891016608578996356562686183034630591694873758408906444323684352.00000000\n",
      "Iteration 86, loss = 15085770942862685018442430299391597276029436187086488995757159375156045783722772877271095942100129496956928.00000000\n",
      "Iteration 87, loss = 15085770925058153363492748431623925628153534805072463081607079844797166802640795666625330398670995054395392.00000000\n",
      "Iteration 88, loss = 15085770907253578930787563539648442342918176826117055086620733329777024462609382013972269825171030760488960.00000000\n",
      "Iteration 89, loss = 15085770889449045238801905337394684426596587034724868121002260133481894845386955448945204518405190134530048.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 90, loss = 15085770877557978097251828215912153901030900219789228666954551890313465685269513291979911349608803689562112.00000000\n",
      "Iteration 91, loss = 15085770873858284838529465903226195083502149872393297049820967148800598495185795014011542537124630807707648.00000000\n",
      "Iteration 92, loss = 15085770870297397248270511806944654423627302962269573540500172969342328551461533632190347251136500806451200.00000000\n",
      "Iteration 93, loss = 15085770866736452621004220345052698247273180589557340590064356143669040795804690327692758591720597670068224.00000000\n",
      "Iteration 94, loss = 15085770863175562993709289914285071318952645270055456029275168298274520215939979591490263542395761485414400.00000000\n",
      "Iteration 95, loss = 15085770859614624477550927455851373947935588125477706233244532470409984368704484350136574172989976899223552.00000000\n",
      "Iteration 96, loss = 15085770856053730776184044356111574482723675987219499569518557293142962516558874905171479596991728347774976.00000000\n",
      "Iteration 97, loss = 15085770852492798371133610901136135917043684070776232927893102463087178577744727726961689517596062311776256.00000000\n",
      "Iteration 98, loss = 15085770848931898558658798797938077646494706704383543109761946288011404817177770218852695651587695210135552.00000000\n",
      "Iteration 99, loss = 15085770845370974301752270680906984154597468425452920674010066121700623422925420458168104625538853907726336.00000000\n",
      "Iteration 100, loss = 15085770841810068378169529574250667078711425830925747701473728948816097753937114886915211469520368255893504.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 13580453437996807096527257670126262058341099005471348276945565259439149220296562089267603694851634224955392.00000000\n",
      "Iteration 2, loss = 16555482594423235987894421153052842151993485559565084843250599632858164561489176840729783139257365441806336.00000000\n",
      "Iteration 3, loss = 16554837175979010493444338135984747182110220411847469566338225640558570959543131229547606769062477213728768.00000000\n",
      "Iteration 4, loss = 16553477598255444943524481999667271274504556964526805172672169954002615820648097926049001007954290001575936.00000000\n",
      "Iteration 5, loss = 16552118005803058174142502864808418521723870173672921735110263552242036984449020964510792979399749127897088.00000000\n",
      "Iteration 6, loss = 16550758524996172529225296938655008739060467945658928753280131306821929391183990491767585742737738626498560.00000000\n",
      "Iteration 7, loss = 16549399155847996150043417028990406528357996688481083948246303148391417775526620316167044773171151645966336.00000000\n",
      "Iteration 8, loss = 16548039898349354226559452610482058810235860562902011506363707700077636960893018304335036002175338164518912.00000000\n",
      "Iteration 9, loss = 16546680752491067874664040488825239968422086910928013511050486253135220498418393614134825834552235793580032.00000000\n",
      "Iteration 10, loss = 16545321718263980617643557149062173339547275577725163611877110424117558936782898301623978071807550161944576.00000000\n",
      "Iteration 11, loss = 16543962795658913571388639396888133307339453905299763892261721504279285827121741524665759118743218881036288.00000000\n",
      "Iteration 12, loss = 16542603984666716370293592720803602013766286966952371156179972107982543626536423402461632066875066129842176.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 13, loss = 16541687107123162201844011836952188984629722035146685492889146044094602182288884940498828032121009101340672.00000000\n",
      "Iteration 14, loss = 16541402128715877965229646919538180397115822834327440939572034614515679212454992682450212302602851269476352.00000000\n",
      "Iteration 15, loss = 16541130398245635411266840348452377564475387133397429425176968150704143467308766270284117905290047833243648.00000000\n",
      "Iteration 16, loss = 16540858674582714868227140021764568627556266364749671856430066798670947880725125489278547319293385713385472.00000000\n",
      "Iteration 17, loss = 16540586955383869037741507235746262605176007823407121007792743438711826098721839733927139538082566873546752.00000000\n",
      "Iteration 18, loss = 16540315240648590697851834703361978654358197574207675063634975252700369722327019763286253490817751647780864.00000000\n",
      "Iteration 19, loss = 16540043530376800404155045379654352305720987651403053016689409269122803942063140756485198407367398883590144.00000000\n",
      "Iteration 20, loss = 16539771824568453341859659905929485653459233048673711734651384837381614762840316917135379494323972546232320.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21, loss = 16539500123223449696202837892369151543734201706489759695569612326600521013776529880553108348188869649235968.00000000\n",
      "Iteration 22, loss = 16539228426341728356105289304390761923175241343506365355392281758692003610658299015299392068860904690679808.00000000\n",
      "Iteration 23, loss = 16538956733923207840127960762550866053954815584597086655383646496206037107867650146122240122871830334668800.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 24, loss = 16538773396106007528285123839008337638331256108784157919297892578430528582345845526223911221486290431115264.00000000\n",
      "Iteration 25, loss = 16538716410150112098258354856736928693753870670021447111832912683528163229422695166383970182146779611398144.00000000\n",
      "Iteration 26, loss = 16538662072560289122203716126520403007290871432230127054410345980479367645691828480851668484338149786386432.00000000\n",
      "Iteration 27, loss = 16538607735617614142506756615536586100024335951801409042439904230831418157456804544822480988053629657153536.00000000\n",
      "Iteration 28, loss = 16538553398853560736466639185999390389837353614180811139504847433192999916804318241825641197524213093629952.00000000\n",
      "Iteration 29, loss = 16538499062268010755996736437715812306879996675434992360438342963261576027588307017745762839220941458767872.00000000\n",
      "Iteration 30, loss = 16538444725860996793672669722463232146283279685614529528734689476017156668055960542683642126531113686925312.00000000\n",
      "Iteration 31, loss = 16538390389632490330990770357436442149807564913425167923836375648352232932240987855301082372740843210539008.00000000\n",
      "Iteration 32, loss = 16538336053582511738310801687496305001909736452648518060427338139629311181547882499411081211217191863582720.00000000\n",
      "Iteration 33, loss = 16538281717711040645273000367781958018132910209502969423823640290485885054572150931200641008593097812082688.00000000\n",
      "Iteration 34, loss = 16538227382018099459273106077640350151379658687148293580177612426220711548858736048864059161572329073410048.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 35, loss = 16538190715962137723058425370680730926966118641275063455960711509092376168420434560931014213322992806526976.00000000\n",
      "Iteration 36, loss = 16538179319159934730806094361371581379143789132131621287175013389043375388920922306915434676174222738325504.00000000\n",
      "Iteration 37, loss = 16538168451982102995522031628891298199605932157388029579841833554321835442021427174708177625125445840142336.00000000\n",
      "Iteration 38, loss = 16538157584905137133642147309458683376775353951667062381489416218986794625612163586940001954420046046429184.00000000\n",
      "Iteration 39, loss = 16538146717835333490255055043105388408985223119560352146009128315508990479022830003821794175573587077890048.00000000\n",
      "Iteration 40, loss = 16538135850772663546857086147026205537995901929773644152843458520780914096287135464015357601872182366961664.00000000\n",
      "Iteration 41, loss = 16538124983717129340484216955707221032253078791685099453460800500738816113545529321901991996652538097041408.00000000\n",
      "Iteration 42, loss = 16538114116668755315568163482981470113105014617832650665481878246617704164483403830057294519955128468897792.00000000\n",
      "Iteration 43, loss = 16538103249627508879533304377071572485420694858165720965412393103437568070853569318380468958392654548172800.00000000\n",
      "Iteration 44, loss = 16538092382593404291631473979380132028319938378331437713531100732752162285639171267540613655322059618648064.00000000\n",
      "Iteration 45, loss = 16538081515566437477790719620934976204911368359573478806901213802688985536559310968775129084069931313528832.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 46, loss = 16538074182415544247297651135977119128229541717095381718566096464592899108369169453795334529594481279238144.00000000\n",
      "Iteration 47, loss = 16538071903070671492592723610580987187994175646924713026860709391784960592239412926637529946608063261704192.00000000\n",
      "Iteration 48, loss = 16538069729648705619935505893888510456569838306206070919271223414811649858183673362382078430250811952988160.00000000\n",
      "Iteration 49, loss = 16538067556245777885513110284158298618549375013780615835288939278036784492767599845754215058749550676148224.00000000\n",
      "Iteration 50, loss = 16538065382843147558343259509396681973599419490566674265692130367954512003857132068796117134407392175325184.00000000\n",
      "Iteration 51, loss = 16538063209440784082886308552312366495034645595891830438454891695521072849345529715788288207173743699558400.00000000\n",
      "Iteration 52, loss = 16538061036038715977645926095710559941094691061050339074134734583843975935199083748068924963762491816411136.00000000\n",
      "Iteration 53, loss = 16538058862636931020406254132674744701105425429773233863921297037305717444575098039350228824153399425499136.00000000\n",
      "Iteration 54, loss = 16538056689235423100059363659746661969729783473926031653409398058097545469052224526488300498336347976630272.00000000\n",
      "Iteration 55, loss = 16538054515834212586965018021787174431424649287290342957282974305581966370034956753296137619678399303778304.00000000\n",
      "Iteration 56, loss = 16538052342433289295943335546365850743961580822975362518200057450077726966821047947867241371496022489956352.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 57, loss = 16538050875805519648590335012619900392496328425093002567055383318668511980878426136196382582589664968835072.00000000\n",
      "Iteration 58, loss = 16538050419937169245472498394077506656208183844527414998630125146974119191086157013195069152359155958284288.00000000\n",
      "Iteration 59, loss = 16538049985253326885469055695776738297637462272238434894165875220741629056652514525047434855333057687191552.00000000\n",
      "Iteration 60, loss = 16538049550573230634626092117388617610687725546387628440077576951274058784505234744110065334509630683873280.00000000\n",
      "Iteration 61, loss = 16538049115893154754142891883861359608194872914318432500673215341168994873762448506985693447053265514528768.00000000\n",
      "Iteration 62, loss = 16538048681213093132911525991736705484821839147896363921547609392617685416002807750530419902953843628965888.00000000\n",
      "Iteration 63, loss = 16538048246533045770931994441014655240568624247121422702700759105620130411226312474744244702211365027184640.00000000\n",
      "Iteration 64, loss = 16538047811853004520060391893750863801652474574480964638259089816431327314871165262101968791479004975595520.00000000\n",
      "Iteration 65, loss = 16538047377172973454368671018917503704964766948731311831159388856923777399218264821366191697430175840993280.00000000\n",
      "Iteration 66, loss = 16538046942492952573856831816514574950505501369872464281401656227097480664267611152536913420064877623377920.00000000\n",
      "Iteration 67, loss = 16538046507812943915560850621028163806720366247282583040454285592888687746159653609995433722719816506146816.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 68, loss = 16538046214487487763817114569611114621820359417857841520708246731546875283712698257963650605100441804996608.00000000\n",
      "Iteration 69, loss = 16538046123313837646146115323866281305330476913650702311413453023383252959930648106300125599754060600180736.00000000\n",
      "Iteration 70, loss = 16538046036377094025984338064936380108653731193606471118435005762559012693957401732122455853056656383410176.00000000\n",
      "Iteration 71, loss = 16538045949441097998025875562400139431544631715347345814357033900338755891529068415881799250930421473476608.00000000\n",
      "Iteration 72, loss = 16538045862505091784887531387433467412207090190197415252937093708437245908398488327734643832120655646556160.00000000\n",
      "Iteration 73, loss = 16538045775569099831001021553869399271989367530694612051795909178089490378251053720256586756667833103417344.00000000\n",
      "Iteration 74, loss = 16538045688633099728970606382360986057988891233679164644781149983996732303541821695253330627868185826689024.00000000\n",
      "Iteration 75, loss = 16538045601697103701012143879824745380879791755420039340703178121776475501113488379012674025741950916755456.00000000\n",
      "Iteration 76, loss = 16538045514761109710089657711774590972216380686539075088093599925492469334825604417153317186952422190219264.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 77, loss = 16538045427825115719167171543724436563552969617658110835484021729208463168537720455293960348162893463683072.00000000\n",
      "Iteration 78, loss = 16538045340889123765280661710160368423335246958155307634342837198860707638390285847815903272710070920544256.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 79, loss = 16538045282224041905297405638512955421205412275409900167148240289899098072146961807563525560535044423942144.00000000\n",
      "Iteration 80, loss = 16538045263989304548433690985214078191502957500807092540003064350895871317284934101458141411453625922748416.00000000\n",
      "Iteration 81, loss = 16538045246601953787365359198942011683721919947420085249938981232794772627949835472241307698777438895996928.00000000\n",
      "Iteration 82, loss = 16538045229214757841031228833612501577813201506773317871472816725848722285288887776003255999690921807446016.00000000\n",
      "Iteration 83, loss = 16538045211827557820625145799310818935013106247370228390069864887030170670347041371002604773930992352100352.00000000\n",
      "Iteration 84, loss = 16538045194440357800219062765009136292213010987967138908666913048211619055405194966001953548171062896754688.00000000\n",
      "Iteration 85, loss = 16538045177053153705741027061735281112521538909807727324327173877520566168182449852238702795737721074614272.00000000\n",
      "Iteration 86, loss = 16538045159665959796442873030891857275058508878539120997329403036510766461661951510381950859987910169460736.00000000\n",
      "Iteration 87, loss = 16538045142278759776036789996590174632258413619136031515926451197692214846720105105381299634227980714115072.00000000\n",
      "Iteration 88, loss = 16538045124891563829702659631260664526349695178489264137460286690746164504059157409143247935141463625564160.00000000\n",
      "Iteration 89, loss = 16538045107504357698188647593500723078212534690951691501652153854118860980695962940998697419371415620026368.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 90, loss = 16538045095771342141006386912965674985164843118153874428800591938701039321903477874700741782271092794064896.00000000\n",
      "Iteration 91, loss = 16538045092124397928891206117483637568737453618238370585720986616398394988755791300489744573793538987261952.00000000\n",
      "Iteration 92, loss = 16538045088646926961863149226434789759802970743809704707120812526403674996432591832893857925923619108552704.00000000\n",
      "Iteration 93, loss = 16538045085169490624446690021649408514445190828809776703483330757325215818497031389780067254777704347598848.00000000\n",
      "Iteration 94, loss = 16538045081692050212958278147891854732196034095053526596909061656374255368280572237903677056958377219850240.00000000\n",
      "Iteration 95, loss = 16538045078214617949613771612078646023729630998809920696208367219168297462625910503552485912485874825691136.00000000\n",
      "Iteration 96, loss = 16538045074737175501089383403835005973034785855675509538165704452281086376269001997294795951329841514545152.00000000\n",
      "Iteration 97, loss = 16538045071259733052564995195591365922339940712541098380123041685393875289912093491037105990173808203399168.00000000\n",
      "Iteration 98, loss = 16538045067782302826256464994264243482319226025675653530890740914124168020397881111067214609038011992637440.00000000\n",
      "Iteration 99, loss = 16538045064304860377732076786020603431624380882541242372848078147236956934040972604809524647881978681491456.00000000\n",
      "Iteration 100, loss = 16538045060827422003279641246749135917820912558163153317742202712222247119964962807314434213399357737140224.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 42397406490248008475968939515052086292980309676017638592527761707837477827751179946594127363369766106576869813387576950800266683169030508784998051311884184438652610101756105927060828190827059261918038384983344744021679733006766513865097216.00000000\n",
      "Iteration 2, loss = 80040877383965245190838632331299814525101019939164030169040529175384642688328105845345527705986361755478036231174286851234683541359256512784371225572650476607786326924713225721388065330169086745458715440139189316553728893657513182938267648.00000000\n",
      "Iteration 3, loss = 80174744222320357296436098616242340061966919453484116001707172110433059329064238692007347647291817217240146389381199289007160736031993692455766330800177177028906751601053854512582386148603975650067084807472904520521575463158188109320945664.00000000\n",
      "Iteration 4, loss = 80167981225827025811116493076927531741692302196082250671797927145850058611079858692632527673071683566626616879315298958312182685185646297790513704977559515503186822827262746447178479149286258324717361300460739621365038567302431381122973696.00000000\n",
      "Iteration 5, loss = 80161069374980934790137521214842121867590530563238340175487254705342586308547863458711962734937749881035715613251727382137817723325807066655713935904722245835076287131667545286950302984954541523257277312602566364688646861549894138358398976.00000000\n",
      "Iteration 6, loss = 80154157961498230176666500842982583368455129738399469656277445501949707529418729289575667175449912208707754363284557602367498880935731586106943619630652575730129203246320705045405172861916711447800555903122695746645735692115809819810594816.00000000\n",
      "Iteration 7, loss = 80147247143742914496949726403755938683889744860603956161702876772268918519583945917596187246879115363343286708889088788102128533520021116423532562500858136049823339983633583974640788197756575132301758138437934878308187163273170304861470720.00000000\n",
      "Iteration 8, loss = 80140336921831688840080449814276431128004850130994488703953431153188409608205133691924812832527453627648932555865264674903996600019764163224239818180703321982832212041407010190694654520038245616036419935158659390523108189640793741974306816.00000000\n",
      "Iteration 9, loss = 80133427295713588293922198882373840060303790843432234119414417109558080033037659917709667311735889692713465092343271396296004321107531253245486004129837699868605169626944933670842889446518935501735435212544923299594160871941198441969876992.00000000\n",
      "Iteration 10, loss = 80126518265337034886796233366357967152149575773524760025753127070966150561478524779620628611985438683267896493963712667525227645979643761593421717999776797693434689493394951543699702795078674520578344593833354126787352769666374679008378880.00000000\n",
      "Iteration 11, loss = 80119609830650739826053184746010301663650655450772841219277427068539369541282448802175803647483506085532748847918882023780169548113672663199796623389269180402649509457185959451399072213728239041268911955101818306122302074964222178504998912.00000000\n",
      "Iteration 12, loss = 80112701991603298647431935812523657819417303703119969624838953692789074288061064373954007737746941241132740276780397072272762191274689093066120757518170198157963862909434333630667068218426106777501211871291686706517182726920276885372928000.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 13, loss = 80107406155442769543408260578882696430131299465020913980922441759770544159962234787257491209527611405355109592365599489775753000725200594585575650416589785371210800278908105765600383077493358422765779350709994791092018063040605698637430784.00000000\n",
      "Iteration 14, loss = 80105775761198418793561000753187762982026829662220346899022035156441347485295978949912142397318438426382082271325505149562030207037453995715567744967291677003705781737326042244672859394464237632273955095960433971259426915877757759608848384.00000000\n",
      "Iteration 15, loss = 80104394127453766040797992017141870532557248153871345896220856641322431142552389271209976058044227236839342369311553694534156589791400291316536979696690824475544074546376741031788832147149838933022268249678169767856868046809345094955565056.00000000\n",
      "Iteration 16, loss = 80103012781467602730193160148884347578833123368021497687358702344044068977311867978242320033349605005058825443520607371859764304877137970696649813551256893595439702275952645653309028338603642014031053245233923277270382078915071830195699712.00000000\n",
      "Iteration 17, loss = 80101631459581897339295792583683290755420392125795396275395450925131240301627881162932717613285044056064266750648877194857345562614524731335493793116466844278007154489696713585259164743407733135828984352368255028837487861864139229407215616.00000000\n",
      "Iteration 18, loss = 80100250161516389119999991740037956553786724224052372409032790717504555336012184257971564022102061648682240411260657269093485510587698406255478164971950577303663659097905677464821051112047701075439853847466060338268904650855343514305167360.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19, loss = 80098868887270534415730538781581572307090689165473196592426443350271582229392263024444188764755043903312467791638770733078301343147909747784476685136909983189421045301610696086577523923877742154318122015185810049587844456495574916530176000.00000000\n",
      "Iteration 20, loss = 80097487636843835838556914347382835362690127133361552478314622228786053543552838477811637984074599398192991843914311096512538578531409444222458960182104238365736944071967936007274582511072972155922124860640573232858096990616669179923660800.00000000\n",
      "Iteration 21, loss = 80096106410235703463259200125639503039544336949775297422270557205909373014564161124783524547138891795885212348521831126715686087204448311925202895575177947436177382839883548259331900500166667939702446948034222506062296562600571023925444608.00000000\n",
      "Iteration 22, loss = 80094725207445963782419773083468562784406053562378507117109904120718424094211598761450911061912086879495423358530316931722887654598276590997349051747791288225320604967388753734637617193080381523144554325662014521548276792853734777782534144.00000000\n",
      "Iteration 23, loss = 80093344028474026870818714909067771916234575793229038918405892826074610518565401894523410395472348311585024654374321278853632151179145097794675334167588865336732637885389703465611406216347389765699031557727821897298671990758105160934752256.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 24, loss = 80092285097542859901559004782055652952863444198900904409076265692612035190332223833755452453973575926111572542318963721072468001017497245803573001398537873349632090443258244535167909401324810838940183973237095497546825544251021813086158848.00000000\n",
      "Iteration 25, loss = 80091959078999800954570643801565653365459343253372043035157475202703283370113761905340724353863728473258816340140428778937021969036494980207646711250221262319025439348058911102834165806011051140393848118341433005906840689199926550451978240.00000000\n",
      "Iteration 26, loss = 80091682798302258368874855643349448972860605465930990499854225711415048545028421002341908328770362654205194200262887672100506027311806981403721058117200666734728721207151016397870847712741795463965326166722320458092928874105545001271296000.00000000\n",
      "Iteration 27, loss = 80091406571334005933027293891804220930092475984047805075941797917628127636167591696379888674970327369199432417422877790440245327194016819010888961314107237912315804093538154557658507214119343805702648165588454183482398147393660958256660480.00000000\n",
      "Iteration 28, loss = 80091130345374375250144772026928699920775731071204073888913223458040783705392381340258787244412706116225500937490757086084474077161080896328527000546178757773297124377500752052773986071925474235427769284354966923935320848575233758492360704.00000000\n",
      "Iteration 29, loss = 80090854120367508498913848329251711302840841545387898312555305459471029330805529088894692961027936669969665245142119939147148267046826511043919597445881807310228495075623188011738930218105166248961726004208150291964267173834793797564760064.00000000\n",
      "Iteration 30, loss = 80090577896313290007722774110184580040789630705041995475411814481303453480263946806348314230125462885836121375756290421651697087138753823226827125634323171739495411762592943029145432520606121191294829024011508722467792870109977294971863040.00000000\n",
      "Iteration 31, loss = 80090301673211754478055073976303908645271551560633550238919619355722679463410560933401438530112451607203610518719471311991091780350612784856321473025170815496182225766003772926615865119044028658929985134105990884976331213319493384864268288.00000000\n",
      "Iteration 32, loss = 80090025451062855641266048452174227102087333431539649454496228306482542867388136215678349223112680376233811088183392238974704022797403459960306789066867452666843135315730669941987065160597969189863318614036998553449304502238396555041177600.00000000\n",
      "Iteration 33, loss = 80089749229866639766000397013231005425436246998383206270724133109829208105053907907554762947002371650765044669996323573793162138364125784510878924310970369164923942181898641837422195498088862246098705184259129953927290438091632317703389184.00000000\n",
      "Iteration 34, loss = 80089473009623129986580469397191978622417927601475677261894579653885757382836493636218538020719636659716472057082400502041780288993279726494085804033258207947147547249570192494002837557927167558638082704999684199430577871491673428951302144.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 35, loss = 80089261232887635108321745896251941643664420423572807865204762127431575489705584536475179921972864199620347751388394127286196145083621947651502504939976253866633109866473134243123971547649075787833028017403263604498276502049440594411913216.00000000\n",
      "Iteration 36, loss = 80089196031590970752191968570857405922804764562431140676851306321003735843049914553399993479126643854483409512608521009361128404626083771111329819150381770859401414847987902912606210100861142540561967568265976020653382226468816159735021568.00000000\n",
      "Iteration 37, loss = 80089140777293359144733053680981092799501724252336733803060154062243659208496779002109938329178739161957115721966151922919113673272347300521235332829883258628103287683709075984422763716984924452063849824929284403938163501999598673424547840.00000000\n",
      "Iteration 38, loss = 80089085533588722397995542364787180528224659155093129147848714481595084173668956499265541750981444263966293963583915074878710793228951961415645372792208200245891421789332357501558656418514934840892551559583453647284751722680671998859804672.00000000\n",
      "Iteration 39, loss = 80089030289933408026507671862804303393370139601874940881573508379357773244653915160935081148837289421626282719457893916048101174595488368564232100714599925596904242848213913484626150227145091725852771298839331849887169449153661330345951232.00000000\n",
      "Iteration 40, loss = 80088975046316195883929819381930982951615025541625730472980280015836856303572105801007271837762328609144097421382523432699916275259472048733751757844416600670534821605039361609058253333802425571004646832456849196905324134369279959189422080.00000000\n",
      "Iteration 41, loss = 80088919802737005000133760840155146678110593283255399912049668782601545627923366724324609701473172525302675294123330475250556528422153113973036605716432975118253004962089938291069430746050327317841395649640458792768204801183873239038820352.00000000\n",
      "Iteration 42, loss = 80088864559196020449698294139218674629653925549255601793111641784636498869136638948389961291474711001455302681073396528462535229623531308170470046536878192593841999999865674579312333875172474814378520632208553541638122254497223218697338880.00000000\n",
      "Iteration 43, loss = 80088809315693103426689320852815156763647210297757596670418723693203222788640216710076176694138276664087014824687910478763967407208606823409764528651080395356964401407991547187297474162706109674604394618797338765393343390634493361562583040.00000000\n",
      "Iteration 44, loss = 80088754072228346468396239931815533108488988890007210841135898060794046212148570518134689185216314428874454896663413068536109708947379531635111753162154155234511812726717571639351177314293070822526769050316010916115023612045574692036149248.00000000\n",
      "Iteration 45, loss = 80088698828801680171852002163066598642880355305070074582389427223039722520375847491001923807894490609060140518227498740993019649012349528888367944242763543356315531300856236292228699050701979065139830346082672654742296366893048941816840192.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 46, loss = 80088656473832535874156862452602496236712062673880137528852036339792125448647288089969752007643527460938453975854878610927492139629394948379225684930108593704933563444387731424281321686891169982038417086039917593422904089217007889569284096.00000000\n",
      "Iteration 47, loss = 80088643433669751783825302378718867722158260757484671238266934671377837828508959399162622975387686163136719517684687550826602042367253706043602493706778212690841778160544382464049694378946224696071670868822578355527414380194683312039526400.00000000\n",
      "Iteration 48, loss = 80088632382883852129779324713663491691377159702645191887969618582522636240031069051544926134043280136968768161324028372065988061908404530724500994080631855719541929350600968232437736510818029018939093707511937307951608824434138828687540224.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 49, loss = 80088621334210532524066695956733493985096065209397163665664314117288361300057658244773198083140076550726105051140247386204014346661631905329839828008603020770105102678386966608678475593051548976768036782701699047113063498460735921580933120.00000000\n",
      "Iteration 50, loss = 80088610285540995380058249316653169939605348857072285339977712360178027111163229483216305178618058892766231584052569245175906109013604045654014444526382309244862570713892349541757777893395234953413820005054865765091745247626628636889579520.00000000\n",
      "Iteration 51, loss = 80088599236872892564035486415072416334292023604057714620348355666698789720843093607306628048258937427794327278261272611057275911800574201113160428142437461036439893623272973101895128631187424252179750561500577490328335734765832230422970368.00000000\n",
      "Iteration 52, loss = 80088588188206328180448981071721040701104448481755006091086850533404519058026029939389529127284212685946615701924965819027037483763792227644493442597772369450090125389310104753957644225275185662574548823061680231414134787634474104632901632.00000000\n",
      "Iteration 53, loss = 80088577139541244393492858942304705522293535139385518316465082239987509606640494411495362618348606594925194872733310905096905420047008205282893674702940427094006013799347484795241371109632369857093370139169926205798420279701372369268375552.00000000\n",
      "Iteration 54, loss = 80088566090877733740456518977694350826257824938195077593648034338940090192400957532375561797204564070406707962382848611648136368420222005972552825561056205795079162393635128750072634989900815759743965950734511865562347613416418048730988544.00000000\n",
      "Iteration 55, loss = 80088555042215819355662310915607711620096953218495140496926952718385343021736036929217984982790196341310315763797714124276044490825933597699518820447898348510032472057235540499533017292490983101528274117982736323726205639392083899121139712.00000000\n",
      "Iteration 56, loss = 80088543993555350866014961460879510357663290268261239293408739105523233752859718025301553102007780419661473122971028736611087734637893188554480145070905676020167087037242444816592238745734883630933698549437155345658116828647297005786234880.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 57, loss = 80088535522576612524944667432065235007521764227186375292880953751560237591901284359735100185287542414005721038909886274421410067854031389751713350598783486760238940792220025472518327720515604346877482123695815756528655627628067599627059200.00000000\n",
      "Iteration 58, loss = 80088532914547972347652166012901046006579266958636880062271862277115197616238582904478087775058605210459296389331934983687919665266847721322502461543441663130607703576533262747583737750217447748312212616734087743284460094515240291918675968.00000000\n",
      "Iteration 59, loss = 80088530704393748983239266960216504707756443239473134386633623561474063280120337589562841567080339061094455453764279867016946765426573794875607011862722961605534466925532575903487452471848562234933355721520786177762214091637149624879284224.00000000\n",
      "Iteration 60, loss = 80088528494661773270353780731631312991477850885014782674815334868329360679131023707437632617508223144252406969512032236020057640095715549775443695693795506186848184435352726560415362037190686968924323694970895458552069467611803302324338688.00000000\n",
      "Iteration 61, loss = 80088526284930294945398813863977423927841418347252747310258832769830925516356988809851377525105498649172315533128691095322422996528606616375310772954108874337704270974016710660597272270357695919456955663307935669278135131754621235927973888.00000000\n",
      "Iteration 62, loss = 80088524075198897590572071078335607389053709500580809955721691279763278076083115607422626548986163455309286871979823104208387919760247570926345588679647492837090510610399458344564717495961313928496370142440522775575211773041093815883005952.00000000\n",
      "Iteration 63, loss = 80088521865467546504390027768129260864465271334531785749767041565941795048666477659369592210743050719284579796679225484284981166876888461449476254956743397249922552016907213790695325574385851399539660342027708107912866115552511908038836224.00000000\n",
      "Iteration 64, loss = 80088519655736264821175033671076119361175739189417131266686129516489558640535692592880132829314271670017355100151033421147516899821029255930750697061175230532923296078602480880070677932041768063588764122296790779311387009901348268495863808.00000000\n",
      "Iteration 65, loss = 80088517446005006272282389311740712864985842384613933357896463355160404438833525153578531766823603849669291196546976543605366794707670018398073064441385706772646941025360251850527611716108144458639805762793172563730196754862657385053290496.00000000\n",
      "Iteration 66, loss = 80088515236273828693517969034417378893644669270900833459126157802262037959631519409434434820616325330538290068177392815646816256393060668816563170286821433360900739069836786404770080492611129912197629914085101243720017476967621147962114048.00000000\n",
      "Iteration 67, loss = 80088513026542674249075898494811779929403131497499190134647098137486753686858131292478196193347158040326449732731944273283579880020951287221101201408035802905877437999375824840094130695524575096757391925604329036730127049685057666971336704.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 68, loss = 80088511332347493371759408263133432533315892126914903404677065325709668512167576425467434423986835547714738742561027827930841314255428143118722011770188117493602880434402686057778093437536982649493626216024889387901311649486794310199279616.00000000\n",
      "Iteration 69, loss = 80088510810741941157150765985955380787084621259572074323168715780556085285892530101043755165870693446791075838868864980308530864500991166126844066055037439238770679717740363009011194284196845285395300052360017044206667807519021795020636160.00000000\n",
      "Iteration 70, loss = 80088510368711239917066754549268429571337795625670355948646792543790968098526310326625427501691329836216904567884972107665284088576436182350962112828721285265438017874927749702897742072267918514931543406726611231828009480740734749435232256.00000000\n",
      "Iteration 71, loss = 80088509926764967386398110913455386765710144458429405450026738012214363272400120974296034802148896166120047073532439065121071321805514363656983453556562601852332136067723050404015705545311780036040007660621318386993511464181771092256358400.00000000\n",
      "Iteration 72, loss = 80088509484818717990051817015360078967182128631499911525697929368760840652702549249154500421544573724942350372104041208172172716977092512949052719560182561395949155145580854986215250444766101288150409774743324655179302298235280191177883648.00000000\n",
      "Iteration 73, loss = 80088509042872572698156096936994578700602471835971972185679727221861187961933756846358328476161751813900877238834251686402187840889920518178337649304806414244819228206219927033281911763067491329769532259888176931956392960044916692551204864.00000000\n",
      "Iteration 74, loss = 80088508600926369570454502514334740916273726689665391409933410354653829755093420375592510733433651830561502123254124200643917559946498603442502765859983659701882049054202739377644619515342732043883810094464781426182761495323371303673528320.00000000\n",
      "Iteration 75, loss = 80088508158980143308430558353957168124845346203047354059895847599323389341824466277638834671767440618302966214749861529290333117060576720720619957139382262202221969017123047840925745841207513026996150068814086807388841179989353158695452672.00000000\n",
      "Iteration 76, loss = 80088507717033951747890138800156197844066418726896501571295153676177572238198438620466946088508396249423171495631801636329719917088404789977809036332448829137646240307637112125829244306687983606611396833504340858125354140574044147867975680.00000000\n",
      "Iteration 77, loss = 80088507275087748620188544377496360059737673580589920795548836808970214031358102149701128345780296266083796380051674150571449636144982875241974152887626074594709061155619924470191952058963224320725674668080945352351722675852498758990299136.00000000\n",
      "Iteration 78, loss = 80088506833141591761131649430271992289608199114906253168385011718009020237375000933311027240928418740582742850319817036003807679086560896478235119994360605965217683773727744576717822664059384496843828223112148072618668912355898882313420800.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 79, loss = 80088506494302622675203165623317754330979693727692619887835618231210541601079881078753664011976876805925966951765625785159671035566706174817298265366549133457259184827414378075391201349052199227296694875855427570611743499092417203650166784.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 80, loss = 80088506389981577008384016433491802001612418507096132479549436808924455133825001169994931453380359826714884591214771735302088599054818689779856866995699198085116867162256924332666249512333459001282455651758890618329623512413786417695555584.00000000\n",
      "Iteration 81, loss = 80088506301575476088715208700274561270532433458845264980940170171380671447280407181330625062739276193762623684989022976285473319172157638600961949319259660316879266298300658269282247494845455189892998685018617947988382893099332693949153280.00000000\n",
      "Iteration 82, loss = 80088506213186216955717010025568405707986976157334783566357910087440734040769445785427174859043167213959420027533689330657567933429473281264956632409672195042913509759847217633289523904172135547914303963752099556417425519665045411293298688.00000000\n",
      "Iteration 83, loss = 80088506124797015658524685695156587663190607206602943587503764723808502150330028457493370452692336306454118352388693649017947952543038843894071128689531337160755005434050036700000753879524965233440453893053828947397190272761940018888441856.00000000\n",
      "Iteration 84, loss = 80088506036407733391204137282732697093545514564781005598630258751745482537390449434402061930058116097731753902009224817794728404857854518572017886504165228930066348010534092182926448862441185860459821311560011442805944048715179980132188160.00000000\n",
      "Iteration 85, loss = 80088505948018485825367113476885409034549874933426252471193621611867086234093796852092540885831062732388130641015958764964480100086420145229036532232467085134462041914611903487474515984973096083982095520407142607745131100587129075526533120.00000000\n",
      "Iteration 86, loss = 80088505859629238259530089671038120975554235302071499343756984471988689930797144269783019841604009367044507380022692712134231795314985771886055177960768941338857735818689714792022583107505006307504369729254273772684318152459078170920878080.00000000\n",
      "Iteration 87, loss = 80088505771240002260854240734049700420108413340872474503465970276171834730714800501067427956846011616160464515491494252101640571514801382536097786326960119021614880165298778037111440943242146396527612868215054494133649629637263644365422592.00000000\n",
      "Iteration 88, loss = 80088505682850731560694867190484677354013138369206264801738087248170356220989530291570048593680847021897680461574093013676078104800867041207068506779483332269287673184314085460577926639363596889047949216834886546052547830896739983659368448.00000000\n",
      "Iteration 89, loss = 80088505594461483994857843384637389295017498737851511674301450108291959917692877709260527549453793656554057200580826960845829800029432667864087152507785188473683367088391896765125993761895507112570223425682017710991734882768689079053713408.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 90, loss = 80088505526693731819452376151138464716071141272969406851915814009553812162005365467287199877752085481677191448133432044748567962821961665906786047078624451294192888892241730450807516066432897574464284904639812014026869731218443704301780992.00000000\n",
      "Iteration 91, loss = 80088505505829534253249721182032141753747503899005837657404200669158135971768698299129382525501837700294555372485328827574708556490834152892321730042343785698125875801741491642803316412294379394762405989934154180080590159188953925161058304.00000000\n",
      "Iteration 92, loss = 80088505488148272427535730107496770594752163276795042323958104743027831262888267772458376273285020761649613763976735741699820009017802000281656481010654320822377134035837731444179669441257951116681026448176961242575822104223612219431059456.00000000\n",
      "Iteration 93, loss = 80088505470470422914368325346327312982953035350524091698470777315052152002228937255996472064439610088580889111778082531133770348063515125613060210156314692063256272816653293705089282865764333161385481289946387475563659514598002038509928448.00000000\n",
      "Iteration 94, loss = 80088505452792469296750346765428047839205548392851586488672843390522602812640827417189205420372698885375940891420820985388806958367978395007248275560971169998882357614687588501131779871423646416581215760692967699960197097216264455137001472.00000000\n",
      "Iteration 95, loss = 80088505435114573514938242528823120213207149785957722714603024186300759139124261646351584573651065754468894653373897403632128973528691584366556154155074255326315694625378142999878230443109108999281794882007795706907456806365708762015072256.00000000\n",
      "Iteration 96, loss = 80088505417436700867448488029935927594308386519375315514824450870201997672036313502701822045867543852481009208251109007470765150631904741711911958024955983610471932521131201379706262441205031312984311863549922826875005366127625824993542144.00000000\n",
      "Iteration 97, loss = 80088505399758770384152859186754397457660534902014266879317762833795530688876821291082413720738743878195221780817982647321115922878867979092147948705391104502820918204228000056830340873274804299181984194523802164291831799358360997721014272.00000000\n",
      "Iteration 98, loss = 80088505382080839900857230343572867321012683284653218243811074797389063705717329079463005395609943903909434353384856287171466695125831216472383939385826225395169903887324798733954419305344577285379656525497681501708658232589096170448486400.00000000\n",
      "Iteration 99, loss = 80088505364402920984722776369250204687914649337447897895450009705044137825772145681437526229950199544083227322413797519819474548344044437845643892704150667765880340012952849351619288450619580137078297786585210395635629091126067721226158080.00000000\n",
      "Iteration 100, loss = 80088505346725059904394196739221879572565703741021218982817059333006917461898506351381692861635733256554922273753076716455767806418507579184023659211921717528398028351237159671988111161920732316281783698240987072113322076194221162254827520.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 15667775124113465482291039917519949286827180159163128845299263108252472697725269007876664671740373808185344.00000000\n",
      "Iteration 2, loss = 17761003680730690181433474305616801171381305549923112118180183438011597651636810247883939273395046858096640.00000000\n",
      "Iteration 3, loss = 17758451685393522527155629222476546665509754956461888929664125687692807377955592942880289686785461609037824.00000000\n",
      "Iteration 4, loss = 17755894638424198079954516696457664208491579882822532870186814860677239195551505741723090161424580520968192.00000000\n",
      "Iteration 5, loss = 17753337959634826362657896171134735442735873584545507657703946463664591527997370716064661655035367695319040.00000000\n",
      "Iteration 6, loss = 17750781648982715175273749487111746283504952314131496617621069804714032143755618742624564156929631489884160.00000000\n",
      "Iteration 7, loss = 17748225706414854730589924312965732968649338823695457387929816224775256759912700756839056356002361354747904.00000000\n",
      "Iteration 8, loss = 17745670131878247463610126324190249346693686321621313915432179060415464910397763820432195521168783840378880.00000000\n",
      "Iteration 9, loss = 17743114925319897846374037530764935534608336425671151198398545314138106765280404349509338684680831680643072.00000000\n",
      "Iteration 10, loss = 17740560086686832758317079622016380602266203256766825801251632313745389492175161658370140275494205626777600.00000000\n",
      "Iteration 11, loss = 17738005615926054634442958273438138398191940023292261670793433395804512625011182808738657562524132229251072.00000000\n",
      "Iteration 12, loss = 17735451512984600539366976846787229334486902893060120628788634217798936512105253886821043791409843156287488.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 13, loss = 17733671622581330313717413641290949205948023766598629543496381772849319684418386723371660159184522365108224.00000000\n",
      "Iteration 14, loss = 17733134239502202076588166162706534460650688855580236131668390912393864267913160213403284076286387198885888.00000000\n",
      "Iteration 15, loss = 17732623531978319569126215775005295550827822715469271735655502725007826971737972113195153160664294879133696.00000000\n",
      "Iteration 16, loss = 17732112839213443080223712800671759314390830084481987042195807096188339702045360893126517635800781803225088.00000000\n",
      "Iteration 17, loss = 17731602161156335043967915911178015536939149873733454466783424731423151618112715800364430293525216976961536.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18, loss = 17731091497806551386515984188557257697312708838784564789308536456609624041322077579785542726435652419715072.00000000\n",
      "Iteration 19, loss = 17730580849163682663636674401106145837928136694624946664624015418561379107443126000748602503854145268613120.00000000\n",
      "Iteration 20, loss = 17730070215227297023703003637774391048299490653084457181430404438793281954700597934418059798398984643411968.00000000\n",
      "Iteration 21, loss = 17729559595996964652123965321997790686386516335371114479896640004756448357459679606339664546025165847265280.00000000\n",
      "Iteration 22, loss = 17729048991472276104668316222073004794605843456474547214875595263264500451490050785872164316054746017300480.00000000\n",
      "Iteration 23, loss = 17728538401652807677852978764894089536254282865737256680941387699577305919578245761705208334452839006863360.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 24, loss = 17728182551468546813723918733318132422083476217501931885495677249484300874668016957775540138202845981704192.00000000\n",
      "Iteration 25, loss = 17728075107050026660137145535641896002992864936870630682035012218812769143191539466702834792083679407505408.00000000\n",
      "Iteration 26, loss = 17727972993598903032342421474767863060042841230968314647308332904220860978831349972494448964510455399710720.00000000\n",
      "Iteration 27, loss = 17727870880746099648552639001638684253138108487908196960637606051102050623395978708033850157614125052592128.00000000\n",
      "Iteration 28, loss = 17727768768481480217749557713489087796533141641960885515295949960673172642009438271973416007944719780806656.00000000\n",
      "Iteration 29, loss = 17727666656805008073285603589569520858205549324319481384852278646081715584143640285449750775441528283201536.00000000\n",
      "Iteration 30, loss = 17727564545716705622556516309226932391057904038143756135458922432626436447343527646657151856808318577147904.00000000\n",
      "Iteration 31, loss = 17727462435216556569274485196572632247524698508408421355368731992817330142485505520545221145351441195466752.00000000\n",
      "Iteration 32, loss = 17727360325304571098619391924037051769834374782004282301923675656335649850271820679020457457754427055144960.00000000\n",
      "Iteration 33, loss = 17727258215980747173555260157134104689541244449553177923655359757245144934562023767701561030680569972785152.00000000\n",
      "Iteration 34, loss = 17727156107245056275578421213058583248405669779760853500006272972438306489389823825250335177415983380824064.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 35, loss = 17727084942322929698729260748919550738512623201467341411528581771871219287600685633704751578275890721718272.00000000\n",
      "Iteration 36, loss = 17727063454726886627145147859769527431791141433520108418888849984983033095625438543726849771780896236503040.00000000\n",
      "Iteration 37, loss = 17727043033158615687745368091423048315193601813375986450282693691943023372711728326653703765289848201543680.00000000\n",
      "Iteration 38, loss = 17727022611615905475776633454487065655094223070396738307080257566975995940156516886129988107787989436858368.00000000\n",
      "Iteration 39, loss = 17727002190096710807118704124930965932021842188908668315014301010086312112948652423330740409222311811481600.00000000\n",
      "Iteration 40, loss = 17726981768601062237311225120046043172661785309584192246110729010317731433194875253975457119643408076374016.00000000\n",
      "Iteration 41, loss = 17726961347128925136742598753568830813437349472994572225406849246753993086507546353582042262327273113780224.00000000\n",
      "Iteration 42, loss = 17726940925680319875772588370360191538805418772921418767586598378757603434291159265963493470640968757673984.00000000\n",
      "Iteration 43, loss = 17726920504255250528473146639392297885657370028121053975586763738201063748826612699882410271257907374850048.00000000\n",
      "Iteration 44, loss = 17726900082853688576340604877859942095753565507299223128849834001976865124147615694000595977464202397745152.00000000\n",
      "Iteration 45, loss = 17726879661475664574914608103054418195779331351128341999401714159128767102360908564037547039310446577319936.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 46, loss = 17726865428695823671534391650637158738557328066691625076006190694913738774068819890178781656892521484124160.00000000\n",
      "Iteration 47, loss = 17726861131228108067441742745890282354702919338008501916120689340454182148408730264431138826187108979310592.00000000\n",
      "Iteration 48, loss = 17726857046959327745084459186215394121453381605489480319643524991511443741784815886668996169189413433114624.00000000\n",
      "Iteration 49, loss = 17726852962691910199795343397732219478369389746960202155521723153920380913121519589996395184448154579763200.00000000\n",
      "Iteration 50, loss = 17726848878425417468839483465932210709627935746116041358050645651387106892222230182433886754571502988820480.00000000\n",
      "Iteration 51, loss = 17726844794159886218864453411564920647251410971763896853661378470764133129615036042844866619620169961439232.00000000\n",
      "Iteration 52, loss = 17726840709895267561006821206964278848543293598827903407112473629581444357929152666078140459513207096082432.00000000\n",
      "Iteration 53, loss = 17726836625631598161914160872879838145525974996114959944835017114691552027692668430997104014311325693902848.00000000\n",
      "Iteration 54, loss = 17726832541368880058622448743797684806645143573003227518297402592030706775046032691983057047351231938297856.00000000\n",
      "Iteration 55, loss = 17726828457107096954843874143829128684335292054467417715752480734108903510865650613985601451939276362088448.00000000\n",
      "Iteration 56, loss = 17726824372846261072794295079890687389270550896776496846010613536543646051994218323292535808095696065658880.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 57, loss = 17726821526298465887862501014482561755617186346946924162820575313125442709046700237171693191144660073971712.00000000\n",
      "Iteration 58, loss = 17726820666806981395601654865171969370059011118779858144802113837408424267452799849763705453078846514397184.00000000\n",
      "Iteration 59, loss = 17726819849955021182046934636170645985128005280062836800042536857018437407548167796765148279319478688284672.00000000\n",
      "Iteration 60, loss = 17726819033103179116578841807362326170046927185279156440449792500930987443789598297881977379089069499219968.00000000\n",
      "Iteration 61, loss = 17726818216251396125154062678650508139890812962462146573440464456994805928104060076056499615623139628679168.00000000\n",
      "Iteration 62, loss = 17726817399399623318909165222369121451963140786535941963773104742739877593120768626137520668840740675125248.00000000\n",
      "Iteration 63, loss = 17726816582547891253383794455809460132949236798172958383473618347209961980946464263844536988792465389518848.00000000\n",
      "Iteration 64, loss = 17726815765696218261901737389346300598860296681776645295757548263831314816845191178609246445508669422436352.00000000\n",
      "Iteration 65, loss = 17726814948844561566707490998771831212336863840405620619788627507942672741867512928424354008918522922532864.00000000\n",
      "Iteration 66, loss = 17726814131992937464088865959974742120944445549085172767314005407034040845137024348340257785715675356987392.00000000\n",
      "Iteration 67, loss = 17726813315141347991081838607441119593128730217193462789802075627041669762794174792738257539236832909197312.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 68, loss = 17726812745832128324289137119741466789449746309629179427798452727337385075203533615438629587741875864862720.00000000\n",
      "Iteration 69, loss = 17726812573933920240605536073472709616570125912883588068216724267014509122608345388981701721609102749073408.00000000\n",
      "Iteration 70, loss = 17726812410563598271932177933993812574115606027748923969777550979143533633858876769098702145639921892720640.00000000\n",
      "Iteration 71, loss = 17726812247193298710654559473861864484563658645774031437490708016571315142654351047409999966374509053739008.00000000\n",
      "Iteration 72, loss = 17726812083822986927161083006813398784337580807530172596393503058381592834607129199433499207088859114373120.00000000\n",
      "Iteration 73, loss = 17726811920452691439955417215653623231677010244311602167043447427681875615683502186507396554496858642186240.00000000\n",
      "Iteration 74, loss = 17726811757082389841641822421035588873679374452958548583288210799173406488338527110437394611894739619807232.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 75, loss = 17726811593712098428508109298847985857910180708496300256874942500346190541695798806273891485976151514415104.00000000\n",
      "Iteration 76, loss = 17726811430341798867230490838716037768358233326521407724588099537773972050491273084585189306710738675433472.00000000\n",
      "Iteration 77, loss = 17726811266971497268916896044098003410360597535168354140832862909265502923146298008515187364108619653054464.00000000\n",
      "Iteration 78, loss = 17726811103601205855783182921910400394591403790706105814419594610438286976503569704351684238190031547662336.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 79, loss = 17726810989739364774275009492650990609679570782322674614074621162808180929582070565025578316562428795551744.00000000\n",
      "Iteration 80, loss = 17726810955359734157532561489622105024710364113615626020087601266799359174221459433393211465354087562739712.00000000\n",
      "Iteration 81, loss = 17726810922685679134163381000362322451069626819728234037154377472531917002717632739570590461509099835097088.00000000\n",
      "Iteration 82, loss = 17726810890011620036722247842130367340537512707084519951284366346391973558932907336985369930990699740659712.00000000\n",
      "Iteration 83, loss = 17726810857337558902245138349412325961559710185062644813945961554315779479007732580018849637135593462824960.00000000\n",
      "Iteration 84, loss = 17726810824663503878875957860152543387918972891175252831012737760048337307503905886196228633290605735182336.00000000\n",
      "Iteration 85, loss = 17726810791989452929578730039864933351169612416044182951016301297653396408280977901136207156119030374334464.00000000\n",
      "Iteration 86, loss = 17726810759315389758065644212660805703746121484644146762209502839640951692215353789788387098927217913102336.00000000\n",
      "Iteration 87, loss = 17726810726641336771732440057887109398551072600134915830744672711309760156851976450347065858418936368857088.00000000\n",
      "Iteration 88, loss = 17726810693967277674291306899655154288018958487491201744874661585169816713067251047761845327900536274419712.00000000\n",
      "Iteration 89, loss = 17726810661293218576850173741423199177486844374847487659004650459029873269282525645176624797382136179982336.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 90, loss = 17726810638520859323506834927310096801665506774434710045396587899623354858916202976589122571738122836508672.00000000\n",
      "Iteration 91, loss = 17726810631644931977936759526012667923604252395066403695718147720859840126159811137633869343494430879907840.00000000\n",
      "Iteration 92, loss = 17726810625110112417711822823319149081404213616900648882964249565074099020069158510467886136711267364110336.00000000\n",
      "Iteration 93, loss = 17726810618575313227846649465486492923661058932516504584894288068650864275382999427114900563295165682286592.00000000\n",
      "Iteration 94, loss = 17726810612040503852801594435223405423689462201241555029482358242546376349994593571855416173195533083475968.00000000\n",
      "Iteration 95, loss = 17726810605505706699972397411876835534391995926235571782880790412059392241448883842883730363116137585049600.00000000\n",
      "Iteration 96, loss = 17726810598970887139747460709183316692191957148069816970126892256273651135358231215717747156332974069252096.00000000\n",
      "Iteration 97, loss = 17726810592436085912846311016864574266003114054307511620588537093914165754531622777983461819580166204030976.00000000\n",
      "Iteration 98, loss = 17726810585901276537801255986601486766031517323032562065176607267809677829143216922723977429480533605220352.00000000\n",
      "Iteration 99, loss = 17726810579366469199792177290824485534505609001135773561233071107641440539895260421845792802717607189807104.00000000\n",
      "Iteration 100, loss = 17726810572831651676603216922617052960751258632348179799947566617791950069945057149061109359271149857406976.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 19868950094404125584266329894933350075043471499632046321662948096832922597213429530235490856066133570617344.00000000\n",
      "Iteration 2, loss = 24977109921931451992969773910938901062656248435267582887014211170379962914899593467066248273797557696593920.00000000\n",
      "Iteration 3, loss = 24978824170389137371896561891507020850660071028480434694465311118199132159405983138255902533852901064835072.00000000\n",
      "Iteration 4, loss = 24977022239495062712569212871659148921826067951419931136842148619581821190595775119216253718767212817285120.00000000\n",
      "Iteration 5, loss = 24975218653986514440367325503850989300709742229394106255597774683261936467229999037187083286562864740958208.00000000\n",
      "Iteration 6, loss = 24973415197808756972633201400484048292324182682808727938875231523337914214396389236454432748910323947274240.00000000\n",
      "Iteration 7, loss = 24971611871857792805773422273520019343390384752071250868438843195269564625586166458007004336044594872451072.00000000\n",
      "Iteration 8, loss = 24969808676124683425923832398012356514227608085810981200977203570789096316507563676701436526498944768802816.00000000\n",
      "Iteration 9, loss = 24968005610600025875017671786186844659538154994436505358385150688163572862846361068458021758037631074041856.00000000\n",
      "Iteration 10, loss = 24966202675274404972772322443352751023350197332087443453747160590042554023445642682909253888404674125496320.00000000\n",
      "Iteration 11, loss = 24964399870138434057408835057624550607931544684197670320705220642183108463114783531025823462057980828057600.00000000\n",
      "Iteration 12, loss = 24962597195182706096788496972255855731093122542419450278217381550979798485258665079965423390088396252643328.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 13, loss = 24961364248201955768184747040277641226891291335107252040939681886213138253666509246526689452885594920189952.00000000\n",
      "Iteration 14, loss = 24960980829076553997911994016742087801252059487674682337576219740290152910367494785018748540161583327739904.00000000\n",
      "Iteration 15, loss = 24960620322662249525575389153759578718618612449567052212846921979675675250402399804908524754261592284069888.00000000\n",
      "Iteration 16, loss = 24960259833080527399476205801828701376539104507087511424382332592727799153654781896456287799139095642374144.00000000\n",
      "Iteration 17, loss = 24959899348711022188959456695471138996980571109122550152969009604625132164321902633622294320882418634981376.00000000\n",
      "Iteration 18, loss = 24959538869547740934182765776621089812727711865122354978592787830918302757201761426622640582901969706811392.00000000\n",
      "Iteration 19, loss = 24959178395590622524066843010695965770409874493742094357201857293519791848080877644018333685096563355942912.00000000\n",
      "Iteration 20, loss = 24958817926839581403100682349280143595308145801099004127123684023107072719060378401794783567324539879686144.00000000\n",
      "Iteration 21, loss = 24958457463294568682420851764707552844726003962117219053116820037210130102769479194800795909504950876504064.00000000\n",
      "Iteration 22, loss = 24958097004955486584300487201646052633270405326645008664698369370888933464466611012733982071475899543322624.00000000\n",
      "Iteration 23, loss = 24957736551822249553228582611680019686222436700799608800195798054820956086252900971579751993095726177452032.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 24, loss = 24957490007234203785163415672266179876882479695842374052948129100339125603863717836754549659950849326055424.00000000\n",
      "Iteration 25, loss = 24957413334957637713992191806008090846087746927221498117720136276168982790620845891534932513682538509631488.00000000\n",
      "Iteration 26, loss = 24957341243613005200157490448926624786045528682890515706565121592813169689812142265854222167859548970287104.00000000\n",
      "Iteration 27, loss = 24957269154801451367326438553953006160888633094364966860554422628358404871461004039004818176216088268767232.00000000\n",
      "Iteration 28, loss = 24957197066199325203222335173508646436957105674577119480096518569831541652707989753584082831335342651473920.00000000\n",
      "Iteration 29, loss = 24957124977805416708475237622858302157512031252899308992965571851099702166126182907099956711213839180365824.00000000\n",
      "Iteration 30, loss = 24957052889619725883085145902001973322553409829331535399161582472162886411715583499552439815851577855442944.00000000\n",
      "Iteration 31, loss = 24956980801642264949267918017856177542755371860142765007494912428638598206318887657229330725268795777089536.00000000\n",
      "Iteration 32, loss = 24956908713873029832951601301448742281226540526576675715028774388654336277655196671368029912792080578510848.00000000\n",
      "Iteration 33, loss = 24956836626312028682280101090724012611749669466145911727636743015955103170286307959493736431768256993296384.00000000\n",
      "Iteration 34, loss = 24956764538959228904677796033904608239193744128799896221824519655560888705965031851505654068810026087088128.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 35, loss = 24956715231833766521581878950176831454587656901343367818103833124747187271844911771456481590885809953701888.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 36, loss = 24956699897840325140920981815564679472170725205597151630526546680760895413489691606053257789049393997938688.00000000\n",
      "Iteration 37, loss = 24956685479968895245988876219695162155880907882312101222999867481170365932684628204970174475378552100356096.00000000\n",
      "Iteration 38, loss = 24956671062570754363870277790081428047727502299722071084994319241925138596163460463836704338695773115908096.00000000\n",
      "Iteration 39, loss = 24956656645181185329140094877918711559030923380433745525989317262422588138653175959212638322872613867618304.00000000\n",
      "Iteration 40, loss = 24956642227799931475265309337960142865634431542798832060967259634695134406457156039054206247484095247417344.00000000\n",
      "Iteration 41, loss = 24956627810427009098533731846094412115103534061842619101675295686232782488698995538411806219223866722484224.00000000\n",
      "Iteration 42, loss = 24956613393062418198945362402321519307438230937565106648113425417035532385378694457285438238091928292818944.00000000\n",
      "Iteration 43, loss = 24956598975706142480212390330752774295073014894941006288534499499613379007372657960624704197394630491242496.00000000\n",
      "Iteration 44, loss = 24956584558358222683054342321109902446921654121533539052306391252691335077489873136055599363866096985702400.00000000\n",
      "Iteration 45, loss = 24956570141018613992679739014699005857179003611023161807124440025671886600640454187189528944098791741456384.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 46, loss = 24956560279665188515779955487440546956467404907993968887406257688679027007105616014067968160510892801261568.00000000\n",
      "Iteration 47, loss = 24956557212884980230025082518292743899269268447521784571158137773547539701591114928138776390742105391104000.00000000\n",
      "Iteration 48, loss = 24956554329326608390900176938219204041107434408810172981331477693993903589076638416388430819691324185247744.00000000\n",
      "Iteration 49, loss = 24956551445861532799491390820896758995474749889874718643934717494719402709142592568167246069783742579212288.00000000\n",
      "Iteration 50, loss = 24956548562396864615277871600791567638979747246571474600216690482695029057298417596206013987217397652652032.00000000\n",
      "Iteration 51, loss = 24956545678932514208676660560515834160012136466261354585568075356725754643364341907727544985177217336082432.00000000\n",
      "Iteration 52, loss = 24956542795468477505615805031097386021680540730188036497052084784939078195059466793969239536989789262708736.00000000\n",
      "Iteration 53, loss = 24956539912004807469030689709174466203572858682183707672846954081677516252035475468844891489409474200862720.00000000\n",
      "Iteration 54, loss = 24956537028541459284129835236053176799883945315928824980648022596343554819202482135965906049028736115802112.00000000\n",
      "Iteration 55, loss = 24956534145078441099057146949677862884396554268936032626328864992682196441122284212857482269194399741116416.00000000\n",
      "Iteration 56, loss = 24956531261615756987884577519020696994002062359961652712826268602565942390075780408282219676579877443600384.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 57, loss = 24956529289347946557674424040333947244415225933816689961079773504404268192770941676548133540622063665938432.00000000\n",
      "Iteration 58, loss = 24956528675992637418660539327701008766045150654108967205864511792053699487773629294877690082547849733603328.00000000\n",
      "Iteration 59, loss = 24956528099281596976431393503756347534711016844850364104863288615504170232178573075988107318720657765695488.00000000\n",
      "Iteration 60, loss = 24956527522589170968953992213668007360077567933227449322043384764412954028009717193415761925714569683009536.00000000\n",
      "Iteration 61, loss = 24956526945896761257764401599468357333009626296629822950970630240811742912964456145893814639402131067502592.00000000\n",
      "Iteration 62, loss = 24956526369204367842862621661157397453507191935057484991645025044700536887042789933422265459783341919174656.00000000\n",
      "Iteration 63, loss = 24956525792511986650176699729762955184678888029754113341129781844206834677963819847238514860184789871230976.00000000\n",
      "Iteration 64, loss = 24956525215819613605634683136312857989633337761963385896488113307458135013446647178579963313933062556876800.00000000\n",
      "Iteration 65, loss = 24956524639127269079596335225667968552827425225466913172403956093816944254895765471259608454395221810085888.00000000\n",
      "Iteration 66, loss = 24956524062434936775773845321939596726695643145239406757130160875793257313187579890227052174877618163679232.00000000\n",
      "Iteration 67, loss = 24956523485742612620095260756155569974346614702524544547729940321514572916041191726719694948706839250862080.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 68, loss = 24956523091289145052522531980572622880309189612442224785514107401324267593497074023665186740338443404967936.00000000\n",
      "Iteration 69, loss = 24956522968618133743211968133300974642088247109079074310887217974073169628780755535987332179473913966755840.00000000\n",
      "Iteration 70, loss = 24956522853275946025125902313372905080278304441008964205370909998125770139066237836022413260075537407148032.00000000\n",
      "Iteration 71, loss = 24956522737937486082876528602982706774078150934973578287015010685517034786374038653836061246849476464738304.00000000\n",
      "Iteration 72, loss = 24956522622599017992483249554648163394095243791425548162785536709163296889120042054124510180276590788739072.00000000\n",
      "Iteration 73, loss = 24956522507260558050233875844257965087895090285390162244429637396554561536427842871938158167050529846329344.00000000\n",
      "Iteration 74, loss = 24956522391922098107984502133867766781694936779354776326073738083945826183735643689751806153824468903919616.00000000\n",
      "Iteration 75, loss = 24956522276583646313879033761421913549277536910832034613591413435082093375605241925090653193945232695099392.00000000\n",
      "Iteration 76, loss = 24956522161245186371629660051031715243077383404796648695235514122473358022913042742904301180719171752689664.00000000\n",
      "Iteration 77, loss = 24956522045906734577524191678585862010659983536273906982753189473609625214782640978243148220839935543869440.00000000\n",
      "Iteration 78, loss = 24956521930568278709346770637167836241351206848994843167334077492873391134371340504819395734287286968254464.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 79, loss = 24956521851677578677317100611695770763517518920968263850192051177839328034213079030188334849936148012204032.00000000\n",
      "Iteration 80, loss = 24956521827143378859898159443624744638008156511549427017028745691512609204638354557910323653767289544638464.00000000\n",
      "Iteration 81, loss = 24956521804074944575538508414816868755159269432940462678274913961821130324520169984927419491226344126152704.00000000\n",
      "Iteration 82, loss = 24956521781007251772274243138944394586540963367982121074016376632924882999525550406737629183246449464311808.00000000\n",
      "Iteration 83, loss = 24956521757939571191225835869988438028596787759292745778568201299646139491373626954835637455286791902855168.00000000\n",
      "Iteration 84, loss = 24956521734871882462033523263088136396869858513090726277246451302622393438659906085408446673980309607809024.00000000\n",
      "Iteration 85, loss = 24956521711804189658769257987215662228251552448132384672987913973726146113665286507218656366000414945968128.00000000\n",
      "Iteration 86, loss = 24956521688736505003648898049287533133416000020686687274602951308574901333232464346554065111367345017716736.00000000\n",
      "Iteration 87, loss = 24956521665668816274456585442387231501689070774484667773281201311551155280518743477126874330060862722670592.00000000\n",
      "Iteration 88, loss = 24956521642601135693408178173431274943744895165795292477833025978272411772366820025224882602101205161213952.00000000\n",
      "Iteration 89, loss = 24956521619533442890143912897558800775126589100836950873574488649376164447372200447035092294121310499373056.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 90, loss = 24956521603755307772624322095230994723829503697739221533670228184616353354077626602623999549259177548316672.00000000\n",
      "Iteration 91, loss = 24956521598848479216542001334738872602023486308373156055260571616594013150549198092703675984710960481828864.00000000\n",
      "Iteration 92, loss = 24956521594234771989310307784116434740996824798869752672825868611293211013121067634294097518835709564157952.00000000\n",
      "Iteration 93, loss = 24956521589621219576812815654436553281842482402106589201989084217147457222367088108863301066550128584687616.00000000\n",
      "Iteration 94, loss = 24956521585007695682818992207561879580927777736637680451709811146109212337579399544770701300978434172780544.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 95, loss = 24956521580394151418465405415826343195556188977387161186746601415708461091387217436865103902039677926899712.00000000\n",
      "Iteration 96, loss = 24956521575780619376327676631007324420858730674405608230593753680925213662037731455247305083121158781403136.00000000\n",
      "Iteration 97, loss = 24956521571167087334189947846188305646161272371424055274440905946141966232688245473629506264202639635906560.00000000\n",
      "Iteration 98, loss = 24956521566553522699476597709591906576332799518391925494793759556378708625091569821910911231896821556051968.00000000\n",
      "Iteration 99, loss = 24956521561939974361051058248884197654069833940385084126893762494105456106618489005242714306284652943376384.00000000\n",
      "Iteration 100, loss = 24956521557326442318913329464065178879372375637403531170740914759322208677269003023624915487366133797879808.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 11755988319507461265946560444098952077116728800365138989308984236725529031357598110054805595199193426690048.00000000\n",
      "Iteration 2, loss = 12731589967153425872622023510722725947060645324297577463400606131438674585777655823179966406678561630978048.00000000\n",
      "Iteration 3, loss = 12729031165158840467696941787664087467946164120516255447494916634231019751814921028606792921003524318298112.00000000\n",
      "Iteration 4, loss = 12726472875260383965368621825005491803296283257238651791529757823474478406937508944026977025469118469373952.00000000\n",
      "Iteration 5, loss = 12723915099528890842890891502158750692382469297964400852758771164075745842835488763729528708887736389468160.00000000\n",
      "Iteration 6, loss = 12721357837860997820752586326134430664083453576045429021950559718805043020827720471801856740114220128927744.00000000\n",
      "Iteration 7, loss = 12718801090153421063845618848900462716659815390581943697141079521946365711710588873202060658134952890597376.00000000\n",
      "Iteration 8, loss = 12716244856302835996342374932703052479458365853110931246998413289058696963471491685262244735202194209374208.00000000\n",
      "Iteration 9, loss = 12713689136205985264602459477829252440533633584648692738647634711597290816732655319897405433679507672268800.00000000\n",
      "Iteration 10, loss = 12711133929759615589057430053538287624831524024967851342150604812889902584397204897785138742603869233086464.00000000\n",
      "Iteration 11, loss = 12708579236860473690138844229089383057297942613841030227569184616264287579368265539603040651012254845632512.00000000\n",
      "Iteration 12, loss = 12706025057405369436393525942810438084695135481763845160485438789071970834902892351848999811379532149030912.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 13, loss = 12704363922423506662671082019303699819212349546331818867305925722031350539486571863184284685703192109907968.00000000\n",
      "Iteration 14, loss = 12703845070165537782482798831457493731053558216669265369326169671258358957645875819878775509012458858610688.00000000\n",
      "Iteration 15, loss = 12703334385389376578230421648655940346450718248113161076699954544327395953102069146437509261573091691069440.00000000\n",
      "Iteration 16, loss = 12702823721142398758325752539713022019375562286532601608450447641347436040657337871809557912275405410140160.00000000\n",
      "Iteration 17, loss = 12702313077423710063975180665236866902170878614914885369952829616304449954654415422604325356305385504374784.00000000\n",
      "Iteration 18, loss = 12701802454232495680788172230792967616561303481995591773849634094698183238913560046301906258980558614822912.00000000\n",
      "Iteration 19, loss = 12701291851567946905482122445405075589608538364644783387188575699837133345676378053526294575628569932726272.00000000\n",
      "Iteration 20, loss = 12700781269429236701450639507722165832363089055329073313801826061603542001920431565469786391546708998750208.00000000\n",
      "Iteration 21, loss = 12700270707815533958015378947421040818984084527758752554584769478007149662342383994562078265358852986765312.00000000\n",
      "Iteration 22, loss = 12699760166726019786713854301095020634304784211913078419243152242675200598481594879520665335709116171026432.00000000\n",
      "Iteration 23, loss = 12699249646159856965759792094962648947147251853367858754267177655808683356613379569631344871211257175212032.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 24, loss = 12698917583128732514349834516768111988344477808366371985488874608763822657987688051407522771102220907708416.00000000\n",
      "Iteration 25, loss = 12698813855588821208636472813064035457539823901091187393887545306702497468044809290710732100700152797331456.00000000\n",
      "Iteration 26, loss = 12698711757517101239317650913505536738391945507664225224220562429234631629282117690916463727784017354817536.00000000\n",
      "Iteration 27, loss = 12698609660266247694418313206616719846748082641174334324617535548621861943536210477869126910695311736307712.00000000\n",
      "Iteration 28, loss = 12698507563836246314686625350994980903488416435974387334799709003310433957823942170899623306077092658020352.00000000\n",
      "Iteration 29, loss = 12698405468227097100122587346640319908612946892064384254767082793300347672145312770007952913929360119955456.00000000\n",
      "Iteration 30, loss = 12698303373438800050726199193552736862121674009444325084519656918591603086500322275194115734252114122113024.00000000\n",
      "Iteration 31, loss = 12698201279471338870209650215843541616449090513088921412310282051694195111765375851407713660351705197314048.00000000\n",
      "Iteration 32, loss = 12698099186324693188213177068651871487138312309216562723455021533245617386535979954835749058861071511584768.00000000\n",
      "Iteration 33, loss = 12697997093998899671384353772727279306211730766634147944384961350098381361340222964341617669840924366077952.00000000\n",
      "Iteration 34, loss = 12697895002493913504931700969375867167864200879022133942795440851654973041088219083536724699883727726051328.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 35, loss = 12697828596444341533876194445200321072881194245193373250330668635953310288018003536705066882129298878103552.00000000\n",
      "Iteration 36, loss = 12697807852652390971773314535309424709767338176658670130328886128056940142812748261587139802725078139928576.00000000\n",
      "Iteration 37, loss = 12697787434593032649696314450753346861304880182859043454979137651040219240058022387471727546163890722177024.00000000\n",
      "Iteration 38, loss = 12697767016566499125341968274991398746665450908746600141324922070766249104504193013620701697286142569742336.00000000\n",
      "Iteration 39, loss = 12697746598572810769070039352884443050305934448102950704050176046597536097555753683847059889458895516598272.00000000\n",
      "Iteration 40, loss = 12697726180611949247556740674057703356215135116524645679939356585107824493948660208719104252651793912168448.00000000\n",
      "Iteration 41, loss = 12697705762683908449694143235052920859055987685877201914587282688488362385261564525092935496854719206260736.00000000\n",
      "Iteration 42, loss = 12697685344788706708806034046244871974839687840564068871209497350165405496758510822400251492098027049451520.00000000\n",
      "Iteration 43, loss = 12697664926926346061928389442119642972011923989963407601274394236075204464579948455022352001718423625138176.00000000\n",
      "Iteration 44, loss = 12697644509096804101665469743330284897670123630915446538629643020919002291180934524764939629012140915949568.00000000\n",
      "Iteration 45, loss = 12697624091300103235413014629223746704716859266579957249427574029995555974106411929822311770682946939256832.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 46, loss = 12697610810352431149094481326573795631998153738009120207823669955140618363748993676719872837343334434340864.00000000\n",
      "Iteration 47, loss = 12697606661662683445375643789141821427426851022070763124578353240586280486657927079230672531118093464764416.00000000\n",
      "Iteration 48, loss = 12697602578113005341168102500724902280425461511697164591622775711225608564622400261623184418117023759859712.00000000\n",
      "Iteration 49, loss = 12697598494564639088129320621347540012447407640859283204312719044810346317036257665572743893954736162865152.00000000\n",
      "Iteration 50, loss = 12697594411017590797367227154467993428829754637691602117053364239149245652320847354223250248641349223972864.00000000\n",
      "Iteration 51, loss = 12697590327471860468881822100086262529572502502194121329844711294242306570476169327574703482176862943182848.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 52, loss = 12697586243927444028601152789230174777784274415610518739749972878217027799221324876864504067887864953700352.00000000\n",
      "Iteration 53, loss = 12697582160384325180237408546011040025899563102915505935021999663583404249432719167042253899080705788346368.00000000\n",
      "Iteration 54, loss = 12697578076842536516366210722206238569049383114159659739155090305321446099357541868208749189142684381478912.00000000\n",
      "Iteration 55, loss = 12697573993302049518483890635010562648994096718048725431591733480323644443029502019025793251359914165534720.00000000\n",
      "Iteration 56, loss = 12697569909762876408806306291340529876407834370851669321142291184207503097291295745781184665752632240898048.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 57, loss = 12697567253583835986078284369332560186672503017671974657398319806425292716358706125864557684460284129837056.00000000\n",
      "Iteration 58, loss = 12697566423848623406872319877351675629385189314981491993682986015460779859248245353081079642413663948636160.00000000\n",
      "Iteration 59, loss = 12697565607141174599550720760282408318481321581765783919706857884563422075101711818250333101264358699171840.00000000\n",
      "Iteration 60, loss = 12697564790433782829236459008823556524056729311138585286845752399881082102887760206095979933542826584834048.00000000\n",
      "Iteration 61, loss = 12697563973726444021857581954002947709220035684343573992162882229541258670325491807855420612575655238828032.00000000\n",
      "Iteration 62, loss = 12697563157019147992234207923390150531742798654489944778316279043862698596712659851622156321679313744166912.00000000\n",
      "Iteration 63, loss = 12697562340311917147762076596332113944527590724737469211458273168144158879594207235590484457557570118221824.00000000\n",
      "Iteration 64, loss = 12697561523604737229189353631426234068454593029439019931310108940831885065986988479091306676853481077211136.00000000\n",
      "Iteration 65, loss = 12697560706897618421695920701102942245752247615485402195213754691607130336593250354031121796250577538121728.00000000\n",
      "Iteration 66, loss = 12697559890190536280850061791529203255072293570338683385548486429234887057727600607834332655708385300185088.00000000\n",
      "Iteration 67, loss = 12697559073483511177011540247565879780871614987780474016998240813077661590794532784313936888593966197374976.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 68, loss = 12697558542248123536691451301092491650114636412796976107325899186763350813996761604630882645031652828381184.00000000\n",
      "Iteration 69, loss = 12697558376301199576344081069786535562196239098067852770043343786060235265948821875065833262818628665868288.00000000\n",
      "Iteration 70, loss = 12697558212959809629642601636190909253854197610954602677199407790757044880001533532783372358087370602446848.00000000\n",
      "Iteration 71, loss = 12697558049618433942192956543997886824631974989488479944634227457007608947037390671170009796713055822807040.00000000\n",
      "Iteration 72, loss = 12697557886277052143635382448346605590072687139887874057663866125449421105651899746412747945328622492975104.00000000\n",
      "Iteration 73, loss = 12697557722935668308041832018209238087067710880909107119225111127954982628125959467274186330607482979745792.00000000\n",
      "Iteration 74, loss = 12697557559594300768736092263960560731628241896955628592533505457950549239723614023186022822579992933695488.00000000\n",
      "Iteration 75, loss = 12697557396252916933142541833823193228623265637976861654094750460456110762197673744047461207858853420466176.00000000\n",
      "Iteration 76, loss = 12697557232911553467908754748546688410075173472779705230339932122324178646076227008721897226504775741210624.00000000\n",
      "Iteration 77, loss = 12697557069570183891567038659811924786190016079448065652179932786383494621533432210252433955140579511762944.00000000\n",
      "Iteration 78, loss = 12697556906228808204117393567618902356967793457981942919614752452634058688569289348639071393766264732123136.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 79, loss = 12697556799981757972335458660437780727988622428652601427356759250916955057491756461411877373765664915849216.00000000\n",
      "Iteration 80, loss = 12697556766792373587673179881073806764094080647582408970193926903963582075110258386375127449990401320026112.00000000\n",
      "Iteration 81, loss = 12697556734124099672404836663326854039317049168916081054561927036775445270201699426681234795717562074136576.00000000\n",
      "Iteration 82, loss = 12697556701455819646028564442121642509202952462115269984524746171778556556871792403843442851434604278054912.00000000\n",
      "Iteration 83, loss = 12697556668787547767796197558860776052871609392827103120361139970526670388103682798530849960498471215562752.00000000\n",
      "Iteration 84, loss = 12697556636119271815491878006627737059648889504782614153260746437402282947054674484455657542888925786275840.00000000\n",
      "Iteration 85, loss = 12697556603450995863187558454394698066426169616738125186160352904277895506005666170380465125279380356988928.00000000\n",
      "Iteration 86, loss = 12697556570782717873847262567675572804757761319315475167591565705217257428816208501923972944333128744304640.00000000\n",
      "Iteration 87, loss = 12697556538114445995614895684414706348426418250027308303427959503965371260048098896611380053396995681812480.00000000\n",
      "Iteration 88, loss = 12697556505446170043310576132181667355203698361982819336327565970840983818999090582536187635787450252525568.00000000\n",
      "Iteration 89, loss = 12697556472777892053970280245462542093535290064560169317758778771780345741809632914079695454841198639841280.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 90, loss = 12697556451528488118721822267484576573076521086828784173712361129245676924015474399778155940851197226778624.00000000\n",
      "Iteration 91, loss = 12697556444890614908454123913686737063499851867495435574922903258540253472591983622657145530102215637729280.00000000\n",
      "Iteration 92, loss = 12697556438356958495771674202548477503787894844259641150621788352353625602697912347213327188578282841833472.00000000\n",
      "Iteration 93, loss = 12697556431823304120125200825896304212521626230402007777789067112103248368944290426150808610391056229335040.00000000\n",
      "Iteration 94, loss = 12697556425289661966694585456160648531929488072813340713766707867470374952033364631376088612224066717220864.00000000\n",
      "Iteration 95, loss = 12697556418756013702156041082966734046000284687090190495339167625028749626701090773457469324046958654914560.00000000\n",
      "Iteration 96, loss = 12697556412222359326509567706314560754734016073232557122506446384778372392947468852394950745859732042416128.00000000\n",
      "Iteration 97, loss = 12697556405688711061971023333120646268804812687509406904078906142336747067615194994476331457682623980109824.00000000\n",
      "Iteration 98, loss = 12697556399155058723360526290954559245984232483029934582714578568022620470002022427795112642832103551008768.00000000\n",
      "Iteration 99, loss = 12697556392621406384750029248788472223163652278550462261350250993708493872388849861113893827981583121907712.00000000\n",
      "Iteration 100, loss = 12697556386087764231319413879052816542571514120961795197327891749075620455477924066339173829814593609793536.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 1919081335551814856904499460585888234169274176849295129740469331134494694351870810493925376383817727761683527775874955983206084634217289849415134851754295432939014026474004629022767584960321126286157940079857490801924371036041695608277303296.00000000\n",
      "Iteration 2, loss = 4016683507167615142586649507268921108819616715750069822980863553377382686621785127744270068927808465994584481964636875169960127652125280237155269236562190983170859520361774311474688216596876681365434803581181994532498658491380740621263175680.00000000\n",
      "Iteration 3, loss = 4068706579987372267062929336454374038462447857902526405527081126721415277311210009083763502272037751794436337542728803187045855735914067951207918272747004441828043397180828087052599278763705733397731764206389320199488815511230446916406870016.00000000\n",
      "Iteration 4, loss = 4069135744272546388788481924817412574130590535626965186701956249609949664960155600115271246661728224753379452694769633053378999530620809897403428830997617140453908756196542853606273880467084539944484600515447744688978611840394640354868985856.00000000\n",
      "Iteration 5, loss = 4068942546083537070330142132528591395172143258655221957544739586526539941947271693644949384056583075827136541103331702500079132426486878875687116722113116182524815197955282170083616699268900354972477882715048594763287393417806978390639509504.00000000\n",
      "Iteration 6, loss = 4068741905784472636689740735208649108133783863218234529819682732395633941401762149002340857708490615964426379192209458776998582725042565091443713348574726257510427325589871709760448537149113788535491761249415842871083904135291508602454409216.00000000\n",
      "Iteration 7, loss = 4068541186168705173418543379827649672999467212816907849564876549836263159880083379912738506092112911919321512231450794340799403842639025412801983605210712944734799087394059161058906737253056731623851774264927858187199519636085265476094001152.00000000\n",
      "Iteration 8, loss = 4068340475386790914316894203074075317368455951792923352778227995146404550993656401677411864647820718292311624955255078611663846078650941231060114482270847752811823633618239663436114764550005502653162771646760602475634020982181574968606720000.00000000\n",
      "Iteration 9, loss = 4068139774493628307793486235813419056433488370527270746688720217756880557900513474115291727830948613100680824479481724109574572873858513211683471568176960559877077994847386065999135132778844179875618703252619222708602581025690000251475197952.00000000\n",
      "Iteration 10, loss = 4067939083501367870095559323203590379036639366041926154247353594120435312213490278324572935040523804350384127246716368463054463020324927114819176506284790524105673420069536774891838295535590841205014766223634662950134163047101609351915241472.00000000\n",
      "Iteration 11, loss = 4067738402409665362506549368004692379535335074502415750000389308965605582274756521749923700477451205724308934545827450013828786813650659308074101193008130449460844739554634039620292627832604689332515600377596121458330669932821791495988183040.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, loss = 4067537731218036629928321059259966829348407093870576347180633415652526951946202502603845125404939018402258172060513804623162763121196379779437265456851540537645722230714596636909162037751784473849565666877587760944587036065020707014541574144.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 13, loss = 4067383618269839990725133723255239963236223564890023391180142725944399639617063674481343306849162757917841272933093143300828912426162752237542068651652767811194312000657652498996624897695371394926746447755055912177097256939813556871246118912.00000000\n",
      "Iteration 14, loss = 4067332813954721987206000427412526932909339973380415240940000300879159636315831153438336204923169864147751600191121504610852346607860320345107968304685884166761040483067154154412789485512356969990471876394113908929501066807187679698246172672.00000000\n",
      "Iteration 15, loss = 4067292555311891247282429772052951598020968818337659019369899536653490217419972148741885588403836057899317661904042776220665865618068057970576231481424466476935831878213483053748171550442766809449565269361475856021873346965968444473284231168.00000000\n",
      "Iteration 16, loss = 4067252423318244306433091846552997399946634956355701334717548848619052560719109962197667319340100131149314901292341571538757745403654949864271003245541753183276202009044849454955652640061705729720626244705268888385680030926025044769768472576.00000000\n",
      "Iteration 17, loss = 4067212293232112189564924528550932274409491977099237124133037921735078562136074749078238641578565047244385898515730302104083245944042166939159096430998185508533184184444226122196929857969333193903797361424669394988102764446116198690729230336.00000000\n",
      "Iteration 18, loss = 4067172163560030400980053370696865255760192417947890169305574003367079601591315421511881922968599785562490625488161395242253634936176409056501672609944633226811312511157065083539165611011311661154139705307222236745556706197805191505119281152.00000000\n",
      "Iteration 19, loss = 4067132034284107410996927615799767493311155330188620871168577120438231200146245935461481896429494569518731693713661555991583605937202435038498473701797321039580392103084364803633242667571936208116958685205248582479131724909361801310681169920.00000000\n",
      "Iteration 20, loss = 4067091905404129273402456889446025641404953086621078830676605299586269112749010474693724828421596754068710090229828587966607789302880540950178286756155358883067035575168089390236695996207275242486332969736686234977196530116637979690923982848.00000000\n",
      "Iteration 21, loss = 4067051776920087844915174083958996977542514047455631333679139988191868402736735634438484590678691186632881215927366905837775600029450738060452242075944664436504781815866237477208808934820846938949580432101525906456609446404043316498507235328.00000000\n",
      "Iteration 22, loss = 4067011648831983865833394390945649021951026543582244990553501054674967700715137178765772649406797426536658216179848835544137091299073025344873873269990154274508763653500809188744192089057786008898763083827041168534019716991176939928643698688.00000000\n",
      "Iteration 23, loss = 4066971521139805491384074744694501450995477280761454035262570604316548916992762882555405545309602524573430765010117162060891412197187419194586642597093163203236855834919802537729075768595937070060888740476854875343039450364452798857929031680.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 24, loss = 4066940702556348091411740289251338581100903772516802597885912727345671746508886428627171127272058461229879181890379157217314038464684821931396674038675447954304372336032102507779722106266833144154129234407035874744535477706451936328865021952.00000000\n",
      "Iteration 25, loss = 4066930542785824171141818762042576369577967907779944959897624177286371914521760828259892037487651716877625168943423338229602617012472502533454278992866558955379911194129283780631899465251375363032757763271802173915593022410955615312828956672.00000000\n",
      "Iteration 26, loss = 4066922491828970601234541363514822323577129626758895752869712415799658047812821044095371799700702518949970623004168935818134244500182131616134033586593767713558708186029214319727827321982273430530738070679993336441263403972072293615030763520.00000000\n",
      "Iteration 27, loss = 4066914466135551401224031979114724190872820633192948560840576979999828818777133988556816043309663997204184368357784748363511274573971481122265685851019707412324345492285412500049602553131696522683276335011372131757381446554362238660614553600.00000000\n",
      "Iteration 28, loss = 4066906440760253933515975132404385605765250006968779321082618828648282108525298862415529803849678351320641190584875415436791795624680604468413255967421007300584102156756979712635446217214433723115470110869219605202945058493619358733506183168.00000000\n",
      "Iteration 29, loss = 4066898415404409284636208141979576030485450176397230966158843148024131824666596203965541059965653099318318994171974782928319784293682808434763465176155138350689188638425810387004174418501982172761140788894708449340672751839168064412389998592.00000000\n",
      "Iteration 30, loss = 4066890390064444034617301121008075328395347935609475204746245075083608033411032847261502435201175377427965060885465543043488530294663038024409583679349794895393442626997305035770393707929089982136958726938308470604667505586016557405218799616.00000000\n",
      "Iteration 31, loss = 4066882364740317467051918531106669886999585085657348466092231846730086051444241768452783288225169422751856395178869769134545108608821349581907262991634565330864558564761456828230520775013348024679513290999973230079220942656212786975290163200.00000000\n",
      "Iteration 32, loss = 4066874339432026620746799605847489625389408302980984308687523989283811356343359911259337754213556997988340415557898157445289306507517747205042368677709783358642005138430265267606113197174217454820556434970608441297736090171360240341753004032.00000000\n",
      "Iteration 33, loss = 4066866314139576677790150686479307185155135903810149005173360581684354362348397624171246096608475018415309139637556989549071496265872223722689635999349865001031712146257731223259445213927641252304522239541129105576757650668652814871096721408.00000000\n",
      "Iteration 34, loss = 4066858288862961715795450240146382404479261241025109672531182676572206024613628794628416585761767009429457404429267657933491252426604787330420796085955477661112616961667853701633631980112541725994914612493346649983091678391297485001619144704.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 35, loss = 4066852125306503512393978565857091388096875353487189215752915360058981140180835026218563491092960738348666954368550073902296747755354532381921937596279993924059040154549594898780611624313220426561998122785772558215520155417320222767926214656.00000000\n",
      "Iteration 36, loss = 4066850093396095576692494053280187875810707291708961819837811005344731306762099902641911949646588131314534972259269788473193593641011599525208253086643949357104472460548361864436655905831603498190016237408564062387961944416361809193759408128.00000000\n",
      "Iteration 37, loss = 4066848483235591452903289587763999435182558426484578338708678448704603912617951692123089068833520452965017785094834471495057338663278811251217317238520986371780482179059126241706121729940547661436770526256316136759962213270315418611657736192.00000000\n",
      "Iteration 38, loss = 4066846878125118160798194277595273106107421851729010548831077240146034558821435696942320630342659962134591284886182193384509357249117642570948879182502083170400298506769098561796826095318879297174351448936585491475897191050751542593851490304.00000000\n",
      "Iteration 39, loss = 4066845273075734285662710374386315924613350317018131095389018044923262788689770759107743512585835004396921011865866405684349967678071936568273841358598917209781107965929319421154708297213595005869123602231984652087382650151246335533935230976.00000000\n",
      "Iteration 40, loss = 4066843668027711819128863836390628440918619288955848125838196873967632702469395946024252747698979646094025080845058019897238380099264346608547110163717331966391188709247768674401494280511044964565934954183482015716826390094537879470305443840.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 41, loss = 4066842062980327489742712463600943393060229488044783301537102280999102198376009761290645852400984426297260461843594593571272635542375873575966349775913834041245767466130324983402622329914616143220200242644798112897917785332277927680084541440.00000000\n",
      "Iteration 42, loss = 4066840457933576115416049914768488139447862598055170349844495187078100862169601856416842563249712429728735137246469845133102361732286524641656824933414007412038914438322987478795818205908365562087485386925017942314112133327272582796783124480.00000000\n",
      "Iteration 43, loss = 4066838852887456215552245806679327639627141957207076050005735855364751432638740703262819947833124537737622816306539122704627452304676301854511468418568018929540363969181755912191860697202023798383666363969594360732110947640323588429975650304.00000000\n",
      "Iteration 44, loss = 4066837247841970751344560905761331974506820889060366843530103759538808432206289358108623870975298987625576080518091730042048119988185201116744414666675534892210647371994630780369192226376129697676991219887621654618511201149827457360513204224.00000000\n",
      "Iteration 45, loss = 4066835642797120463091310403621468664314087724505009340794918768020210491477963585024265798882255338718008075254699993084414417964973221403909197286561471874664897475083612207522423399075817971359521966206373395589962137075383317783608557568.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 46, loss = 4066834410092239213581923231816761612306805389396297828727469926650155655605349066385583351113062195119358404080041247700396031990586299527367651125343978034612369119523435872923975730266446597616308885232169920125574719633358733323299454976.00000000\n",
      "Iteration 47, loss = 4066834003711906359121771943280057190513847005319779382849442237286338905743379927851338514683190712203464009110733523838611028066067293008582004989634701677088222642951882638564357241507338462270796137882359093808905410770227673340074524672.00000000\n",
      "Iteration 48, loss = 4066833681681037686879735960813560168520475165535328998634804724104170207067868003874658291819531717751199731451625753397892293457623030265085156348201255869450504045791042223528142442091340935868158981646042107434305879241807363335790788608.00000000\n",
      "Iteration 49, loss = 4066833360660073612045677520940611793667466785741222808902187533322732997357679692560015793954430621356070045929551471893023916969541231994387996134410067981405322724691626357555600256416738500972779170634294551324217119769683167861173911552.00000000\n",
      "Iteration 50, loss = 4066833039651222298244784154270229376069928427580795612877257429577169758368823094772983358981359207370824962797874205125125706983362671730607743513903873946163478408158242563801787636436283862996031968872725786829045919378494502457921503232.00000000\n",
      "Iteration 51, loss = 4066832718642544214249645623630246691634459497672555245145176536097246081117455289368634016216864675532255897082121208094939941622623871746354355358428158370862715918972887831586856737417351690761795464493172780629797632373501834734457585664.00000000\n",
      "Iteration 52, loss = 4066832397633889819800593224413224654469017156243246409487331432055358583248991934204651592044996042106907483320682641114355878091005038979814042685349773183246203936091537073599465219042730283073543328986374066163325128395681269257802350592.00000000\n",
      "Iteration 53, loss = 4066832076625262076090887723047033345482354726852735547413001591131261787186296085561081951289831544396432303007847807939573729117146169333200939929968384681774473772802190786618055503892958485499523608461423929896225380323429318808806883328.00000000\n",
      "Iteration 54, loss = 4066831755616660983120529119531672764674472209501022658922187013324955692929367743437925093951371182400830356143616708570593494701047262806515047092283992866447525429104848970642627591968036298039736302918322371828498388156745983387471183872.00000000\n",
      "Iteration 55, loss = 4066831434608085060292887030653207871590992942408174523260247961796563039266775379695158087617575837469275351980844691129315068478388321448649296954646764588035093248355511377283960271977694297910057389302522248726845665456433006603369709568.00000000\n",
      "Iteration 56, loss = 4066831113599535047906276648018606186459105256464157750804504304966022456804234758402792398494465068927180435893104081554788503631329344235157223125881616421152310058876178130736664149567067196502548879141297132207916455442089516651715231744.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 57, loss = 4066830867058814052863477279740065750392185280302902706490905167886851322480507306015009456776170495609903026465910316262443163645219606630707302216469352580439603593189785686118711442247641409736887837550383930535698034071464001469016965120.00000000\n",
      "Iteration 58, loss = 4066830785782817514192064148051852279525609705678440359009759182540282427808824759331245192579646511210807699811990805324221193892651708414314452384164605267526339857136686788056951038525563480356851578560501640207382580872915316739500146688.00000000\n",
      "Iteration 59, loss = 4066830721376694564208079095796524762712054836773259754050974653511638747625823789738695729739856482043696616907230810654911095267138785588587488221267193124596908160593727224799995625898605176571778138084205255834600759431731448930239512576.00000000\n",
      "Iteration 60, loss = 4066830657172544982662874597668838269009251684788488562139931531079767333057586749164436856598378527368798369619439789194460525807666366106774418933884083504511629070378651304570746558439551834889123649074632327024898811561895696435741982720.00000000\n",
      "Iteration 61, loss = 4066830592970810994520140313076170276621972226764767031423619062907647584939887869037592213698722651516993476298148306854333489736270603856141515242203756853604768794849980631355903711055061765504812773558723583341677489244015244920517296128.00000000\n",
      "Iteration 62, loss = 4066830528769109579503274459190073174230979327899576357309380805212827583473682607991252083863927385983366979414039165832408793679914796529864090338819759485763752965489315422703927512056498997371230405242851990791022868588496433994654547968.00000000\n",
      "Iteration 63, loss = 4066830464567407424188093413697008551612798098144419072817822679098068951401761582874900487823112561124327337156357698871434044441398990228033131826610845543307604307806650089857340707412801517845586025399706826623719004713378494873579028480.00000000\n",
      "Iteration 64, loss = 4066830400365708970364488326238781530130558522839094840212863895083003472358419378108606222812395532892353421766537861605709561113683178803969841358526514473927119791733985377983806930994777595280251703192929520539661356936256196728567365632.00000000\n",
      "Iteration 65, loss = 4066830336164008295645937663959651947966753954863870776475945505808122101497929881132277559183619826684140070256001046522834918239487370453245950063967433680701236790695320293526441337641349538538731346404331499605655979500336513997917388800.00000000\n",
      "Iteration 66, loss = 4066830271962309841822332576501424926484514379558546543870986721793056622454587676365983294172902798452166154866181209257110434911771559029182659595883102611320752274622655581652907561223325615973397024197554193521598331723214215852905725952.00000000\n",
      "Iteration 67, loss = 4066830207760612868595357872257132945456651466033155532020667674617868404622676999739711961574224888871018530223506023869486057948375745556226436345448604691170533415193991118168594996095571116192186725045324030670839170385290174098319605760.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 68, loss = 4066830158452477257047254221242248092878652109124517203534858320873322292784234372474288381220392862382355534671506251703997606864209786151757442025935184188563532930591914069902487480115248610986973850443514821089526707458514958126248099840.00000000\n",
      "Iteration 69, loss = 4066830142197279281849938939797146935114275989801564632717804886959898048940186238463556167551923272288280131013152536206643308641584204664475232555359084560288119274360894513840434489532075505616678219394630791933832254614083651931727724544.00000000\n",
      "Iteration 70, loss = 4066830129316053803495163699417720407478939018952568579273264139050242956176727127661032515536741795264362139983913746145921225097889621328665599392189702242164073541065902452155510680232522191189189117466643229119296798462327924535620272128.00000000\n",
      "Iteration 71, loss = 4066830116475226404200394451255872677510818713581500816173943067191660017322513317384219726008912706035787184796501450046601292025339133949184999804717796671838469339317687690371336925934169541585669058857458786853963835834997809900444188672.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 72, loss = 4066830103634882419705445322443815655896678479358629629464496073553002864000693444824894369011855856301996158551817992146966086903267977606161846779916414325194602031835554027667888657908783430998642527557916610260586695605898406739207782400.00000000\n",
      "Iteration 73, loss = 4066830090794545097895332918094466316327233223145457936150927895693793386130315448895672207868975040496923440669285467698781360420636812043120496234539281150086930179251421482715935840689609722940174100003836578217052744353191157334886318080.00000000\n",
      "Iteration 74, loss = 4066830077954208516383535705352084496984976297822252853214679586254522538865653217036461512932113784017263868160325269189646687120165645455632679297987064549594391154989289061958593629115570726273767683977030117790168036320083036125777625088.00000000\n",
      "Iteration 75, loss = 4066830065113871194573423301002735157415531041609081159901111408395313060995275221107239351789232968212191150277792744741461960637534479892591328752609931374486719302405156517006640811896397018215299256422950085746634085067375786721456160768.00000000\n",
      "Iteration 76, loss = 4066830052273533872763310896653385817846085785395909466587543230536103583124897225178017190646352152407118432395260220293277234154903314329549978207232798199379047449821023972054687994677223310156830828868870053703100133814668537317134696448.00000000\n",
      "Iteration 77, loss = 4066830039433195810654883300697068958049452198292771162896655184256955474648803465178783563297451777276632569139155369906042454490112149790955094053030748449656242768914891302908124571812914890706300389787516450042916939342362159717600460800.00000000\n",
      "Iteration 78, loss = 4066830026592857748546455704740752098252818611189632859205767137977807366172709705179549935948551402146146705883050519518807674825320985252360209898828698699933438088008758633761561148948606471255769950706162846382733744870055782118066225152.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 79, loss = 4066830016731231366535150166144742647964407070697871803885925135648836774410736943796476686083804556173827252146222891024760037790647792347019944643750931174027170819410343348302950251397676681606789387313074576083120495504299867118864695296.00000000\n",
      "Iteration 80, loss = 4066830013480189846720067611677606863820842186519368102741482790974311486067066330412300431214459783908937993443264100483759039872506678713124315366690928154372742734526939114184552078603691810913369031132386484048693572564455872572407414784.00000000\n",
      "Iteration 81, loss = 4066830010903945787466753831851476086611838455595522146580822457180294550362376577949811753499850871559732798760417598786284697618791760611737335786411934895209119547518740875720022164646969743976758026884971971749095421841543506566483804160.00000000\n",
      "Iteration 82, loss = 4066830008335782824622071633682796109390654719547195069243845795436369306650967413732488180694751555420422501993081047759190891823625659652723230138922270134835450323463898345624863472980757232789064854355865226792636256262714519503172009984.00000000\n",
      "Iteration 83, loss = 4066830005767714619961733961205958721249577337414594120203812291444588780471176050476632282260155832933994813043002216930503893344939427564561426421021926925198783124625071712439862303891787779785309157317775648767280222792573941427094945792.00000000\n",
      "Iteration 84, loss = 4066830003199647895898026671943056373562876617061926391918418524292685515502816215360799316237599229098393414840068037979917001230573193427506689920771416864792381582430245327644082346093087749565677483334233213975222675761631619741443424256.00000000\n",
      "Iteration 85, loss = 4066830000631581171834319382680154025876175896709258663633024757140782250534456380244966350215042625262792016637133859029330109116206959290451953420520906804385980040235418942848302388294387719346045809350690779183165128730689298055791902720.00000000\n",
      "Iteration 86, loss = 4066829998063513707472296901810284157962286845466624324970311121568940354960380781059121917986466462101777473060627354139693163819680726177843683311445480169364445669718592433857911824850552977734352123839874772774458338480147848174927609856.00000000\n",
      "Iteration 87, loss = 4066829995495445502811959229333446769821209463334023375930277617577159828780589417803266019551870739615349784110548523311006165340994494089681879593545136959727778470879765800672910655761583524730596426801785194749102305010007270098850545664.00000000\n",
      "Iteration 88, loss = 4066829992927379519046567131677511942361697073871322258022203718845195194417945346757444519735333695105161531281186670299469326408788258928180676702119543473936509757006939540071741303608018205903026764345516331573694001198664076608411795456.00000000\n",
      "Iteration 89, loss = 4066829990359312054684544650807642074447808022628687919359490083273353298843869747571600087506757531944146987704680165409832381112262025815572406593044116838914975386490113031081350740164183464291333078834700325164987210948122626727547502592.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 90, loss = 4066829988386984853506664044910324631799436054216422521314490024915718740916614208712955625398157308503608918986026592269492715431711389898065166159083780239734376579133229651083640985976647256742175736185171384901776528704013710420153991168.00000000\n",
      "Iteration 91, loss = 4066829987736774920887354112481568930470908749422795238255497845456948695915305405082095148771045323534722147423575717095382398847331169425068266364256963171650198739848148531031818018998553917540955239589031908938262809032926829481394438144.00000000\n",
      "Iteration 92, loss = 4066829987221524332320734896659620726483856009102106182117798094490292595320649620821569894333676598683889559590432834502167402759404188263462389787021364740741155314473708585271846582658886196812684211074092434598384995161306448611699064832.00000000\n",
      "Iteration 93, loss = 4066829986707890555274494150454736698676117932468494190046690972669605737609222565466086833843025440535366467639249802794268556508914969710773914883403565269282208944347539880541437875293428156347846358124633371020454372894182046086696271872.00000000\n",
      "Iteration 94, loss = 4066829986194279727476024344065845797911218113423847119672499771867016428674984196280959225738980621474650882268808875196921358905385719400244981853358179610892380252203375025843957942927146169036930862520655027558650290434630617613289390080.00000000\n",
      "Iteration 95, loss = 4066829985680667419080924154463019856691941632599266828543668834224549858529314298955808685222896683763109006151223295721474054937536471138608981605662960803272285903415209922757256799270594758941891343862129540863547721535880932749456965632.00000000\n",
      "Iteration 96, loss = 4066829985167055110685823964860193915472665151774686537414837896582083288383644401630658144706812746051567130033637716246026750969687222876972981357967741995652191554627044819670555655614043348846851825203604054168445152637131247885624541184.00000000\n",
      "Iteration 97, loss = 4066829984653439841097463008829497893344635347390239804776727485259862195815111448025461739366650571038372672421762833014379234273197978713122846674972856889571565892550879219805412089376953093183564260435984281006745610859985050240941031424.00000000\n",
      "Iteration 98, loss = 4066829984139829013298993202440606992579735528345592734402536284457272886880873078840334131262605751977657087051321905417032036669668728402593913644927471231181737200406714365107932157010671105872648764832005937544941528400433621767534149632.00000000\n",
      "Iteration 99, loss = 4066829983626216704903893012837781051360459047521012443273705346814806316735203181515183590746521814266115210933736325941584732701819480140957913397232252423561642851618549262021231013354119695777609246173480450849838959501683936903701725184.00000000\n",
      "Iteration 100, loss = 4066829983112602915912162440021020069686805904916498931390234672332462485378101756050010117818398757903747044069006094588037322369650233928214845931887200466711282846186383910545308658407298862898445704460407820921437904163735995649443758080.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 11034484883728507469022106201400691481934450576690992791047191668925507069494432833496009197638752262946816.00000000\n",
      "Iteration 2, loss = 13284869541041780018995548066689128159687347325700249061061020414508439266994710231635150318158208592510976.00000000\n",
      "Iteration 3, loss = 13284031317723092311211071689849171054556000088870115398264872158484158830756095497757268084233383054409728.00000000\n",
      "Iteration 4, loss = 13282874127304256911489195609460115878452829494230216811124691007085342493497195954400783589625755737260032.00000000\n",
      "Iteration 5, loss = 13281717004353705084142305974497945611921563599303564163067581551054763748759831462440118869880964248502272.00000000\n",
      "Iteration 6, loss = 13280559982201170955566151321184443858342461931078635032111565479078375019745567614084417984493602079768576.00000000\n",
      "Iteration 7, loss = 13279403060841339898898474975320536242914464421933246137217568363478266616022038828522598387997236747304960.00000000\n",
      "Iteration 8, loss = 13278246240265434326117251636360492033166215061371426701077283684950447408392977076733979862524923991818240.00000000\n",
      "Iteration 9, loss = 13277089520464688871416314010675098107300488295166172255192766919808430084504810455985680770229956654399488.00000000\n",
      "Iteration 10, loss = 13275932901430309650485826121829933585280420837796223610508562221258218426037676102206622786551741008576512.00000000\n",
      "Iteration 11, loss = 13274776383153527223447668007223612808417410316278254195589937733740823848357103403901324746970157528645632.00000000\n",
      "Iteration 12, loss = 13273619965625555854135909028366059970457347082603649027255012274207252677705026914523907380271437221724160.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 13, loss = 13272844103864781860530088377633335215247548132186996776293856079600036332978192169323525398615004394029056.00000000\n",
      "Iteration 14, loss = 13272602846857352247172183451256144607313355074799387116330844621368689254277114232854528399096837799149568.00000000\n",
      "Iteration 15, loss = 13272371592914141542879634791945573177856297885167112992908433536380931738835869570482000612743021460455424.00000000\n",
      "Iteration 16, loss = 13272140344044987826110610193893245650549537575654263594205290389975237740972351996452567754494842012434432.00000000\n",
      "Iteration 17, loss = 13271909099205056640919601388216437131339824309272480803210676826411568737544161608872918924353417456386048.00000000\n",
      "Iteration 18, loss = 13271677858394182987392525281542159876126396926390719450984705904853623201174900702857773292045546937122816.00000000\n",
      "Iteration 19, loss = 13271446621612295569270210166857394489310161098773342736133599317532628866948841875061639140786514035736576.00000000\n",
      "Iteration 20, loss = 13271215388859327164365437006121294112183399316941035960200366088552314742231156430901624280465014700113920.00000000\n",
      "Iteration 21, loss = 13270984160135190180131223416432149201581509977632873910044078582653903472982522131981838887602683044167680.00000000\n",
      "Iteration 22, loss = 13270752935439831653632184701151716777916594437016669247486501485494878519551255764388489115445158299566080.00000000\n",
      "Iteration 23, loss = 13270521714773186399717078156725236250926623594991268325539037487115219525442977987919982537217842597593088.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 24, loss = 13270366576173521581333044865816979525519825946674599736150609283468249960627848283259174331031579661434880.00000000\n",
      "Iteration 25, loss = 13270318333491893266303054556792991617101200690332392077600676196095581266364778808857779930484285680648192.00000000\n",
      "Iteration 26, loss = 13270272090373624131413850867461869042507251842086845117299064228358639780193489905751268727538104292868096.00000000\n",
      "Iteration 27, loss = 13270225847625435544561191272339652606800765180628484728885104894892269746299107579660561484024497806770176.00000000\n",
      "Iteration 28, loss = 13270179605038402984904281874960391714841409653625389159499238776022476691774498421338031097347178437279744.00000000\n",
      "Iteration 29, loss = 13270133362612518304299217337379741292846431623564914203267891208004258072057865013258478514159321450807296.00000000\n",
      "Iteration 30, loss = 13270087120347763169422210649222924924804635406043610396975519197411358161885163165990205864430571196776448.00000000\n",
      "Iteration 31, loss = 13270040878244159987669001489836891563618593504221249306774453069542533958801335777727510544864695692558336.00000000\n",
      "Iteration 32, loss = 13269994636301696536823731852305123598614175461828864623854330828780281645963686722182593975441457837768704.00000000\n",
      "Iteration 33, loss = 13269948394520370779850425402141534761345692869488295296746758809188350587231766644974156392824151449010176.00000000\n",
      "Iteration 34, loss = 13269902152900172531569200466915693709584703680308736068109768681085487601903328774195698980329245609295872.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 35, loss = 13269871126530168235343032862907102935714144828575818673848385865724409257288126098195082325588104705998848.00000000\n",
      "Iteration 36, loss = 13269861478342600649994895295651805164519025639131573573762193061410538234172846729513464654272916717830144.00000000\n",
      "Iteration 37, loss = 13269852230025732996604128767219170897746579539925091737955214064141205360401246061037979927649066055368704.00000000\n",
      "Iteration 38, loss = 13269842981757086058845152193420683277310160240547020261963095109796931203346762306690492907533988775591936.00000000\n",
      "Iteration 39, loss = 13269833733494900599203108609487839166597375488695804043715684505239674883797630649825855191450925232291840.00000000\n",
      "Iteration 40, loss = 13269824485239154210282258336073689612705652781211671517060651925170679404208908192249769382696107408097280.00000000\n",
      "Iteration 41, loss = 13269815236989853003190530376636493420972057346229105836403178367398696673001942997106134771279653853200384.00000000\n",
      "Iteration 42, loss = 13269805988747007163107806403606681933625031230638912259085232161604979870878981836301450173885095484588032.00000000\n",
      "Iteration 43, loss = 13269796740510594282638346737637306197762001931281319218954482982490772000295081811641418193808664284889088.00000000\n",
      "Iteration 44, loss = 13269787492280638806213867392561401434731230360694259333631654821291080694935635175701635991090834454872064.00000000\n",
      "Iteration 45, loss = 13269778244057118326438628689032018691630144015717961036964417352707148957255699030287806169027837977165824.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 46, loss = 13269772038837110735374921789535714704583873541032575569060971362707044699855155854942949888146365690675200.00000000\n",
      "Iteration 47, loss = 13269770109213544396228061235420117944284251033536813742841484734165126021660739559089845967643224578195456.00000000\n",
      "Iteration 48, loss = 13269768259562452155451228546347703549985181954628496678625500864366344737675576990251022179320034148483072.00000000\n",
      "Iteration 49, loss = 13269766409919952132422574719587169459599823632803494708093999913672746694105791201734600145223525288902656.00000000\n",
      "Iteration 50, loss = 13269764560277723035178773379476109072491023758273912582858856532500483257215769545931046634908938821173248.00000000\n",
      "Iteration 51, loss = 13269762710635752641503966519098004777984651874770783994109708725232050610162815896552563068356037644910592.00000000\n",
      "Iteration 52, loss = 13269760860994034840290225134994597770743642754159625787441375494058696844525582190455250155554703209922560.00000000\n",
      "Iteration 53, loss = 13269759011352583890789383568568491929887815262087565323132612500534176413287213908308206239861878799990784.00000000\n",
      "Iteration 54, loss = 13269757161711381459677654809444910839405973714151153137967876751232233591183666860679733451247208764538880.00000000\n",
      "Iteration 55, loss = 13269755312070439769170896864540372109972248566619355540757530241770372195057637173857630369730930203951104.00000000\n",
      "Iteration 56, loss = 13269753462429760856305086068340962010032328228870333582969966638084842861049574202223196758649749301624832.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 57, loss = 13269752221387926336963969314740273585146404030420983041466459263068248735779488757980913740061492742455296.00000000\n",
      "Iteration 58, loss = 13269751835463764698476988582749315728178900788527843413863566672896537266974290665266268867540898983968768.00000000\n",
      "Iteration 59, loss = 13269751465534034731548747054698320022595167541629200143141170990449683556657013332134187358018403676717056.00000000\n",
      "Iteration 60, loss = 13269751095605989393372934146640668321595748850469746436780337037282105934491352072337010127951922039095296.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 61, loss = 13269750725677944055197121238583016620596330159310292730419503084114528312325690812539832897885440401473536.00000000\n",
      "Iteration 62, loss = 13269750355749917050345095340900141335608107152554288487274212124373206415424073742174353537849314414428160.00000000\n",
      "Iteration 63, loss = 13269749985821900230672951115647697392848326192689089501470889494313137699224703443715372994496719344369664.00000000\n",
      "Iteration 64, loss = 13269749615893891559144712228339598523871298870336534721541141527998071527587130562781591504490949007900672.00000000\n",
      "Iteration 65, loss = 13269749245965893072796355013461930997122713594874785198953361891364258536651804453754308831168709588418560.00000000\n",
      "Iteration 66, loss = 13269748876037910882735808474472953617939635594438324088112731582220450634840073179777424264540119636115456.00000000\n",
      "Iteration 67, loss = 13269748506109936840819167273428321312539311231514507183145675936821645277590139323325738751258354417401856.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 68, loss = 13269748257901653862833168903534937887524488858204872395342793498391852661522635634986832397012997861539840.00000000\n",
      "Iteration 69, loss = 13269748180716843127717121902689260761655285349234751615387187839281767110850359172885680913877964653854720.00000000\n",
      "Iteration 70, loss = 13269748106730914245433674806762186275482321338631575793577215496656901712366678283062182624001797532942336.00000000\n",
      "Iteration 71, loss = 13269748032745323511122299235525432351293633284803134515520591699449641913197590220534445048018856855994368.00000000\n",
      "Iteration 72, loss = 13269747958759734813846899998774764695550633640352854288932361568178632750168951512388007235372622362443776.00000000\n",
      "Iteration 73, loss = 13269747884774137968427595424079751966024880358389929856470556773162621042578515386716370369379563135303680.00000000\n",
      "Iteration 74, loss = 13269747810788547234116219852842998041836192304561488578413932975955361243409427324188632793396622458355712.00000000\n",
      "Iteration 75, loss = 13269747736802958536840820616092330386093192660111208351825702844684352080380788616042194980750387964805120.00000000\n",
      "Iteration 76, loss = 13269747662817369839565421379341662730350193015660928125237472713413342917352149907895757168104153471254528.00000000\n",
      "Iteration 77, loss = 13269747588831777068218069473618822537715816552454325795712455250269832482042612490986719828784506610909184.00000000\n",
      "Iteration 78, loss = 13269747514846186333906693902382068613527128498625884517655831453062572682873524428458982252801565933961216.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 79, loss = 13269747465204534219788642164272781719104678524595911873325721030436365559169012270430060461293248226263040.00000000\n",
      "Iteration 80, loss = 13269747449767572072765432764103646293930837822801887717334599898614348449034556978009830164666241584726016.00000000\n",
      "Iteration 81, loss = 13269747434970389962973500746993186679898484157561942445615714028774626514390629637931470080697079290658816.00000000\n",
      "Iteration 82, loss = 13269747420173271001296835098951401387682471183044989769417031802958674300100632283673402660165808681910272.00000000\n",
      "Iteration 83, loss = 13269747405376158150728098454367874900803523436662520247623530574951473994231982992559234529644656623353856.00000000\n",
      "Iteration 84, loss = 13269747390579039189051432806326089608587510462145567571424848349135521779941985638301167109113386014605312.00000000\n",
      "Iteration 85, loss = 13269747375781926338482696161742563121708562715763098049631347121128321474073336347186998978592233956048896.00000000\n",
      "Iteration 86, loss = 13269747360984809413842006848186864097938238150624306424901058561248619895923788347310231321397669530697728.00000000\n",
      "Iteration 87, loss = 13269747346187692489201317534631165074167913585485514800170770001368918317774240347433463664203105105346560.00000000\n",
      "Iteration 88, loss = 13269747331390577601596604555561552318843277429724884226908875107425467375765141701937995770345246863392768.00000000\n",
      "Iteration 89, loss = 13269747316593462713991891576491939563518641273964253653646980213482016433756043056442527876487388621438976.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 90, loss = 13269747306665136772647429164739471975214665779790213438011424194016526408524129204475602997526478683373568.00000000\n",
      "Iteration 91, loss = 13269747303577739861763639348836255099599383138799454293582733902592371586988249566352697458860323751591936.00000000\n",
      "Iteration 92, loss = 13269747300618305476841229279900249445238600815129626290707350394560677836199913452718325205403197476175872.00000000\n",
      "Iteration 93, loss = 13269747297658885351170653552366847669997637357106925648110722548082738538394722819753051295303014484541440.00000000\n",
      "Iteration 94, loss = 13269747294699453003284219817916928284082543442815258696703732705987295423746836060499978805182594392522752.00000000\n",
      "Iteration 95, loss = 13269747291740030840577667755897440240395891575414397002638711193573105489801196073153405131745705217490944.00000000\n",
      "Iteration 96, loss = 13269747288780608677871115693877952196709239708013535308573689681158915555855556085806831458308816042459136.00000000\n",
      "Iteration 97, loss = 13269747285821186515164563631858464153022587840612673614508668168744725621909916098460257784871926867427328.00000000\n",
      "Iteration 98, loss = 13269747282861760278386058900866803572444559154455489817506859324458034415683377402351084584761625325600768.00000000\n",
      "Iteration 99, loss = 13269747279902336078643530504361229260312218877676467071973444146107593845597288060623211147988029967171584.00000000\n",
      "Iteration 100, loss = 13269747276942909841865025773369568679734190191519283274971635301820902639370749364514037947877728425345024.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 17396226578008738610165451804238020225858073389084645834284361821195562964518998313198397012286060791595008.00000000\n",
      "Iteration 2, loss = 22799293564774995563043517125909975853701370907937693968252843518373110416099357933445018952186107101446144.00000000\n",
      "Iteration 3, loss = 22808527447700903723128788483049091019689256640884493710223821223181899444387692836085315080389085295017984.00000000\n",
      "Iteration 4, loss = 22807101025763574543287365604366555535837659661675856916188319336774922559210664289234964961864613944623104.00000000\n",
      "Iteration 5, loss = 22805659172031899723390979667496912650876257351534077431227459953187174240635359037912521899657697895120896.00000000\n",
      "Iteration 6, loss = 22804217386861547351159067723682336068310529102897023950428121336142755665317510818956809112127623588216832.00000000\n",
      "Iteration 7, loss = 22802775692808686051766589578700258023050925211945081746778600230768171657733461813611445900291160403869696.00000000\n",
      "Iteration 8, loss = 22801334089900404329626841228525876305861913589463913246977866257324381613578726584400685115667135160385536.00000000\n",
      "Iteration 9, loss = 22799892578130978113646322767256776584359063882820963824839718134947097978190619319872191782111172511137792.00000000\n",
      "Iteration 10, loss = 22798451157494658888299818275157509304809684826845746236557230591537025563221061955998033763438422908731392.00000000\n",
      "Iteration 11, loss = 22797009827985653323270632473798727007675940150048230106018817704397355185232090632361684130056500771028992.00000000\n",
      "Iteration 12, loss = 22795568589598245495609170795222360434356153137308505012911852856408801834122816955036006959167855484993536.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 13, loss = 22794559704440989760185999184443279025671068806005025027365098663484807121388657136924872658135904906379264.00000000\n",
      "Iteration 14, loss = 22794244819080662596346381909327656550245424287550494203445906921250747620523315865365228167091172665196544.00000000\n",
      "Iteration 15, loss = 22793956561278450599146580283777388173027262058550041751370548089897212298592235767030146171419790281801728.00000000\n",
      "Iteration 16, loss = 22793668345873720228908864607279885811293778496525337446550293542633871714017687062685315309289436172255232.00000000\n",
      "Iteration 17, loss = 22793380134169701635381678486511476489933249970977090493060456705008716389751533826958734927996346643251200.00000000\n",
      "Iteration 18, loss = 22793091926110017810883988684548594706073258531920040451637939125349140502717444202998154920860189989011456.00000000\n",
      "Iteration 19, loss = 22792803721694554681401120470170409426755253254177168440052695511225108429050254345450788541025419939282944.00000000\n",
      "Iteration 20, loss = 22792515520923238913637925801877814987934451400134676605442553888931597267693787496589844308370613891760128.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21, loss = 22792227323796058285378546672754293778936722513523598638997152262851103201805347530127523642875534746058752.00000000\n",
      "Iteration 22, loss = 22791939130312943537399787710272912672608660675486458790791105991151104602609656397099634591092172266668032.00000000\n",
      "Iteration 23, loss = 22791650940473862077126027562656291373819251335972680237330116418851591291859524427405380939633227519229952.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 24, loss = 22791449195817827108914446930877306692951990242049690119887952256884223404695099828906491010796490632200192.00000000\n",
      "Iteration 25, loss = 22791386227107093618623729796772475403470545863387660005975361135228549216807188902955940703643463323746304.00000000\n",
      "Iteration 26, loss = 22791328582525714103642855170404839264981472295125543920303714633278314803356266918327455222854285449494528.00000000\n",
      "Iteration 27, loss = 22791270945839551959034329758695550589674249341080506887664675825248656444569232733048501628012141727449088.00000000\n",
      "Iteration 28, loss = 22791213309310445288201193224237559076943389443252538068177280702143044514427421345981301293216737943093248.00000000\n",
      "Iteration 29, loss = 22791155672927096689618694507196419927000974190360446018130257981515450977998713358437366753095580974579712.00000000\n",
      "Iteration 30, loss = 22791098036689510237358786276544305676738380401160552840460394995238377107564007479179297534322083188703232.00000000\n",
      "Iteration 31, loss = 22791040400597685931421468532281216326155608075652858535167691743311822903123303708207093636896244585463808.00000000\n",
      "Iteration 32, loss = 22790982764651603401446977929546289190795773120055752587568211566373282003272108501707757427451003330887680.00000000\n",
      "Iteration 33, loss = 22790925128851274869651172475256041881333005990638201306472316460040258224853117985969087486006596525359104.00000000\n",
      "Iteration 34, loss = 22790867493196712558249910176326992008441437143669171000690368419930255384709028287278882392583261269262336.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 35, loss = 22790827145560396371828042158373803016801927978032804096877135947723659751279040345924863704182338835447808.00000000\n",
      "Iteration 36, loss = 22790814552152669055190340778541092980193531351248953219695397011284959836815787196964380724471179247616000.00000000\n",
      "Iteration 37, loss = 22790803023515546932762262758940924534878787093284494390054332681933089132392767729853211479859351502454784.00000000\n",
      "Iteration 38, loss = 22790791496434080666957163080618760303562647205620787252098432544084529983175398300033105791045897765781504.00000000\n",
      "Iteration 39, loss = 22790779969360705508050063981031254338520869368012776546602173505023497583822864472735660075629404483485696.00000000\n",
      "Iteration 40, loss = 22790768442293180716466997525483511349496203264483305658332523034874292179840876428531134663233069918453760.00000000\n",
      "Iteration 41, loss = 22790756915231469625560389693225978504466257526225475660858395146784402320701345788556133813796182769532928.00000000\n",
      "Iteration 42, loss = 22790745388175604827905861836036036098562046703289863377674088495733838184651462222911453740706041971081216.00000000\n",
      "Iteration 43, loss = 22790733861125553730927792602136303836652556245625891985285304426742589593444036061496298230575348588740608.00000000\n",
      "Iteration 44, loss = 22790722334081332630913992667415471866303293428258849895439192267300661636202662139361065390097752089690112.00000000\n",
      "Iteration 45, loss = 22790710807042949676008367369817885261297011888701381314009326681153056857489137874030954272620077207519232.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 46, loss = 22790702737567483374715836544634446992555708359702060250973460147318413255693143751698012755370041675874304.00000000\n",
      "Iteration 47, loss = 22790700218899324497010375977432426702919980103868455905232927426695353733377470374055440902926164572504064.00000000\n",
      "Iteration 48, loss = 22790697913183057325974339620704102544581587098339275241990501626856963860962070297888270801810906727579648.00000000\n",
      "Iteration 49, loss = 22790695607777006289702329481081427443239681474200300597482673926755450545297950241099459721006906102251520.00000000\n",
      "Iteration 50, loss = 22790693302371607104942746377006358244518066851072862422860819326254140794577623586326572907948884164083712.00000000\n",
      "Iteration 51, loss = 22790690996966476808932039425094676480627322265862683042066928629337915014396611709885254855336078434369536.00000000\n",
      "Iteration 52, loss = 22790688691561578735022634604596829319545056349762863528669915849154261754226826232912109823107777611956224.00000000\n",
      "Iteration 53, loss = 22790686386156888438782815901679781539923008190235471265049056994468173380382874902831540651223507496075264.00000000\n",
      "Iteration 54, loss = 22790684080752442586860157337093085973783569156087405177635438052132161343392846098506943079743979387879424.00000000\n",
      "Iteration 55, loss = 22790681775348237105182706241864570084235362428562343163492271690273724370975841111175717581995780920573952.00000000\n",
      "Iteration 56, loss = 22790679469944267919678509947022061334387011188903963119682770577020361190850961232075264631305499727364096.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 57, loss = 22790677856051231250941711079138070303405768591295496469565842399492074717889430590968917392594055379025920.00000000\n",
      "Iteration 58, loss = 22790677352318141326970323938996613652031739834719615291010450994410132026785824180866140069669124742053888.00000000\n",
      "Iteration 59, loss = 22790676891175334411049129187001058863658960569306681640233857407668593494289242646013614172852068573773824.00000000\n",
      "Iteration 60, loss = 22790676430094526722103650853527170488258609137603510481486880256651416532509211060400685192024410288029696.00000000\n",
      "Iteration 61, loss = 22790675969013820884956989244357595535142678174808391896159586402446771377751647193852744378032061172154368.00000000\n",
      "Iteration 62, loss = 22790675507933135418170090980048883266483631305794883825516229207604632584398576871117801197406773890252800.00000000\n",
      "Iteration 63, loss = 22790675046852458099527098053684516071607338074294019960746446676507496335607303965908057070128311341940736.00000000\n",
      "Iteration 64, loss = 22790674585771772632740199789375803802948291205280511890103089481665357542254233643173113889503024060039168.00000000\n",
      "Iteration 65, loss = 22790674124691111610385017538900126755637505248804936437080456278058226382586555573013767868918210978906112.00000000\n",
      "Iteration 66, loss = 22790673663610458736173740626368794782109472929842005189931397738196097767480674920379620901680222631362560.00000000\n",
      "Iteration 67, loss = 22790673202529809936034416382809635345472817429635396045719126530206470424655692976508073461115646650613760.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 68, loss = 22790672879751294676313186928003936473021685014006582242067134595019341883611697666321553316192477270507520.00000000\n",
      "Iteration 69, loss = 22790672779004689728549158040686597260799285082711636735753775775994957416689852252341316336962410716856320.00000000\n",
      "Iteration 70, loss = 22790672686776146271281510833765045465446787232156867258520321318885655308226490263926249074962013897097216.00000000\n",
      "Iteration 71, loss = 22790672594560000214965835309164523430553948857090257017930717749797724750537899040101541480155449233768448.00000000\n",
      "Iteration 72, loss = 22790672502343878603081875798397036617009371394561579394961838171944801826534700068852431045389358771208192.00000000\n",
      "Iteration 73, loss = 22790672410127748843054010949685204729682040294520257566119383930346876357969703680078121557276443575058432.00000000\n",
      "Iteration 74, loss = 22790672317911619083026146100973372842354709194478935737276929688748950889404707291303812069163528378908672.00000000\n",
      "Iteration 75, loss = 22790672225695493397070233921233713491918754913193936011371262779023526693120609611292102107724025549553664.00000000\n",
      "Iteration 76, loss = 22790672133479384007402132417382744289048307906934224697212745196788107585960106766330790252978172187377664.00000000\n",
      "Iteration 77, loss = 22790672041263266469590125575587430012395107263161869177180652950807685934237806503844279344885494091612160.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 78, loss = 22790671949047136709562260726875598125067776163120547348338198709209760465672810115069969856772578895462400.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 79, loss = 22790671884491433657618014835914458350577549679994784587607800322172334757464011053032665827787945019441152.00000000\n",
      "Iteration 80, loss = 22790671864342100445849351051534472897458939237466829177534766562749954047236945843948819851921694608326656.00000000\n",
      "Iteration 81, loss = 22790671845896413754384366022599894237601874488640014637946727263439600495861126473583843843558042025066496.00000000\n",
      "Iteration 82, loss = 22790671827453171506090982376968837712570900993606461860431087087630010313024532360778583839241809518657536.00000000\n",
      "Iteration 83, loss = 22790671809009953702229314745170816408888188411110841700536170903055427763873330500548920994966051213017088.00000000\n",
      "Iteration 84, loss = 22790671790566723676151789106456277494531345372346255231830892722863341397879432514031459570670055806992384.00000000\n",
      "Iteration 85, loss = 22790671772123501798218168805686083653957255971094312968999189206416257576447331945039197199720885134557184.00000000\n",
      "Iteration 86, loss = 22790671753680283994356501173888062350274543388598692809104273021841675027296130084809534355445126828916736.00000000\n",
      "Iteration 87, loss = 22790671735237041746063117528257005825243569893565140031588632846032084844459535972004274351128894322507776.00000000\n",
      "Iteration 88, loss = 22790671716793828016273402565431157058452234129825841974630503993330003567589232820537211033526548383662080.00000000\n",
      "Iteration 89, loss = 22790671698350597990195876926716618144095391091061255505925225813137917201595334834019749609230552977637376.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 90, loss = 22790671685439456564992637214729955681819070430684838533191788669355931805497395279859768898098943729074176.00000000\n",
      "Iteration 91, loss = 22790671681409604589297934066153779724004304889702007021749616312212460243663217589588357998949978167312384.00000000\n",
      "Iteration 92, loss = 22790671677720459102861031722422518918250138302423999907958433788605386988826256297990163743930422917070848.00000000\n",
      "Iteration 93, loss = 22790671674031810653202354993296307613243943603417289352455305753443468952258937475429111743067176415789056.00000000\n",
      "Iteration 94, loss = 22790671670343170351687583602114441382020502541923223002825752382026553460253416070393258795550754648096768.00000000\n",
      "Iteration 95, loss = 22790671666654517827956954204016057540122931024160190344385837014992134151405198539069607268014095780020224.00000000\n",
      "Iteration 96, loss = 22790671662965873452370230143862018772008113143909801891819496311702717387118778425271154793824261645533184.00000000\n",
      "Iteration 97, loss = 22790671659277233150855458752680152540784672082415735542189942940285801895113257020235301846307839877840896.00000000\n",
      "Iteration 98, loss = 22790671655588576553052876685609596161995723745896380780813240241378881313984140780149050792097768642969600.00000000\n",
      "Iteration 99, loss = 22790671651899940325610057963399902467663659503158636534120474201834467094259518083875797371254759242072064.00000000\n",
      "Iteration 100, loss = 22790671648211300024095286572218036236440218441664570184490920830417551602253996678839944423738337474379776.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 12008943787644619343132364555643892567528935254300462465848940557947221293229911902127539123236858146848768.00000000\n",
      "Iteration 2, loss = 15325566598670402760876781784486851520178224711004456541929076486843283673865537828843784624081595738882048.00000000\n",
      "Iteration 3, loss = 15327957570114595491135456963029800517405724721670960549167239882791542215526133414460414693277157753356288.00000000\n",
      "Iteration 4, loss = 15326869458095150361227124993252482620650261976436239293955959333819027449458568586555290419368702557290496.00000000\n",
      "Iteration 5, loss = 15325778731844028706970154663495481411213391480884511705712630755660180993142455960058985075753700756029440.00000000\n",
      "Iteration 6, loss = 15324688081131696978260078856033198594463591699399749016328329047093638705819160049533966279852356462968832.00000000\n",
      "Iteration 7, loss = 15323597508033320390836412567696852625205887533451981004861629453494948313774077463912540874314407886192640.00000000\n",
      "Iteration 8, loss = 15322507012544981724516664581742549282381467748837505697591512379454136518923099727955263962653864352546816.00000000\n",
      "Iteration 9, loss = 15321416594661166722912897444334759883511808158874356769576324135540731289069824531483676192407087405334528.00000000\n",
      "Iteration 10, loss = 15320326254376336685205457687804920524770123664342635278253687041089252958332457311743721051069964387090432.00000000\n",
      "Iteration 11, loss = 15319235991684997725366171203178365208134774172341985413365884066031735855919089302369936819545918675091456.00000000\n",
      "Iteration 12, loss = 15318145806581596883323550181383926150659155718005380872071781868148945862964780459939168641973894328090624.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 13, loss = 15317381528677362184835493396600930544938276864476586666844812750477922493557309064120951708564743430078464.00000000\n",
      "Iteration 14, loss = 15317144319898910178595675873477476959904902125912969577344569696529541416738409740762409679316699387527168.00000000\n",
      "Iteration 15, loss = 15316926292385652170699915092604187385278753121065532596999044999069030541347343175550801617846036263337984.00000000\n",
      "Iteration 16, loss = 15316708282810740748075224904991629924245805959009753541051908850207937349296792482947712451370401138737152.00000000\n",
      "Iteration 17, loss = 15316490276350288259279928180938365639798660026814302387143314955860700104651794384034884751702567380058112.00000000\n",
      "Iteration 18, loss = 15316272272992787488083711408543064082243490747247399390317444442147475250013945978849955429789304975523840.00000000\n",
      "Iteration 19, loss = 15316054272738179360443260887709223466655334248342374057990880996916994337310215990335231348866134606610432.00000000\n",
      "Iteration 20, loss = 15315836275586423135639049928715118424120422342536005360795751301444244643731617330864717242198932605370368.00000000\n",
      "Iteration 21, loss = 15315618281537476035915575507352937317279298432886911217895788371067962810328713558431118079716869120458752.00000000\n",
      "Iteration 22, loss = 15315400290591283061301476592498350898098375466184743239644363225509381661309372104739340251328877200146432.00000000\n",
      "Iteration 23, loss = 15315182302747813656257108166860065139892327301757085654015570875724741654566852654069887306984364093472768.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 24, loss = 15315029474699443120897594027603464521004886286914921048339288118976079487136708303442514674242023678017536.00000000\n",
      "Iteration 25, loss = 15314982040024194810578930925262599345973528149059381514198631087828535360591691322335958927930848792543232.00000000\n",
      "Iteration 26, loss = 15314938440460627116466547079982863541092499982424192685925355182563883617681453245704973223355791517417472.00000000\n",
      "Iteration 27, loss = 15314894843987717627836064562830460393373897871215606402979028732660655157166511171657814159513488793272320.00000000\n",
      "Iteration 28, loss = 15314851247641224554860923917705790716632293359303713197075216620535654936798052646692668008320221526884352.00000000\n",
      "Iteration 29, loss = 15314807651418829750600056499442681019674276574341236497181927010735659028744712384890404092604353012039680.00000000\n",
      "Iteration 30, loss = 15314764055320541363197367645985476376282601153840820509172734567005669977568287803776221465712707982327808.00000000\n",
      "Iteration 31, loss = 15314720459346355318580904688362004249565890279046143130110851957473186510987880194587520600971874070953984.00000000\n",
      "Iteration 32, loss = 15314676863496275690822620295544437176415520768713526462933066514010709901284388266086901025055263644712960.00000000\n",
      "Iteration 33, loss = 15314633267770290257706656460616257546157362166574004198829016241000736331615115891986564157942639603220480.00000000\n",
      "Iteration 34, loss = 15314589672168417352556800193952241774802610157031025801014244131869521527244107261718207869664357597052928.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 35, loss = 15314559107659918857278255384952093723306673735980576930309095867454422243904901513227418515069074390646784.00000000\n",
      "Iteration 36, loss = 15314549621008061603309300422271915291105312997239534052211511517473600808917953043185546008706526446878720.00000000\n",
      "Iteration 37, loss = 15314540901332892940795117079193990387191590179407191170347422160416102706556467894368650851109679510061056.00000000\n",
      "Iteration 38, loss = 15314532182255971188990727282361813955278375586803248783677113291126679512063624217572751234068303631613952.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 39, loss = 15314523463184480175099245225435629199570460396376669611744317807881452268008759321322020672685612690571264.00000000\n",
      "Iteration 40, loss = 15314514744117963603061971983532111988233640907419378125628854540960278478931217824205312179539421605920768.00000000\n",
      "Iteration 41, loss = 15314506025056397028447191542818227099919656207393441707709999499128150511145607473647028594589256176893952.00000000\n",
      "Iteration 42, loss = 15314497305999802858650643582640923487531078799458631924140083007683825362196871167841467314538884420861952.00000000\n",
      "Iteration 43, loss = 15314488586948162760348541092625424735056712999211499311703562073201047306820964717356930469357950687248384.00000000\n",
      "Iteration 44, loss = 15314479867901493029828694748660420990062066081677332282147586023169821434141482957243816165740104443232256.00000000\n",
      "Iteration 45, loss = 15314471148859787555983175547287653447210072818721647681066973859781395835737077824358225113675227138621440.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 46, loss = 15314465036002136315472528245719875452787912245990502649176897679194881752917985945009889683023339045519360.00000000\n",
      "Iteration 47, loss = 15314463138683098525443867066870620144468812196454752233406059345310006824152714157677138434516692643086336.00000000\n",
      "Iteration 48, loss = 15314461394757554936147577502033742598459229243864990237985652337702970854805869319513096836044090467418112.00000000\n",
      "Iteration 49, loss = 15314459650950886655322239541733232737488152372619708783614408371874308133661998110859344277734232812421120.00000000\n",
      "Iteration 50, loss = 15314457907144497448425659406026541653576387586182491380413096639311982563759688452443659296553122282864640.00000000\n",
      "Iteration 51, loss = 15314456163338307871054760049956304877342086918805057021114364168502219335621415523395351122369217726251008.00000000\n",
      "Iteration 52, loss = 15314454419532319960245517808008608677230938779865566757186604625381269085387628678095719518519225325977600.00000000\n",
      "Iteration 53, loss = 15314452675726531678961956345697366784797254759985859537161424344012881176917878562163464721666438898647040.00000000\n",
      "Iteration 54, loss = 15314450931920940990168099328536492931595346449787774309570429658460804974071715821217286968474152260861952.00000000\n",
      "Iteration 55, loss = 15314449188115543819791994087553814580733837030514988971476833236852539204568241746494586732268953045827584.00000000\n",
      "Iteration 56, loss = 15314447444310352390049498629665849342886856958436469831690997074805587685250152464283162593071078353928192.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 57, loss = 15314446221740586215102874834303002217968587365377711396941896539481335766315474982619090556528255557435392.00000000\n",
      "Iteration 58, loss = 15314445842277230471676693587547085497558456555546682529477443977364751876514087426924827814908357754486784.00000000\n",
      "Iteration 59, loss = 15314445493492506346209767625552797470902511655625536647626086704607464785961556566481414813183964744777728.00000000\n",
      "Iteration 60, loss = 15314445144731511652831162091977450568070847601902479321092543923233838095503554893798944920749902131101696.00000000\n",
      "Iteration 61, loss = 15314444795970553626100130579151656497261574916986320920990087128712722855573641599979870768376550818578432.00000000\n",
      "Iteration 62, loss = 15314444447209595599369099066325862426452302232070162520887630334191607615643728306160796616003199506055168.00000000\n",
      "Iteration 63, loss = 15314444098448649794853925560416585966317160003422970429595535535287996192556511138629521043650085293916160.00000000\n",
      "Iteration 64, loss = 15314443749687710101446681057965568311519083002910261492708621734193136677890642034242144761307089631969280.00000000\n",
      "Iteration 65, loss = 15314443400926790778399199900375413341177890096179163070505644592460783524629266473667766112331155803996160.00000000\n",
      "Iteration 66, loss = 15314443052165863307207813404840913297053943551935420442429092786983427826806093495568188410008397242433536.00000000\n",
      "Iteration 67, loss = 15314442703404939910088379578278585789821373826447999917289328313378573401263819226231210234359051047665664.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 68, loss = 15314442458891064082466155529677294565773879464206368186138467511891247190813959196387786833845321457467392.00000000\n",
      "Iteration 69, loss = 15314442382998406378218363087934280593433396804136025352336975194647184611380647424165512723543602707300352.00000000\n",
      "Iteration 70, loss = 15314442313241482330891936507293502926248229599809038900944319132645483681902724666766087709233127176011776.00000000\n",
      "Iteration 71, loss = 15314442243489296429246463941289385663734302609084658165035330038362762415110000200269912216101234227019776.00000000\n",
      "Iteration 72, loss = 15314442173737118675744896713229613475003129255872921634999915607825043692879073151298935776316166011617280.00000000\n",
      "Iteration 73, loss = 15314442103984936848171376816197668749380579083904863002027713845414823698367247393565359809857685429420032.00000000\n",
      "Iteration 74, loss = 15314442034232752983561880584679637755312340502558643317587118417068353067714972281450484080062498663825408.00000000\n",
      "Iteration 75, loss = 15314441964480581341168242360078124371918232377481389941956884984339386253905393295623406930287548998615040.00000000\n",
      "Iteration 76, loss = 15314441894728399513594722463046179646295682205513331308984683221929166259393567537889830963829068416417792.00000000\n",
      "Iteration 77, loss = 15314441824976223797129131569472493726010197261679755830417662457327698173303089843300154287380706384412672.00000000\n",
      "Iteration 78, loss = 15314441755224044006591588006926635268833335499089858248913854360853728814931713439947878084258931985612800.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 79, loss = 15314441706321267618845557396514725262956423581014635271802646000994513191157471821350413546154162357534720.00000000\n",
      "Iteration 80, loss = 15314441691142734040960022573680036200042638639622405653573953871609450039130360112524658960757112424103936.00000000\n",
      "Iteration 81, loss = 15314441677191351675937908858935184188740431290010801625057495058332610616603314786302333673899064737923072.00000000\n",
      "Iteration 82, loss = 15314441663240918569680767014706533273129022710622247580812484571348567635525668601765698101946098514919424.00000000\n",
      "Iteration 83, loss = 15314441649290483426387648835991796089071925721855532485099080418428274018307573062847762766656426108518400.00000000\n",
      "Iteration 84, loss = 15314441635340042171986601653818800099677763504954334234980495267699228492668129460785928141356635151925248.00000000\n",
      "Iteration 85, loss = 15314441621389619250909341482020580526294796972456585448077453110396438692292730048155791386087199845908480.00000000\n",
      "Iteration 86, loss = 15314441607439180033544270634333670805346323164933548249427261625603643802793735800475256524124115072712704.00000000\n",
      "Iteration 87, loss = 15314441593488742853215176121132847352843537766788672102245463806747099549435190907176021425497736482914304.00000000\n",
      "Iteration 88, loss = 15314441579538305672886081607932023900340752368643795955063665987890555296076646013876786326871357893115904.00000000\n",
      "Iteration 89, loss = 15314441565587874603664916098189459253175032198633402962287049166842762951139449183721450518255097853509632.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 90, loss = 15314441555807317289079733641620990983553961405640197315396413828934669190244151505620657847297437744496640.00000000\n",
      "Iteration 91, loss = 15314441552771608129059455075670749648836378326107958129988603003934155796470189938597947214213980337733632.00000000\n",
      "Iteration 92, loss = 15314441549981331248647837065824561992886799174310005113991632508091537784736691002477222204175029563817984.00000000\n",
      "Iteration 93, loss = 15314441547191247886653970832156569839277618913437351987492060276192730206345880732579974711123166212653056.00000000\n",
      "Iteration 94, loss = 15314441544401158413552175595030318880331373424430215706587307046485170719533722399538827928061184311296000.00000000\n",
      "Iteration 95, loss = 15314441541611062829342451354445809116048062707288596271277372818968859324300216003353781854989083859746816.00000000\n",
      "Iteration 96, loss = 15314441538820971319204679782833471888656128808903298938904225923325049201347608315931335308590395774992384.00000000\n",
      "Iteration 97, loss = 15314441536030892031282766218137652271938325366786967915341441023298742895237696754796687342211944790622208.00000000\n",
      "Iteration 98, loss = 15314441533240802558180970981011401312992079877779831634436687793591183408425538421755540559149962889265152.00000000\n",
      "Iteration 99, loss = 15314441530450706973971246740426891548708769160638212199126753566074872013192032025570494486077862437715968.00000000\n",
      "Iteration 100, loss = 15314441527660619537905427837786726858208212081009236969690394002303563162520323046910647466352586719756288.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 22940676816271855472975251794623186759344415214275696987454041994811246162504683070761053256043000309481472.00000000\n",
      "Iteration 2, loss = 28738072171319881934620164078879394227206118221491091840136073411512856366597294986940454728690971430617088.00000000\n",
      "Iteration 3, loss = 28739448984117751548266119001461510105018738204779885291759043339033498402542296505167485885501693686185984.00000000\n",
      "Iteration 4, loss = 28737234078820245552649402835594468423203117253943768601742482622535453249329606866347485950192249481461760.00000000\n",
      "Iteration 5, loss = 28735017703595729405401015105050686236495139572997824309084874776882882226467357540083533723214044230320128.00000000\n",
      "Iteration 6, loss = 28732801498561014268455855971346645071831423878333953223888003638423423337118909412460092476983663667445760.00000000\n",
      "Iteration 7, loss = 28730585464452044573416001236701581524411887130733151950352285987659402675107912068218260023384457963634688.00000000\n",
      "Iteration 8, loss = 28728369601256019586206164990549384681530564797831373061091925081191822733850622575270323554554770649776128.00000000\n",
      "Iteration 9, loss = 28726153908959714869267983749217999793778303196607070423295241660881553689549832290218219488598059110105088.00000000\n",
      "Iteration 10, loss = 28723938387549987466482147408478822849573485019165139962886303106039491164026306744915874777086028064751616.00000000\n",
      "Iteration 11, loss = 28721723037013649606937866505409351931529347951290934473484516145379016784010925674828621578182846199103488.00000000\n",
      "Iteration 12, loss = 28719507857337513519724351577087085122259129678769806748709287507613512176234568815421792050052682198548480.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 13, loss = 28717959302965469402563805741491704105972848356528265482932376674194196924330249842034132275864127756304384.00000000\n",
      "Iteration 14, loss = 28717480325231624729143986711561838764132824749995559978610277014689625591449190150432821715618241246658560.00000000\n",
      "Iteration 15, loss = 28717037326261017819453898700662148984678459649308787792743392155083105856467262620305777982665773742555136.00000000\n",
      "Iteration 16, loss = 28716594350554755092111454583431116583710065162491943866922766778136745173179727930285077556634733716176896.00000000\n",
      "Iteration 17, loss = 28716151381689128062049090896052912239251232578403189600860319234411628089363520423201230550412758286860288.00000000\n",
      "Iteration 18, loss = 28715708419656550807290938012342272259558325372770769326258037577310385617985244383093918298106020482777088.00000000\n",
      "Iteration 19, loss = 28715265464456888883462557856217502927215908526636053646201939855040475773775242420797356419491912199700480.00000000\n",
      "Iteration 20, loss = 28714822516090020068405370358513428135482677477309379472588406111426860388306553273433559114368062433787904.00000000\n",
      "Iteration 21, loss = 28714379574555899547327896160536149978553487218471203673112775695872025466489291144613931589326935150297088.00000000\n",
      "Iteration 22, loss = 28713936639854384727711791848259629665230149093650252644987491992838426478492001227647490410799097511411712.00000000\n",
      "Iteration 23, loss = 28713493711985373757758240697379553773228242633938473814792871705513531617292215803469247411949240347262976.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 24, loss = 28713184061413274126975899370921645678935228243493101931166888123702313423742362801979139732814679165108224.00000000\n",
      "Iteration 25, loss = 28713088281335024693253323556656061542656764628575962124354583274831021227913914360298781576977360460185600.00000000\n",
      "Iteration 26, loss = 28712999694609798506226622005544456034800562059865701316431635814562467174606729888554665673388095814238208.00000000\n",
      "Iteration 27, loss = 28712911111443481947525853764724624526055760895084783042019232909652815372647483694389674855845257090695168.00000000\n",
      "Iteration 28, loss = 28712822528551953393888702358998061266894631206717063648038790141208976372744346120037559106619733939585024.00000000\n",
      "Iteration 29, loss = 28712733945933721734980490944549617755073257329948653459626144043895484520088389758386891663242600114028544.00000000\n",
      "Iteration 30, loss = 28712645363588778822657314183434948916808885627266908270907719953967337270117817191912473472367030880436224.00000000\n",
      "Iteration 31, loss = 28712556781517120582847219406681882215210139279915505978946730539552033350551729711851705007319613872013312.00000000\n",
      "Iteration 32, loss = 28712468199718759237766064621206935260951148744163412892553537796267076578232823444492384848120586189144064.00000000\n",
      "Iteration 33, loss = 28712379618193666268910181144204900295792276288716374291170630401004958047194807428496316308056061264265216.00000000\n",
      "Iteration 34, loss = 28712291036941853898495426982592294930407652369843356483608370349383181574280377790151297967146276197761024.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 35, loss = 28712229109239240568341901418543089608198883591870941973266338969551114486972288594136992203367652893130752.00000000\n",
      "Iteration 36, loss = 28712209953842272729814572099455949046400684293951950627729236426824062589435073660144881953924750816313344.00000000\n",
      "Iteration 37, loss = 28712192237019939886894956790708890689636319217599613882420258306803088217095948166593049930695064860753920.00000000\n",
      "Iteration 38, loss = 28712174520865644844270330856009224022603056071651461990940449316433233560560794602773405126911399341064192.00000000\n",
      "Iteration 39, loss = 28712156804722565721731402601700551417530174961858059484436164971059381493339786262390257255031979607326720.00000000\n",
      "Iteration 40, loss = 28712139088590433630529295875619485439586805850302147569079441367096448044893608367112037554611589451087872.00000000\n",
      "Iteration 41, loss = 28712121372469207829944483988044300719859180549420505215502405185819420492413273829312750758916105204400128.00000000\n",
      "Iteration 42, loss = 28712103656358933134768446297668895164152444065532675556009717077825812830988668445380991661353062902005760.00000000\n",
      "Iteration 43, loss = 28712085940259568804281656114771543403552828211075437561233503724390612337810805127690764995188338875957248.00000000\n",
      "Iteration 44, loss = 28712068224171147431059734791129625733191347536099368054668063780493829191126873546342866973809232060612608.00000000\n",
      "Iteration 45, loss = 28712050508093660866958776988798797079285248403091822830439822582390460846375076283812098543868917722382336.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 46, loss = 28712038122649625261797521677745581720698968617224968200893154509246179111893757811006347502872958334926848.00000000\n",
      "Iteration 47, loss = 28712034291594968644174271279165363105401096882277714543371055664153993767547094688959731508609586623938560.00000000\n",
      "Iteration 48, loss = 28712030748251415916552179119166452312241985270711130410013349474447830019844701152177775329258693137006592.00000000\n",
      "Iteration 49, loss = 28712027205039716453606265574559525471602235888753085722840852006372842371148118008166900408225638595428352.00000000\n",
      "Iteration 50, loss = 28712023661828505879294672306613303057927704757553693388082834362998007396159379915667968688002068069220352.00000000\n",
      "Iteration 51, loss = 28712020118617739378825919956633887165413246870793410273434635893725811099788601078292385375180445523640320.00000000\n",
      "Iteration 52, loss = 28712016575407412878128055855649105257167485409715914275959469266683752209754882787277550943087358591893504.00000000\n",
      "Iteration 53, loss = 28712013032197506006841316658798094648733536280539594880973397822509324364653731498810467758355745440006144.00000000\n",
      "Iteration 54, loss = 28712009488988059505685229055802580709025167670827673117844294879927540287294134300517131087719729735925760.00000000\n",
      "Iteration 55, loss = 28712005945779048930228077032829528216694118668042216368951436447703392343990698939821943771138837278883840.00000000\n",
      "Iteration 56, loss = 28712002402570445761966191907073729413500751540888969913737311202729371628777134455386709121899181501317120.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 57, loss = 28711999925485512268546538503604734418104574857226654460125372732474724966559363052305188874780467972276224.00000000\n",
      "Iteration 58, loss = 28711999159275557907476138443415665041597161648003244012862555146482092990649540789167232172212079187460096.00000000\n",
      "Iteration 59, loss = 28711998450607689880031531954861163512102065444497338073518634139774123348798915053916423052403577943228416.00000000\n",
      "Iteration 60, loss = 28711997741966107764825545674763870005770203856781640282326578374444362463306758254957760029451667258736640.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 61, loss = 28711997033324574538482991422332646942134864094141807726375970591584616845185385961150291326580704975781888.00000000\n",
      "Iteration 62, loss = 28711996324683045386212389838873596415390901150258297273362150140597372499344912376105422150383155059621888.00000000\n",
      "Iteration 63, loss = 28711995616041536604301551600275408573103822300156397335032266348972634514908932334873550607552666977435648.00000000\n",
      "Iteration 64, loss = 28711994907400040044606571368593738341490873906323463705512744552965400347315648419929477644742415995633664.00000000\n",
      "Iteration 65, loss = 28711994198758555707127449143828585720552055968759496384803584752575669996565060631273203261952402114215936.00000000\n",
      "Iteration 66, loss = 28711993490117099888151995601868640857852875762489783784651936275293448551780763803955125565876274800361472.00000000\n",
      "Iteration 67, loss = 28711992781475644069176542059908695995153695556220071184500287798011227106996466976637047869800147486507008.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 68, loss = 28711992286058830925957795077429447067647112698506929678885040441728851973719197689307483656663771606155264.00000000\n",
      "Iteration 69, loss = 28711992132816892201864709228235441664555253336743170507023354772498341863732736708841166257569772144164864.00000000\n",
      "Iteration 70, loss = 28711991991083338966735551275385404043113118189823599833838507230519254296767105105604002066975133729292288.00000000\n",
      "Iteration 71, loss = 28711991849355041284425336296637939011547079238559541949109317804066807972160807806120227285082448479649792.00000000\n",
      "Iteration 72, loss = 28711991707626747676187073986862646516872417106051806167316915709486862919835409215399052029863175596802048.00000000\n",
      "Iteration 73, loss = 28711991565898470364236622352976044169763262248569358797271662942396922956633605459728274881337552181133312.00000000\n",
      "Iteration 74, loss = 28711991424170197126358123388061614359545484209843233530163197507179484265712700412820097259485341132259328.00000000\n",
      "Iteration 75, loss = 28711991282441911666263766416230666938653575714848141954244370076344541757949099239624121057612892983001088.00000000\n",
      "Iteration 76, loss = 28711991140713622132097456775427546980870290401096728275388755313637097977904599357665545329067032466948096.00000000\n",
      "Iteration 77, loss = 28711990998985357042362863148457462244435265999883247214153864542164661831545491728282566760561646151663616.00000000\n",
      "Iteration 78, loss = 28711990857257075656340458845598687360434734323644477741171824443202220596062789263849190085362610369200128.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 79, loss = 28711990758173718731397443185663879126581345298360700384160277236567247350600593598650916580078112506642432.00000000\n",
      "Iteration 80, loss = 28711990727525322838434920677880732972180219788495304343914365438976142784041503985032454046912487880654848.00000000\n",
      "Iteration 81, loss = 28711990699178606487708355350749683896243865212852539265165893665958823489455119472117381871450782884167680.00000000\n",
      "Iteration 82, loss = 28711990670832957543833389294327839485848237151366165155855702843536837532464196655003385684423117987905536.00000000\n",
      "Iteration 83, loss = 28711990642487300451814517899961650001669855452367146840671937357369849030911476420364190444048628358053888.00000000\n",
      "Iteration 84, loss = 28711990614141651507939551843539805591274227390880772731361746534947863073920553603250194257020963461791744.00000000\n",
      "Iteration 85, loss = 28711990585796002564064585787117961180878599329394398622051555712525877116929630786136198069993298565529600.00000000\n",
      "Iteration 86, loss = 28711990557450349546117667061723944233591594449151702409804577558231389887657809260259602356292221302472704.00000000\n",
      "Iteration 87, loss = 28711990529104692454098795667357754749413212750152684094620812072064401386105089025620407115917731672621056.00000000\n",
      "Iteration 88, loss = 28711990500759039436151876941963737802126207869909987882373833917769914156833267499743811402216654409564160.00000000\n",
      "Iteration 89, loss = 28711990472413386418204958216569720854839202989667291670126855763475426927561445973867215688515577146507264.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 90, loss = 28711990452596725625803432023910407803986104913376973666360193385016935586399343483610319756809549727662080.00000000\n",
      "Iteration 91, loss = 28711990446467035854623850583026129977188300082637456990675363962630211365157188918103868480825552648798208.00000000\n",
      "Iteration 92, loss = 28711990440797687695592194314833313117731376985001317451401524809779745979502833565005734613725116809347072.00000000\n",
      "Iteration 93, loss = 28711990435128543240158171495249123102843294825181283058967052250554344207893413650037577080295299309633536.00000000\n",
      "Iteration 94, loss = 28711990429459419155083912020525795772412096759142859181216516350691448797688487278882417180232543643893760.00000000\n",
      "Iteration 95, loss = 28711990423790282847793794538885950831306768236835468994655618455211049570640864781439458700149550877769728.00000000\n",
      "Iteration 96, loss = 28711990418121154688647582395190450963984193352040723013968295223475652888155039701521699273413382845235200.00000000\n",
      "Iteration 97, loss = 28711990412452026529501370251494951096661618467245977033280971991740256205669214621603939846677214812700672.00000000\n",
      "Iteration 98, loss = 28711990406782898370355158107799451229339043582451231052593648760004859523183389541686180419941046780166144.00000000\n",
      "Iteration 99, loss = 28711990401113757988993087957187433751342338241387518763095963532651959023854868335480622413184641647247360.00000000\n",
      "Iteration 100, loss = 28711990395444621681702970475547588810237009719080128576535065637171559796807245838037663933101648881123328.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 19870208244132999286901760172381287530573293609663362148488671355082552637795587580168929746757871921528832.00000000\n",
      "Iteration 2, loss = 24159525976216007451117747884205842951794640774961468051258185792377174854256764289648669080929162145824768.00000000\n",
      "Iteration 3, loss = 24158445776261676698567909663767716018702562344086595244173363101160801108659020542947556099936476584738816.00000000\n",
      "Iteration 4, loss = 24156438888840024705978377598065045984740413523891581829481524309749455092263771150676969122625923626041344.00000000\n",
      "Iteration 5, loss = 24154432020535107944321842305779851063093900443849601878492732201078022558632458834064595584776782747795456.00000000\n",
      "Iteration 6, loss = 24152425318933246601140227804134769121337838576893761920768141701855835818725837750439023907782293670854656.00000000\n",
      "Iteration 7, loss = 24150418784044100301033312226150885128926665194263768019430516681783411450554746375923731834303178065575936.00000000\n",
      "Iteration 8, loss = 24148412415853857940081547756163299024092964712027691218770802093081436421872562005306323941471512497750016.00000000\n",
      "Iteration 9, loss = 24146406214348655451430001881868867765481422902421415224901707573628081160780978719460610959665012764835840.00000000\n",
      "Iteration 10, loss = 24144400179514604323792026077131413090388464625142891126315218770066508461696298346684806459220920463523840.00000000\n",
      "Iteration 11, loss = 24142394311337926045823693878063415232177688846310766790796579289597415470619087851867311230658611093962752.00000000\n",
      "Iteration 12, loss = 24140388609804711735878593413667833243688636331841382792153838119501458620561155519493341210948264418869248.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 13, loss = 24139037244724194234928424901422921085101468779500300751544654978609563531683346580209635405430326871195648.00000000\n",
      "Iteration 14, loss = 24138617155522768747648551562482178627212426816653682769622078048035328537111450097023901241517930857889792.00000000\n",
      "Iteration 15, loss = 24138216063003072632205649035156402218516697065302072843244275874740893373562716579317649112130504150220800.00000000\n",
      "Iteration 16, loss = 24137814980172391471607433494168276398650609061764176625614155530187214455255744198482125065461561856884736.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17, loss = 24137413904006614988682874937342772362646382725756781992357440030384900572135195234615470907632654835777536.00000000\n",
      "Iteration 18, loss = 24137012834505164665214694370631389871928509793882150326450328249438771060313453043428553851019227002044416.00000000\n",
      "Iteration 19, loss = 24136611771667938649404075069729815504212569797232229054473136890536294112768049905856385728785969185816576.00000000\n",
      "Iteration 20, loss = 24136210715494786200588768282667665394517620441823100367764734674394922655105733597682784054016623815688192.00000000\n",
      "Iteration 21, loss = 24135809665985633985473625967945833878798878990040966413462949627309633786270327361180957346589768289353728.00000000\n",
      "Iteration 22, loss = 24135408623140347559684210049482627239640910422927197794653799797487885520992173807185121226282794502586368.00000000\n",
      "Iteration 23, loss = 24135007586958820997349751134001559517867917452817419834980814556244644779967906507867687999586980918722560.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 24, loss = 24134737370105363808331323715287878881885738244965874567955175797145146506757741319324358070550007914692608.00000000\n",
      "Iteration 25, loss = 24134653366761260644067387329456036649327297844922519277861571609918272912356978435263601589351548712910848.00000000\n",
      "Iteration 26, loss = 24134573160952105995877998633577234183785411045538280124878759364363305780794523481024388261703204091199488.00000000\n",
      "Iteration 27, loss = 24134492956014301634709591699044104435243389392056450783160126013762950271005202545284141263471632430661632.00000000\n",
      "Iteration 28, loss = 24134412751343104542123842303481788858400387152293037624804490399645852823227383034056919773345343818694656.00000000\n",
      "Iteration 29, loss = 24134332546938457681113413081279871936777128863659531208696829875796995625528483024666330417896565120172032.00000000\n",
      "Iteration 30, loss = 24134252342800361051678304032438353670373614526155931534837144442216378677908502517112373197125296335093760.00000000\n",
      "Iteration 31, loss = 24134172138928786135314846474152026300950206408487983882667922775796493074401150550056851424317650895896576.00000000\n",
      "Iteration 32, loss = 24134091935323741080166945744365234902289658148168332458062739540282341359568224541024964152820453536169984.00000000\n",
      "Iteration 33, loss = 24134011731985278849169986539716222453979868389029164599199830050016440073061407703930505229388065024245760.00000000\n",
      "Iteration 34, loss = 24133931528913301664597104804872848070627793480918749835596298340058758680139131028471086013858588557049856.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 35, loss = 24133877487788822094208178979447553753179016569996904679916444635082322447490063030425774632580422148554752.00000000\n",
      "Iteration 36, loss = 24133860687699782640939713132154910318502405702186702556102922573293006023305154459152264229654648869355520.00000000\n",
      "Iteration 37, loss = 24133844647045699224691016725870250060795573123977221634374744062143075693117837842320831344834841208160256.00000000\n",
      "Iteration 38, loss = 24133828606523244999161102141508084226582377743778564497308471019636751486486902636317505751295192318607360.00000000\n",
      "Iteration 39, loss = 24133812566011436323643511581432757289536809773849562334067496159976251749844293426986743355382057863741440.00000000\n",
      "Iteration 40, loss = 24133796525510313938857771735365994618572637401753436174019692801886589205998997301954539423829561511510016.00000000\n",
      "Iteration 41, loss = 24133780485019845252228261251530415918558846077439609193670762290387753676703824591120097743250404327555072.00000000\n",
      "Iteration 42, loss = 24133764444540022115611074791981676115712682163395437187147129961734742617396977876958219260297761578287104.00000000\n",
      "Iteration 43, loss = 24133748404070889343797691715413673115839290665940463286753456466525070023168342955857498768379169298448384.00000000\n",
      "Iteration 44, loss = 24133732363612389899780774656215991402459396122486178051374719158543718082085337905141542894066854352912384.00000000\n",
      "Iteration 45, loss = 24133716323164564524279850304110356344486766720595802510378791356515699516956949812436346904094940409626624.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 46, loss = 24133705515029511674531102380962950024395169280328976282451778000667469117717821329177086640598184632516608.00000000\n",
      "Iteration 47, loss = 24133702155034906438462249541821453441229105287772576503280286068578851727055165759679208870431141346148352.00000000\n",
      "Iteration 48, loss = 24133698946924404707597298823405659344849107818655214402931002015389209762491023810139202097770605599260672.00000000\n",
      "Iteration 49, loss = 24133695738838510371326468696911988072385095637723354040777202472107252374555082786700336432520765292675072.00000000\n",
      "Iteration 50, loss = 24133692530753051960754574150440778247298403063717958692859647439182931120675303600859620121326048233127936.00000000\n",
      "Iteration 51, loss = 24133689322668000957377946501186822111349392365344773638620825593508737094885395291278856477472567853056000.00000000\n",
      "Iteration 52, loss = 24133686114583402175988065107844017570343208548923342010365397585682184292275243654346640294367860187201536.00000000\n",
      "Iteration 53, loss = 24133682906499227098081261287607156866040213883159409087535852092595763806878557728724774885298038668001280.00000000\n",
      "Iteration 54, loss = 24133679698415475723657535040476239998440408368052974870132189114249475638695337514413260250263103295455232.00000000\n",
      "Iteration 55, loss = 24133676490332127682357123021590404283086907909822428843470471991280813426321089467599098755895992235589632.00000000\n",
      "Iteration 56, loss = 24133673282249231863043457258615720162676234333543636242792148706159792437126598093433484722277653889941504.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 57, loss = 24133671120625821143071085977797892495878471898576481152152032477532270544680871459414574438205470034362368.00000000\n",
      "Iteration 58, loss = 24133670448627839576849600874952580186396753505273078133540892820913340454523582586170449735060953159958528.00000000\n",
      "Iteration 59, loss = 24133669807006549970995191856731756208504740943957704197891715052903165245509597240019754188537907002736640.00000000\n",
      "Iteration 60, loss = 24133669165390100362620553577451906057568389065152988551145887549424501506203277903837296330028752597680128.00000000\n",
      "Iteration 61, loss = 24133668523773687420893489318921608738654428555155171830831146032798349217425046946518234211580309493776384.00000000\n",
      "Iteration 62, loss = 24133667882157298923598141074224346641088728957695287728137128507407204562332208241774769253172340590641152.00000000\n",
      "Iteration 63, loss = 24133667240540914500374745498499257080414406178991725728379898313888561179520268245793903821437784054300672.00000000\n",
      "Iteration 64, loss = 24133666598924546373439160598662857667305590675313452140369817447859922885831923084863436496396876985139200.00000000\n",
      "Iteration 65, loss = 24133665957308206765007244381631666012436412902929433272917247904938793498109868885271165858069856483540992.00000000\n",
      "Iteration 66, loss = 24133665315691867156575328164600474357567235130545414405464678362017664110387814685678895219742835981942784.00000000\n",
      "Iteration 67, loss = 24133664674075556066647080630374490460937695089455650258569620142204043628632051447424821268129702047907840.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 68, loss = 24133664241751010811470216051675922167128403712674641899117651247394581998781102735044383307541920801095680.00000000\n",
      "Iteration 69, loss = 24133664107351447090801540382884240000363074584064538118889721971050806158996834630496354580300316360572928.00000000\n",
      "Iteration 70, loss = 24133663979027225021463842066195193529428788076857097837603614937926782313265946198377091305721735956922368.00000000\n",
      "Iteration 71, loss = 24133663850703984803466736971799728449316314889923284364083254886075565087231646578044313959435535950807040.00000000\n",
      "Iteration 72, loss = 24133663722380711992894010525626883074072827152938894067068596179244337682950157287610740399762037010333696.00000000\n",
      "Iteration 73, loss = 24133663594057463626753000093287072920177600328492436387674661463648117912354060249752764000129012270628864.00000000\n",
      "Iteration 74, loss = 24133663465734215260611989660947262766282373504045978708280726748051898141757963211894787600495987530924032.00000000\n",
      "Iteration 75, loss = 24133663337410966894470979228607452612387146679599521028886792032455678371161866174036811200862962791219200.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 76, loss = 24133663209087718528329968796267642458491919855153063349492857316859458600565769136178834801229938051514368.00000000\n",
      "Iteration 77, loss = 24133663080764478310332863701872177378379446668219249875972497265008241374531469515846057454943738045399040.00000000\n",
      "Iteration 78, loss = 24133662952441225870119900600560194687592843025016470093641775217539520331654473769225481528637300938899456.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 79, loss = 24133662865976316819084527684820481028830984749660268421751381438577628005684284026749393936519744689537024.00000000\n",
      "Iteration 80, loss = 24133662839096417111981041091773096713530324743958478395103515045300876909026306273880106676426343375175680.00000000\n",
      "Iteration 81, loss = 24133662813431576772185454097407459956234844261273312441783080970548573412161027296218853548184039661240320.00000000\n",
      "Iteration 82, loss = 24133662787766923839699689875761759895942697441378963223554864161931328440217088921637178646918704819863552.00000000\n",
      "Iteration 83, loss = 24133662762102270907213925654116059835650550621484614005326647353314083468273150547055503745653369978486784.00000000\n",
      "Iteration 84, loss = 24133662736437630196944019439386877386032534257859231095908792540314342313171908298761627424408272237494272.00000000\n",
      "Iteration 85, loss = 24133662710772993560746065893629867473305894712990170289427725059187102430351564759230350629836586863296512.00000000\n",
      "Iteration 86, loss = 24133662685108348776404207009928512486796501530608465277073082914314860002969423802173874781918076755509248.00000000\n",
      "Iteration 87, loss = 24133662659443708066134300795199330037178485166983082367655228101315118847868181553879998460672979014516736.00000000\n",
      "Iteration 88, loss = 24133662633779051059576583904581457439994961528332411046490223960825372603643344470535724032734231806345216.00000000\n",
      "Iteration 89, loss = 24133662608114398127090819682935757379702814708438061828262007152208127631699406095954049131468896964968448.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 90, loss = 24133662590821441576129851647415284376676979329656018532092009854025257054646940141786948678420542389223424.00000000\n",
      "Iteration 91, loss = 24133662585445446968050124720505986380807890780992900956190002180628902255104109239667732930377577605890048.00000000\n",
      "Iteration 92, loss = 24133662580312478900091007321632859029348794684455867765525915365678441555731053444135482304729116863102976.00000000\n",
      "Iteration 93, loss = 24133662575179547498779463943509284509912089956725733501292914537580492306886086027466627419141367421468672.00000000\n",
      "Iteration 94, loss = 24133662570046624245611825903330055064258138866508243442933488373227545602602916028322971586900442713423872.00000000\n",
      "Iteration 95, loss = 24133662564913688770228329856234308007930057320021787075763700213257095081477049902891517174639280904994816.00000000\n",
      "Iteration 96, loss = 24133662559780757368916786478110733488493352592291652811530699385159145832632082486222662289051531463360512.00000000\n",
      "Iteration 97, loss = 24133662554647825967605243099987158969056647864561518547297698557061196583787115069553807403463782021726208.00000000\n",
      "Iteration 98, loss = 24133662549514898640365652390835756986511319955587706386001485060835748607223046361647552044549444946886656.00000000\n",
      "Iteration 99, loss = 24133662544381967239054109012712182467074615227857572121768484232737799358378078944978697158961695505252352.00000000\n",
      "Iteration 100, loss = 24133662539249039911814518303560780484529287318883759960472270736512351381814010237072441800047358430412800.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 8955136297675644914124729832836602591198916375285603473260783233867918240494945867229062087035799193255936.00000000\n",
      "Iteration 2, loss = 10976441664569053353306044303973852412164945521870616366925659615493190448228372015409064114528459385995264.00000000\n",
      "Iteration 3, loss = 10975734820498489389466073942675913544561926262977900146112031062733570160157255005678048446852436968079360.00000000\n",
      "Iteration 4, loss = 10974428905070101595610763527697990444306277191291415247279016834219224348797294931038726798579281178918912.00000000\n",
      "Iteration 5, loss = 10973123014130840124126041107160231571812556218454181340462661920613771487289194933657552995177263823585280.00000000\n",
      "Iteration 6, loss = 10971817278556143493686536723971665073309969610625074546670590570317947898382460255578189415436351113789440.00000000\n",
      "Iteration 7, loss = 10970511698356113365698893094634096170967339474104749097666992161198658202565439273666162446082506517118976.00000000\n",
      "Iteration 8, loss = 10969206273512265675713851092400724988608039111459823969247741987717630061400690332007419569843821885784064.00000000\n",
      "Iteration 9, loss = 10967901004006122470390080593983010455392507053391401291613896342145343044872119837831807559458507622187008.00000000\n",
      "Iteration 10, loss = 10966595889819181351954535462259376279132920918062650577345787525517269089278241945793576027624299927961600.00000000\n",
      "Iteration 11, loss = 10965290930932970478173814577399540194326784464309157111049652827912639673024311126266471037089527755702272.00000000\n",
      "Iteration 12, loss = 10963986127329015969778540485087133667025913041588345125863335873474435638375132495242938887265813874606080.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 13, loss = 10962965741587051652936954970039212929683805708767099441780709318468982488243414130271099683121188028022784.00000000\n",
      "Iteration 14, loss = 10962661324412981945766679883318169503565868817161540327038285377535214554802208982632742284058170348273664.00000000\n",
      "Iteration 15, loss = 10962400406209697682625705802391911809375417533971704067066674366936000771101493892734162455034170276052992.00000000\n",
      "Iteration 16, loss = 10962139503717990423513569525410724972058963631845401468704683020043312827653147592658208107334466829025280.00000000\n",
      "Iteration 17, loss = 10961878607437774189282678647080097935049769959271959362311964097554144597471679747943301347728967149289472.00000000\n",
      "Iteration 18, loss = 10961617717366828610718828577565998092547470294055831647339421728955302687467294082972700139207931333705728.00000000\n",
      "Iteration 19, loss = 10961356833505007021231723233870214116462499160969422618062711966836741295527637082292821521528514177662976.00000000\n",
      "Iteration 20, loss = 10961095955852150532015208526078017068031160628516170259947128868170910802697659104162283954427633376165888.00000000\n",
      "Iteration 21, loss = 10960835084408130809802775381565972035175084905871928330483871478974021132129050822559202347692799375179776.00000000\n",
      "Iteration 22, loss = 10960574219172776743572411703502832468456445605271168506327871860601018848024066469454396581040692769325056.00000000\n",
      "Iteration 23, loss = 10960313360145962037093584753751249724242561345267905596438722725004364509674846073207281327595530187964416.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 24, loss = 10960109345860198329490206035942349044814803022659767681124406283256724125166294733353379218421074135875584.00000000\n",
      "Iteration 25, loss = 10960048477972507403643553091826587059148547107704182373428102793390139487973144582555839520969169188159488.00000000\n",
      "Iteration 26, loss = 10959996306347073813101436523390276049517282219291044943194391449353248270054323649133212238200609978712064.00000000\n",
      "Iteration 27, loss = 10959944136869818918798591936575303393500699573774505073653423503723455960547108413570309075641520489496576.00000000\n",
      "Iteration 28, loss = 10959891967641304450529975111268484372050097686125798958194609367722592125170107324140207194937779470467072.00000000\n",
      "Iteration 29, loss = 10959839798661123001100319150252565296027794680712716303139215854100529535833449504582953928748150242148352.00000000\n",
      "Iteration 30, loss = 10959787629929246052005955370722338407194152826241002387929731639749759286570843993560352590358746236977152.00000000\n",
      "Iteration 31, loss = 10959735461445702121750552455483011463788809854004911933123668047777790283348581752410599866483454022516736.00000000\n",
      "Iteration 32, loss = 10959683293210487136262157735562411928920388945248122835784237746312121253885764072371096230448861231972352.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 33, loss = 10959631125223584799252960535071849655023382824945346684164291407862747109058796118391443575561318398164992.00000000\n",
      "Iteration 34, loss = 10959578957485009369974795195413928521217610358743710838542584693983422301850823371140740245177768804876288.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 35, loss = 10959538157141879652713550330417389018180586981082144754388649757284741667959148006694500067970743223189504.00000000\n",
      "Iteration 36, loss = 10959525984186132847475682544922293042003617297980493270030839171451602286930887258494912267353242235568128.00000000\n",
      "Iteration 37, loss = 10959515550341578434705416563116367387392622127857952864819425199109538175840833040785698460447440231202816.00000000\n",
      "Iteration 38, loss = 10959505116886922892985452890652653579771623984651468767497639754820484527459454175757686115788671401590784.00000000\n",
      "Iteration 39, loss = 10959494683442273271981244213844690377372092706972069482925541389394555600965304031674111281030675419889664.00000000\n",
      "Iteration 40, loss = 10959484250007556238397642491193372116149245557205957158240958129126728495302205850808182476052029683793920.00000000\n",
      "Iteration 41, loss = 10959473816582775866306600391670871332994459354109453896380677305889504482751058341922499227526146560098304.00000000\n",
      "Iteration 42, loss = 10959463383167921970528236242846756685679292050791754440002730590001630382609614733110562718769495131815936.00000000\n",
      "Iteration 43, loss = 10959452949763000662170479048179286979540808875387341943512298979271858103299223087516272239792193949138944.00000000\n",
      "Iteration 44, loss = 10959442516368018052341257811126721019916075056030699561314563471508939553241231468283527080604361562259456.00000000\n",
      "Iteration 45, loss = 10959432082982963955860690859258627464576648545831022036067555737031621551733393103505828424522467054190592.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 46, loss = 10959423923014893848937022190524657863242746731965221294728015293581037734868897838221837789577757929242624.00000000\n",
      "Iteration 47, loss = 10959421488448619141603664309922279911997657393945122688705186657158177896918401909509590286640804433756160.00000000\n",
      "Iteration 48, loss = 10959419401698926471264741189093385805078187797243238581076105771652766678155780040902234749080460677087232.00000000\n",
      "Iteration 49, loss = 10959417315025626724110313965471730948365446700337106641146284827418711998524846179798603864675406688813056.00000000\n",
      "Iteration 50, loss = 10959415228352736421187129973553416049236075888441346046363590736371035183124232549336225410948295563411456.00000000\n",
      "Iteration 51, loss = 10959413141680247414351283875394096033907321724043312590854448834764733687392141731989900334552302567292928.00000000\n",
      "Iteration 52, loss = 10959411055008161740638752005479857170824872616521167326087252788536058147469023082140928398824133883854848.00000000\n",
      "Iteration 53, loss = 10959408968336459029689771018949836775531844472093299737378065938322502201950383055976311970396727679123456.00000000\n",
      "Iteration 54, loss = 10959406881665161688900080595150983800930809793919481390879218609422822848381164551690348445973851970469888.00000000\n",
      "Iteration 55, loss = 10959404794994257496053822727166780636347638125730745977780348806219516269918671442995239245535269657509888.00000000\n",
      "Iteration 56, loss = 10959402708323760710402831756399831160902148333174220858360212190266336919546049210560082712437924024025088.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 57, loss = 10959401076334173094328920767851155450383077947274195042519224191169227551385344027580396796782424302288896.00000000\n",
      "Iteration 58, loss = 10959400589421903670867599816099216534158112536824492021723514043842713348544642491510772798493485130842112.00000000\n",
      "Iteration 59, loss = 10959400172072738395656431762851785214757538817434050337599933456142372583706693040930691853595083597152256.00000000\n",
      "Iteration 60, loss = 10959399754738783668080553317210520379312317924772179967971856414425531879604072755515943743881753491668992.00000000\n",
      "Iteration 61, loss = 10959399337404849310864438216430118228323981125891920113027716032071197536905946013914193267535485220159488.00000000\n",
      "Iteration 62, loss = 10959398920070929212900157457052319956455463192658787618362331311270617647190964752981541134546160232431616.00000000\n",
      "Iteration 63, loss = 10959398502737021337151734704591039295261075715694621432507308586087541574318679618336687581577072345088000.00000000\n",
      "Iteration 64, loss = 10959398085403139942871004300448880123860637560646548915741403518075723771272236090648730951985164841910272.00000000\n",
      "Iteration 65, loss = 10959397668069268733770155568737152294688641452489281656317466779745159148928039334867273139076788255719424.00000000\n",
      "Iteration 66, loss = 10959397250735409746885164843941942076190775800600980705703892037032098343426538705373613906188648769912832.00000000\n",
      "Iteration 67, loss = 10959396833401569093323961129521508273704105833116129218305860287745293263189082265311652543330864934682624.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 68, loss = 10959396507003815347801676224493109114633639869940272593196513429200422535249069320972216332470942135484416.00000000\n",
      "Iteration 69, loss = 10959396409621401796421743456967229446613277293537920808111565985272882290261826230508026846879936732463104.00000000\n",
      "Iteration 70, loss = 10959396326151601741362326464992340731553314781586041505074827255900074442769515881369066823954896596762624.00000000\n",
      "Iteration 71, loss = 10959396242684832795835695188313819463677705424337806787007863439668213172265844851604154646048657356357632.00000000\n",
      "Iteration 72, loss = 10959396159218071998452969249579643269584849704602216274814474287181354446323971239364441521489242849542144.00000000\n",
      "Iteration 73, loss = 10959396075751311201070243310845467075491993984866625762621085134694495720382097627124728396929828342726656.00000000\n",
      "Iteration 74, loss = 10959395992284546329615564703139118344507761446374713147490908650335135722159325306122415745697001469116416.00000000\n",
      "Iteration 75, loss = 10959395908817787569268815098891028418860594136017283686765913163784527632357901048264002384474293145698304.00000000\n",
      "Iteration 76, loss = 10959395825351028808922065494642938493213426825659854226040917677233919542556476790405589023251584822280192.00000000\n",
      "Iteration 77, loss = 10959395741884263937467386886936589762229194287167941610910741192874559544333704469403276372018757948669952.00000000\n",
      "Iteration 78, loss = 10959395658417517399336495289605017447256157433079478458996107701941455271374976337832661590816286725636096.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 79, loss = 10959395593137963798381671440318816839618100467314881661918487197921730235190344652830954679972913509040128.00000000\n",
      "Iteration 80, loss = 10959395573661481902920075420608075413392303315785675725488855175510722440649075776490636688189394901794816.00000000\n",
      "Iteration 81, loss = 10959395556967523928944168356699183938825999222773460916349901095572411507291063061044144446941093058052096.00000000\n",
      "Iteration 82, loss = 10959395540274170954653232635157914192629152715075078393323865798700539507646508596843681916694527683330048.00000000\n",
      "Iteration 83, loss = 10959395523580822054434249582588816983323683026133017973234617833701168780282852841405818913121374675402752.00000000\n",
      "Iteration 84, loss = 10959395506887469080143313861047547237126836518434635450208582536829296780638298377205356382874809300680704.00000000\n",
      "Iteration 85, loss = 10959395490194114068816401805020191222484301601358091875714153574021174144853294558623594089291537742561280.00000000\n",
      "Iteration 86, loss = 10959395473500765168597418752451094013178831912416031455624905609021803417489638803185731085718384734633984.00000000\n",
      "Iteration 87, loss = 10959395456807412194306483030909824266981985404717648932598870312149931417845084338985268555471819359911936.00000000\n",
      "Iteration 88, loss = 10959395440114061257051523643854640789230827306397427461041228681214310054340979229166105788561960168587264.00000000\n",
      "Iteration 89, loss = 10959395423420708282760587922313371043033980798699044938015193384342438054696424764965643258315394793865216.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 90, loss = 10959395410364800821827185287633868951019470860551183260949099149036494065284217394975381497485450043981824.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 91, loss = 10959395406469501183477303948513982636261209975240284391313742879056291488551244652697238277790016429096960.00000000\n",
      "Iteration 92, loss = 10959395403130711625718098870218290609793637566016002480954345729004879938020091463989239592877062243745792.00000000\n",
      "Iteration 93, loss = 10959395399792042253081497526601688421621681310103222607230174869192255919775450183777926944829772878839808.00000000\n",
      "Iteration 94, loss = 10959395396453368806372943514012913696558348235434120630569216677507130629249910194804014770109071147139072.00000000\n",
      "Iteration 95, loss = 10959395393114703507808294839368484045277768798277662859781833149567007883286167623355301648735194149027840.00000000\n",
      "Iteration 96, loss = 10959395389776030061099740826779709320214435723608560883120874957881882592760627634381389474014492417327104.00000000\n",
      "Iteration 97, loss = 10959395386437364762535092152135279668933856286452103112333491429941759846796885062932676352640615419215872.00000000\n",
      "Iteration 98, loss = 10959395383098693352862514474032591212316211621161162187140926904192885192411794428340063941256619870912512.00000000\n",
      "Iteration 99, loss = 10959395379760019906153960461443816487252878546492060210479968712507759901886254439366151766535918139211776.00000000\n",
      "Iteration 100, loss = 10959395376421354607589311786799386835972299109335602439692585184567637155922511867917438645162041141100544.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 13998369904184558763430987108165183509628371012979333384341360420431130195909140670585323206129759475990528.00000000\n",
      "Iteration 2, loss = 16506210606036852633359217334711234788213265449787027131575788442621553519093396162887382980559106268463104.00000000\n",
      "Iteration 3, loss = 16504585030592656843734706191051493362065707654648737619244500740186416462517716822746354947060478125277184.00000000\n",
      "Iteration 4, loss = 16502835633321906538722824662173646568678628298103552410763511372218337834281226203904737771015045124194304.00000000\n",
      "Iteration 5, loss = 16501086416960553514077954586210295965736902093649635111151198339978465033218565216513226491501637992972288.00000000\n",
      "Iteration 6, loss = 16499337386007051538526760625508897025662676343816258072597039641642858006273842779422089746896357209145344.00000000\n",
      "Iteration 7, loss = 16497588540441889881487911072335170575652366024576870330826502939802923800223142628542194298227378192646144.00000000\n",
      "Iteration 8, loss = 16495839880245437627257470484275747604606769958593418884929829606811281929556036591287720869657211542962176.00000000\n",
      "Iteration 9, loss = 16494091405398041452735763739570310148524114465368079167844930689721794911217153596878552788644599842209792.00000000\n",
      "Iteration 10, loss = 16492343115880064331110926392347230390968133139428315024256866563078330351274717409584971489341935139684352.00000000\n",
      "Iteration 11, loss = 16490595011671844791139377982901845294154298662763657681229973610189748222111559541101661245861135283912704.00000000\n",
      "Iteration 12, loss = 16488847092753755991189135737792958383874786676792376240791280535281169310498150527605402309038123241177088.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 13, loss = 16487636646033004949658472048955438440178335213310059010872664044331030160857720040934346007135863198187520.00000000\n",
      "Iteration 14, loss = 16487266185206916419690914861746259552067593696419896118474399034810250080883626715490702281669882905886720.00000000\n",
      "Iteration 15, loss = 16486916657766862691481839010622846954806473795713568295691583484258663348223391403738302665262208494075904.00000000\n",
      "Iteration 16, loss = 16486567138499150818973584864791999977515481542388244440620870592972945125378561986499138550320876163694592.00000000\n",
      "Iteration 17, loss = 16486217626641185828885692540567831950053240081181891588067512136483696693026585508487308390835538356076544.00000000\n",
      "Iteration 18, loss = 16485868122192812906483960617007786470547430299354269826433589503635869704493311036724030173216525133021184.00000000\n",
      "Iteration 19, loss = 16485518625153865014818329666252789526451602627896172935310822087656911996261891511942723303853929455943680.00000000\n",
      "Iteration 20, loss = 16485169135524189376190574601846370984339126363445522054569684943328025857798625355545905532494787570040832.00000000\n",
      "Iteration 21, loss = 16484819653303604694398801654526852952543733071345983603523141802322904672603520027597897922172249152946176.00000000\n",
      "Iteration 22, loss = 16484470178491980599140525417184714250541364551395250288193998044721507727687816847695515619337118467227648.00000000\n",
      "Iteration 23, loss = 16484120711089152090501662796446967134231259643962276939642366729687533495675118110953477793716194657697792.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 24, loss = 16483878685980304841715663102795263395611074607823009524567150237106624496642048762408174716480226028683264.00000000\n",
      "Iteration 25, loss = 16483804610173569728827484884686941309022466726869947528410395605896014715681968465270513817167400466907136.00000000\n",
      "Iteration 26, loss = 16483734718824163695163904977807174629337605315619997611257412577109503951464768430898567646739864376836096.00000000\n",
      "Iteration 27, loss = 16483664827923537057446575700593209219266214007031296695912971963700641289644832140677482186051592562671616.00000000\n",
      "Iteration 28, loss = 16483594937319262487638340128181749824835995852068685505415839087906825612677869950708566476052293765562368.00000000\n",
      "Iteration 29, loss = 16483525047011307393163576908795416150915936300681587216271715294748046742316692190891024303354669051150336.00000000\n",
      "Iteration 30, loss = 16483455156999706403633883728697674761082738312298739703443292905140565492948937885706951644682723537190912.00000000\n",
      "Iteration 31, loss = 16483385267284422852401686567138972823314010518113244040499485932231870414046518656292952759975745922531328.00000000\n",
      "Iteration 32, loss = 16483315377865495443150535779354949438077832696310160205339774028810723592277972235893723152631153691721728.00000000\n",
      "Iteration 33, loss = 16483245488742881398124928341137792968014748249948106117127890210215861668693862182501967792578116993417216.00000000\n",
      "Iteration 34, loss = 16483175599916592939540722259404021023798887635296048084674196472064788460136884622405485259836872928002048.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 35, loss = 16483127197463818374307510907660178437959932075858023236933229214046371116734672860852638246037951229198336.00000000\n",
      "Iteration 36, loss = 16483112382956725751794594971431778899467259213682657035792146226409064495610117245530574644132687212707840.00000000\n",
      "Iteration 37, loss = 16483098405252321658020379658631773549741353490184232575506426191955853659226554778707888153010278739476480.00000000\n",
      "Iteration 38, loss = 16483084427590261431086229308107030380540413510519584988719839974344616274363721835963382141999304547500032.00000000\n",
      "Iteration 39, loss = 16483070449940044531318487659687851954571885655483290639174027510094577410073435266095700170598080627867648.00000000\n",
      "Iteration 40, loss = 16483056472301685217968989054776842150955588790722476887147744460759491519338840549773940582163550264360960.00000000\n",
      "Iteration 41, loss = 16483042494675177379929804489915742164354457688102660578235809828530606693738589623854204086685594906787840.00000000\n",
      "Iteration 42, loss = 16483028517060512869057028627160206920985738710111197506564648949662920388710885070811291630817389821558784.00000000\n",
      "Iteration 43, loss = 16483014539457728351998235487259789252871823225554986598565347811008944054783815269508598954619646309826560.00000000\n",
      "Iteration 44, loss = 16483000561866785125069874714978850059544631456248967876338426759779915605288842486701430554694946887041024.00000000\n",
      "Iteration 45, loss = 16482986584287675040128040972373044267221409764680497134010311132230832495664169304864587377696466819612672.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 46, loss = 16482976903899889406901864058870953342412147144769203165509921630846242567109835230879598217098317350830080.00000000\n",
      "Iteration 47, loss = 16482973941024639053958468946786695141716057126066629298560416383581703230325420360868205272712237706903552.00000000\n",
      "Iteration 48, loss = 16482971145506371371576915014270360087357908488332776757141357935009372858176990854312340775263097906528256.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 49, loss = 16482968349994689426506850475270930917910387370193603613039021458335349128101324062498611145385049029869568.00000000\n",
      "Iteration 50, loss = 16482965554483463777495484861154825880297069952762505997856866151381467893486312652096028502929185234223104.00000000\n",
      "Iteration 51, loss = 16482962758972720906010510520241166464311905557955577580684009671318987424157798230061489771272686903754752.00000000\n",
      "Iteration 52, loss = 16482959963462440441692164107669089985498010091991207846836515358785401358711287252581997317048492204490752.00000000\n",
      "Iteration 53, loss = 16482957167952652940080090640729890470540709695541812568340288202824469239253520035377047590307193887391744.00000000\n",
      "Iteration 54, loss = 16482954372443331919706597771104446429646055046691298076106210546264932795958654971489743667671611568291840.00000000\n",
      "Iteration 55, loss = 16482951576934479417607661833278844131259734554817825421602676055043042664967141415301385312478451430588416.00000000\n",
      "Iteration 56, loss = 16482948781426115804143046172113946259838632313703005119513621388521305207683472910624970158094775308255232.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 57, loss = 16482946845352668193876467981643966037208576869222851558150925148036420571715133624662114881446199800365056.00000000\n",
      "Iteration 58, loss = 16482946252778660270893281682308849333883549103349530715991223591569338153812140352132795215627867297677312.00000000\n",
      "Iteration 59, loss = 16482945693675915659869611343497275303478087640338221372904665642609905925250956375757576716976338369511424.00000000\n",
      "Iteration 60, loss = 16482945134574397344503694365309634877377048622979865013791094587273356653240283736924815747021931846565888.00000000\n",
      "Iteration 61, loss = 16482944575472917732821327742357633551744089383806568632577003184725569467898148831336750280464942808170496.00000000\n",
      "Iteration 62, loss = 16482944016371452380390795460808236105230949010280399611641667443731536735539159406417783157264897053556736.00000000\n",
      "Iteration 63, loss = 16482943457269987027960263179258838658717808636754230590706331702737504003180169981498816034064851298942976.00000000\n",
      "Iteration 64, loss = 16482942898168548156997423246028562701998617585144155238860113618914729540647022163536745834241985928495104.00000000\n",
      "Iteration 65, loss = 16482942339067139841574228330089580771964752674206495659039800524135714620220614661294172084469713309007872.00000000\n",
      "Iteration 66, loss = 16482941779965749859474820424525375257942083447672285542435030422782955425058251348483296204727796340097024.00000000\n",
      "Iteration 67, loss = 16482941220864370062555294191391601086147856268028880683172228651111449410598134807578919141669410288173056.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 68, loss = 16482940833649860207075091254970413918531562886286654710412010738591778590992100006816987212637180562243584.00000000\n",
      "Iteration 69, loss = 16482940715135096918754809083441812424645499429421418309585871346899874066851949214679558830203590309576704.00000000\n",
      "Iteration 70, loss = 16482940603314581811347282168148529674762834732496629895343894611649748181071171702134091201862607168339968.00000000\n",
      "Iteration 71, loss = 16482940491494315222328868060157771675254155979707489760245945120622199904425215424107194700599778401583104.00000000\n",
      "Iteration 72, loss = 16482940379674050670346430286653099944191165636296510676616389295530902263919708500461597962673655818223616.00000000\n",
      "Iteration 73, loss = 16482940267853773896148134506231910602454044836616565284176471474822100806571505450528202644727296134479872.00000000\n",
      "Iteration 74, loss = 16482940156033509344165696732727238871391054493205586200546915649730803166065998526882605906801173551120384.00000000\n",
      "Iteration 75, loss = 16482940044213244792183258959222567140328064149794607116917359824639505525560491603237009168875050967760896.00000000\n",
      "Iteration 76, loss = 16482939932392986351308750189176154214602139034518111187692984997356959793476332742735311720959046934593536.00000000\n",
      "Iteration 77, loss = 16482939820572715688218383412213223678202083462972648949658248174456910244549477755945815693022805801041920.00000000\n",
      "Iteration 78, loss = 16482939708752455210307898307680724484030469938317991968965479681238113876324869541062818481770095584477184.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 79, loss = 16482939631309558128098200923163094094776863444477133297937580896981181239140741031425551527971744479444992.00000000\n",
      "Iteration 80, loss = 16482939607606609544506097157829546332891027571860408120709140350515301606593609581760665378158438795706368.00000000\n",
      "Iteration 81, loss = 16482939585242508967467763376154193305049320723729243699622817402588777192805993304509131568494289587535872.00000000\n",
      "Iteration 82, loss = 16482939562878453205220908953172738183012758881917622410841155105259766774108262823646192552237676414107648.00000000\n",
      "Iteration 83, loss = 16482939540514407628153936202621714403204639086996806379401461137612009536112779114689752352664594157666304.00000000\n",
      "Iteration 84, loss = 16482939518150347791835129110668086744276700426428862987683011508410497845134149925064213809734568617443328.00000000\n",
      "Iteration 85, loss = 16482939495786296103660227356658804159131515403373563801838136542953988698717318152963874320151367810809856.00000000\n",
      "Iteration 86, loss = 16482939473422232193269467595733003963312199924049298307182899581879975735457790254575736250547929903792128.00000000\n",
      "Iteration 87, loss = 16482939451058186616202494845181980183504080129128482275743205614232218497462306545619296050974847647350784.00000000\n",
      "Iteration 88, loss = 16482939428694145113207474763603128940587337152963988347240298978456962531747721545425455378075177757704192.00000000\n",
      "Iteration 89, loss = 16482939406330089350960620340621673818550775311152367058458636681127952113049991064562516361818564584275968.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 90, loss = 16482939390841512786369047731998668516524017785513620796308808056587316476209794458768882639730283020025856.00000000\n",
      "Iteration 91, loss = 16482939386100916551135502708576482905120647700980160396164260216298138514050930234815746167090162096406528.00000000\n",
      "Iteration 92, loss = 16482939381628092769063078550166457016350067194473237619303887028027582486240598141479099831151261124657152.00000000\n",
      "Iteration 93, loss = 16482939377155283246242488733159035006699305553613442202722269501310780911413411528811551838569303436689408.00000000\n",
      "Iteration 94, loss = 16482939372682469649349946247179440460157167093997324683203864642721478064305326207381404319313933381926912.00000000\n",
      "Iteration 95, loss = 16482939368209664200601309099144190987397782271893851369559034447877177761759038303476455853405388060753920.00000000\n",
      "Iteration 96, loss = 16482939363736856714816695616622855246192709040412217004445810587096626823072301045190207624160136556183552.00000000\n",
      "Iteration 97, loss = 16482939359264051266068058468587605773433324218308743690800980392252326520526013141285259158251591235010560.00000000\n",
      "Iteration 98, loss = 16482939354791237669175515982608011226891185758692626171282575533663023673417927819855111638996221180248064.00000000\n",
      "Iteration 99, loss = 16482939350318430183390902500086675485686112527210991806169351672882472734731190561568863409750969675677696.00000000\n",
      "Iteration 100, loss = 16482939345845620660570312683079253476035350886351196389587734146165671159904003948901315417169011987709952.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 10303925761371047291663710546536319389505825462826758515845475812637772811332932391259652877943932584984576.00000000\n",
      "Iteration 2, loss = 13954476661035894895811356505261002792880868385175092986243197328294494287899771861280901308040201602334720.00000000\n",
      "Iteration 3, loss = 13967269232135435228539835380801982738757320781415240110928474181108394574213042832283199548873314985312256.00000000\n",
      "Iteration 4, loss = 13966519137006874322893505497442454564235285481996148766493340757990483582745143547426343279467961708969984.00000000\n",
      "Iteration 5, loss = 13965731966958227543141326514813251758159922478062917340136305365336166967775442228752367146137307494481920.00000000\n",
      "Iteration 6, loss = 13964944739619950409418415824678988384573881932045773038292573870747276813056380919814813038054192726409216.00000000\n",
      "Iteration 7, loss = 13964157556378069388388244827864496220399890761237395193110455150450380800281246615827029799846240027934720.00000000\n",
      "Iteration 8, loss = 13963370417507779892309697256680297204359302317683192978700642528161418959596018812861845167172001650966528.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, loss = 13962583323007334144315078122064373010051461354920984235181370630577345482495151451764062197137572240556032.00000000\n",
      "Iteration 10, loss = 13961796272874248997549235685477562402182196840972447222580760681408634913770881541729269382298114235301888.00000000\n",
      "Iteration 11, loss = 13961009267106008712581396856603323850326323193808683377432635249385751619968256421852259001821491139444736.00000000\n",
      "Iteration 12, loss = 13960222305700119957376528224472064776961227334560565702423147228537916965177266329422120731579334474596352.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 13, loss = 13959668149101949542858630785394027195927469224280356100827527154015617630023029556391326039390322367135744.00000000\n",
      "Iteration 14, loss = 13959494108073662014956663917754580212139104753594799185819298373171645704031083297730988856210923378966528.00000000\n",
      "Iteration 15, loss = 13959336684237598805933220770314161584250895470823599759222893942945639034761208698830354828731089617420288.00000000\n",
      "Iteration 16, loss = 13959179307684161531986103728030303489300264413869821077240676473444145591292916614904314780103178931142656.00000000\n",
      "Iteration 17, loss = 13959021933029626787782254936018941512182283175488449665681514451486905277167835880423359812275663261401088.00000000\n",
      "Iteration 18, loss = 13958864560149647786218288360115990922739378084594420739390858130526336170936026997706036562692989482172416.00000000\n",
      "Iteration 19, loss = 13958707189043886379322132475631131158987273184412999754615358965144832673282897139456584317461931149492224.00000000\n",
      "Iteration 20, loss = 13958549819712318122662071268731326999577707562406254093734292964107387150523054053099405916542014062592000.00000000\n",
      "Iteration 21, loss = 13958392452154910423662483387639198149379666668523606933253361472433989424409308068533705146545939287113728.00000000\n",
      "Iteration 22, loss = 13958235086371659208251416163382572071501773684008736170235777158252138222660760476996882480800294456262656.00000000\n",
      "Iteration 23, loss = 13958077722362531883853248244184068470813014058811064981187241366581823367030221608388141705917780635680768.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 24, loss = 13957966906974088875808713305965784702215087922988412160879897802126722023780908504160873437099930620526592.00000000\n",
      "Iteration 25, loss = 13957932102903362217133456478185360019352227614769443445232966791841463996584283218543468812932121991577600.00000000\n",
      "Iteration 26, loss = 13957900621539136321882154256001718386112379720185022496699465036786472179522864814133382248644479192924160.00000000\n",
      "Iteration 27, loss = 13957869149346341021356638453954036782071299904607230807172248366356851589348100865281056833411299919527936.00000000\n",
      "Iteration 28, loss = 13957837677249431041273163246473097185029337928448292786403280981180924766264896998589891440274879344017408.00000000\n",
      "Iteration 29, loss = 13957806205223546394576542565362079483805145740630736314116263795255928252229332344677574308072955284815872.00000000\n",
      "Iteration 30, loss = 13957774733268623933151510041552309356582382650431568794790993164558092326887476917723812773367636056604672.00000000\n",
      "Iteration 31, loss = 13957743261384677916249900016446390682480867523497917588706224750641171443222476198397705179515864943165440.00000000\n",
      "Iteration 32, loss = 13957711789571681862404020141725201971706651037913689026772840896333907331408488579742354603140461560332288.00000000\n",
      "Iteration 33, loss = 13957680317829650030865704758791347103379552059326010469269597263190054444428659542426859387598369191886848.00000000\n",
      "Iteration 34, loss = 13957648846158574273491048529700481003716816950222237710322919187464610237721191668926020479542763104239616.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 35, loss = 13957626683718083154902622063655328925694953349979204978904196062577328645700165515810597080160938137485312.00000000\n",
      "Iteration 36, loss = 13957619723069321514828605488265641065051237246226122272307117359211424265519203709404821841163676551217152.00000000\n",
      "Iteration 37, loss = 13957613426932584968609086400655111473965717087896214046259993870659180898511431723829091397003204053958656.00000000\n",
      "Iteration 38, loss = 13957607132618734880603961546035798412958473983302755444589494086586201807698313640735177597778480733356032.00000000\n",
      "Iteration 39, loss = 13957600838312709047783937452473842451840411300225895533019065163651916132351165736213654774842207842074624.00000000\n",
      "Iteration 40, loss = 13957594544009530991258828970460489777794744927818185574262981219596019781352215256749201096621179340324864.00000000\n",
      "Iteration 41, loss = 13957588249709192562884730762051395317038721228566981362447667590673510210139664784816617509768570494517248.00000000\n",
      "Iteration 42, loss = 13957581955411697836733595496218731606463717021228605000509911608756888690994413029178503540957793671446528.00000000\n",
      "Iteration 43, loss = 13957575661117028479481636162587722230058536621399607025234170280419899498652415800403161320158493220536320.00000000\n",
      "Iteration 44, loss = 13957569366825204861488616106019229872280064122861597951304380265025048994518166642303588480737730975760384.00000000\n",
      "Iteration 45, loss = 13957563072536220871646606323054995727791234297480094624315360564763585270170317491735885732685388386926592.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 46, loss = 13957558640073605560585670183887966314059725216350609737342698069094336941559445700296580442336036458070016.00000000\n",
      "Iteration 47, loss = 13957557247950471562457977614104314921972624065245249416834302955299472885843205723858356475494973998891008.00000000\n",
      "Iteration 48, loss = 13957555988728571694821987479274108081215464392657551608403219704310475379231314813215037501682555140702208.00000000\n",
      "Iteration 49, loss = 13957554729870787933883834063392213370370104247601481672403135226250186705660620851141815437427006196678656.00000000\n",
      "Iteration 50, loss = 13957553471014110283480830273455162425533550394886862683740811351573993456353926318114364865002914837430272.00000000\n",
      "Iteration 51, loss = 13957552212157563003380311890627632661221054742374550989055682096817840920035990465490099146128019215613952.00000000\n",
      "Iteration 52, loss = 13957550953301117575078610232104416319192979558770291867790236138874220190740522331930821594088432763666432.00000000\n",
      "Iteration 53, loss = 13957549694444796405971464977232462352351897347233856886096803803041888266012464815630829605587923498958848.00000000\n",
      "Iteration 54, loss = 13957548435588573014591183777692649270903858785849152374886267432149586876025976309633226257249311037325312.00000000\n",
      "Iteration 55, loss = 13957547176732471845369482647318012296197124787154110951779351017432323654466449066513608709113069579534336.00000000\n",
      "Iteration 56, loss = 13957545917876486787198432582650292622894630123014249462370873561081346692912535023128077671169080575393792.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 57, loss = 13957545031384982650381607864757238216681670677744510429466852763260065224643127726366358234119643124989952.00000000\n",
      "Iteration 58, loss = 13957544752960625961726531303655547134162533531067593790074173843647926765723464122039062059198670551646208.00000000\n",
      "Iteration 59, loss = 13957544501116453765868919394270305147471319353122481478164111118947692150726920086802974124780217486540800.00000000\n",
      "Iteration 60, loss = 13957544249345099495057336359010901288803675216300476006922424617398947648373447119889526187597702327435264.00000000\n",
      "Iteration 61, loss = 13957543997573946890807410437874038006259183607916414631051711043539016123924460236724754820749099324669952.00000000\n",
      "Iteration 62, loss = 13957543745802802434701389854681519797497445637044997461054572133424087144037270771085182507247321055494144.00000000\n",
      "Iteration 63, loss = 13957543494031664089703298274947260394072772894308063445462614221117910072571429368589509483755661336510464.00000000\n",
      "Iteration 64, loss = 13957543242260521670633254026240828453756723332814807326933868976939231728824689257331236933590589250732032.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 65, loss = 13957542990489385362671138780992655318777738999456034362810304730569305293499297209216863673435635715145728.00000000\n",
      "Iteration 66, loss = 13957542738718259239888905208174913526027196712988066656028708813880632038876151933008989229964213096546304.00000000\n",
      "Iteration 67, loss = 13957542486947129043034718966384999196385277607763776846310325565319457511972107948038515259819378111152128.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 68, loss = 13957542309648873030462833381500286220947830725029372172034182056352715213408112285074766165817026655813632.00000000\n",
      "Iteration 69, loss = 13957542253964007396432551805840989556091930841952839788267148537051789302817437756476946268175609454657536.00000000\n",
      "Iteration 70, loss = 13957542203595184364662496897086024262049543098881519214108140521354745942204645333965007355977473468661760.00000000\n",
      "Iteration 71, loss = 13957542153240920029015304560389619549342217181527233484558662952040999077383388674602477011218430223712256.00000000\n",
      "Iteration 72, loss = 13957542102886696434087638913414940205548659451736168784377058701452264935371119102865941933193510646710272.00000000\n",
      "Iteration 73, loss = 13957542052532478950267902269898519667092166950079587238600635448672282701780197594273306145178709619900416.00000000\n",
      "Iteration 74, loss = 13957542002178247207196331284979495249515855582775878332545456534338546015206130605011572013806965309308928.00000000\n",
      "Iteration 75, loss = 13957541951824027686340618306976988442613674671741135735300639615622313145474759742037636462455458099101696.00000000\n",
      "Iteration 76, loss = 13957541901469802054376976325516222830374428532571909983650641699097328367322040815919801621093832338702336.00000000\n",
      "Iteration 77, loss = 13957541851115582533521263347513716023472247621537167386405824780381095497590669952945866069742325128495104.00000000\n",
      "Iteration 78, loss = 13957541800761354864521645031566864142787313072989780583287433197919860083297501672446731465043993184698368.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 79, loss = 13957541765301704069414463181487138801388961378318531858725883229313761750812792410730241598910864130310144.00000000\n",
      "Iteration 80, loss = 13957541754164733387051578467738582990552607492957018643734548924577077332063196730268237335386628110155776.00000000\n",
      "Iteration 81, loss = 13957541744090975299212691756343065990770332854352869893601607052433670695590076179786008795624460699828224.00000000\n",
      "Iteration 82, loss = 13957541734020122024676058022106567794539729989006380537398032805383671195397734977037242774005310814158848.00000000\n",
      "Iteration 83, loss = 13957541723949278935319305960300500940537569170550696438536426888014924875907640546194975569069691845476352.00000000\n",
      "Iteration 84, loss = 13957541713878429734854624895036175281198343123960529185269639972837426647996198052208809074123954326601728.00000000\n",
      "Iteration 85, loss = 13957541703807586645497872833230108427196182305504845086408034055468680328506103621366541869188335357919232.00000000\n",
      "Iteration 86, loss = 13957541693736741519105144436937955304748333077670999936078034472163683372875559836142974900916010205839360.00000000\n",
      "Iteration 87, loss = 13957541683665898429748392375131888450746172259215315837216428554794937053385465405300707695980391237156864.00000000\n",
      "Iteration 88, loss = 13957541673595051266319687644353649059852634622003309635418035305553689461614472265695840964371359901679616.00000000\n",
      "Iteration 89, loss = 13957541663524212251034888251519754742741850622303947639493216720057444414405276543616173286109153299791872.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 90, loss = 13957541656432279647570280280120506152327354192115904632818834327212723984539795466015315596878480068837376.00000000\n",
      "Iteration 91, loss = 13957541654204884696283312803576360482781808051292337569233209999890886846333696588170394838838950391447552.00000000\n",
      "Iteration 92, loss = 13957541652190135930565902329577777858649316896700933291262372757772956409635701574207768799557905566138368.00000000\n",
      "Iteration 93, loss = 13957541650175959571957841846169436667755268777372784475910155643741454728403975141390376257891298275491840.00000000\n",
      "Iteration 94, loss = 13957541648161791361493686700705440550643974295557279866431513193454955591734046126098182769571515718434816.00000000\n",
      "Iteration 95, loss = 13957541646147629262137460558699703238869745041876258411358051740977208363485465173949888571261851711569920.00000000\n",
      "Iteration 96, loss = 13957541644133463088709281747721793390204138969438914853347802956626959862955985513038994846278775337910272.00000000\n",
      "Iteration 97, loss = 13957541642119296915281102936743883541538532897001571295337554172276711362426505852128101121295698964250624.00000000\n",
      "Iteration 98, loss = 13957541640105122593709018787821628619090173187051583531453730724181460317335228773692008342965797857001472.00000000\n",
      "Iteration 99, loss = 13957541638090954383244863642357632501978878705236078921975088273894961180665299758399814854646015299944448.00000000\n",
      "Iteration 100, loss = 13957541636076790246852661165865808921758961042176896415433233155480963316276269451870220892999645109682176.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 23757952236728686275266258901197046389837224196525153512184031655038917520169116927969753129575528037089280.00000000\n",
      "Iteration 2, loss = 26325495677827886072505784072560496367402131272054458287770701014341274820418798909054265762707604181614592.00000000\n",
      "Iteration 3, loss = 26321479194201444426385213841666389298231800919862902077151719869300031316618716457372120570176967108722688.00000000\n",
      "Iteration 4, loss = 26317462893822501806124472855561355306880354335504844991777602024722153103199625075483240092188834680274944.00000000\n",
      "Iteration 5, loss = 26313447206276409337667347590683128843735745720108030048011306562931686292327434359501502473872998425165824.00000000\n",
      "Iteration 6, loss = 26309432131469695588203753818477196009939622347374449567141078333131940943343069169105967247129526971400192.00000000\n",
      "Iteration 7, loss = 26305417669308901347139465317305560517307761947275062179265524180143730932430150490263492523878726047367168.00000000\n",
      "Iteration 8, loss = 26301403819700534811304634513752845782524927699730249690988952293807859957525109638840140202653602447097856.00000000\n",
      "Iteration 9, loss = 26297390582551128621961129848236710443624143697198326526536394855200139350249770183277569342027637165391872.00000000\n",
      "Iteration 10, loss = 26293377957767227642586677768091330749312564488406573418943246041013884259068651818305237580594548297433088.00000000\n",
      "Iteration 11, loss = 26289365945255393032946815396539573095862851897107559512992049355432414921569869073703000663641703405584384.00000000\n",
      "Iteration 12, loss = 26285354544922194100950985194748648953330421384566498159338922966384054119903333896775913389803294785798144.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 13, loss = 26282761633652885436145332416582810355438717212355314458828908139802103856648551854031380724573477502189568.00000000\n",
      "Iteration 14, loss = 26281946428352552599289094948118812998607593971443883769101956783763699430090397612988694958065443399532544.00000000\n",
      "Iteration 15, loss = 26281144328541099693169309478181322880148281949579971920867043087300431502497165819583989709945340010954752.00000000\n",
      "Iteration 16, loss = 26280342253210578113687318365158939032446427098223401007301070644209747331728041427468484403127399248560128.00000000\n",
      "Iteration 17, loss = 26279540202358600454678857591358554837155213626169418707446662977205901361176381101758856406991974170624000.00000000\n",
      "Iteration 18, loss = 26278738175984486346127831438427356633414712801112233830860335663581181119932100479100984767079199522422784.00000000\n",
      "Iteration 19, loss = 26277936174087441344003469456791699727406444965569036304868558988197838513219951350787961782073663778979840.00000000\n",
      "Iteration 20, loss = 26277134196666711744994527886599664794225698649620237086165676554643138169073672595738875017394079083266048.00000000\n",
      "Iteration 21, loss = 26276332243721584586509289657721057877881530570909468160813905285229357438335990180498807305193281246199808.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22, loss = 26275530315251281720804794996470924432120968346979207867886864792308753315355250731411249050849508209983488.00000000\n",
      "Iteration 23, loss = 26274728411255049444569800142997345132039300507909867164078898679468590428165193127395287819781472117587968.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 24, loss = 26274210023719385431241457668312383011937443432663337041441062388481219101690641628318137670043616554254336.00000000\n",
      "Iteration 25, loss = 26274047033853741984261555354811501276208347767460094149971837032484905993184797300632861026605404522545152.00000000\n",
      "Iteration 26, loss = 26273886660261646961372595557527549831634321877379662171167158888970075221708649372507740320397615857401856.00000000\n",
      "Iteration 27, loss = 26273726287648755132307623221912840229478684069316671049197696301136037164237152535178855568852186310901760.00000000\n",
      "Iteration 28, loss = 26273565916014797608317762195803985034910564305353861990235485365397707850230992010314638011523899674591232.00000000\n",
      "Iteration 29, loss = 26273405545359721426467627782562741268342063941659047656102290767412570740038484584001293801658395180138496.00000000\n",
      "Iteration 30, loss = 26273245175683510290469409306300418782207675703206939635050963179690620744536035421188424832562023360364544.00000000\n",
      "Iteration 31, loss = 26273084806986156052179201429072672502724645952484893721207927938486855319161847104350832050887959481679872.00000000\n",
      "Iteration 32, loss = 26272924439267732044892152192378608093937757427106707767435357017506297364972096391215306936757626146390016.00000000\n",
      "Iteration 33, loss = 26272764072528144564953350209858257207345343295676973406187141783681417619506112980242060376682538918215680.00000000\n",
      "Iteration 34, loss = 26272603706767426204938416833289000138078418108246267460957580891992226261011086541531888584049996731514880.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 35, loss = 26272500037047234977173404813375276722496460584392775159053059618623514895391260999210889946103669940289536.00000000\n",
      "Iteration 36, loss = 26272467441121492036500470693850658782613689980964265275039688686962624825439553173945008884736201515335680.00000000\n",
      "Iteration 37, loss = 26272435368257564510717097785910757212321971040728974246076782496854792793638800587543889510094169605931008.00000000\n",
      "Iteration 38, loss = 26272403295432849927478163735131523221532132630093923983691945579571706465488119841163214367046168096014336.00000000\n",
      "Iteration 39, loss = 26272371222647283101632425837958196219982145648957960840896580625153345484493131594601391028817599116869632.00000000\n",
      "Iteration 40, loss = 26272339149900872181323789432335121281454763734833729023564262297344712395215633265383618548755287402086400.00000000\n",
      "Iteration 41, loss = 26272307077193629388768112525178816016624117343990194840505352591763311014498320979797695506879470052048896.00000000\n",
      "Iteration 42, loss = 26272275004525526205461726433684072667250568745133103571162340185301632436374903776505425216476260499193856.00000000\n",
      "Iteration 43, loss = 26272242931896587075836347171683926454682378850800387833155949069194684294530773908082404837586132944289792.00000000\n",
      "Iteration 44, loss = 26272210859306783481388306056373169620679909929697792905928667920334957682999640413190437683495200819773440.00000000\n",
      "Iteration 45, loss = 26272178786756148014693224439529182460374176531875895612974795393702462780028692961930319967590763060002816.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 46, loss = 26272158053123578273622498694697924674663506494188817614096915056372488162804270946467573759719716474060800.00000000\n",
      "Iteration 47, loss = 26272151534020325050251682481816468662387212285587500031418471238311885030527413443685042925712255150456832.00000000\n",
      "Iteration 48, loss = 26272145119521716987951642410368302037004882957648234416300198869295483296946233108806921098937493473984512.00000000\n",
      "Iteration 49, loss = 26272138705024681517425332562178734651694005669649300534781836603064572663791954356292216568099905380286464.00000000\n",
      "Iteration 50, loss = 26272132290529226786816658275192111580237334059103342592736959103364155675626374603666128386546315602952192.00000000\n",
      "Iteration 51, loss = 26272125876035316129478045528658879990612476757203461663734480383341720881921405472065260814216012840828928.00000000\n",
      "Iteration 52, loss = 26272119461542986212057068343328592714841825132756556674205486429849779733205135340353009591169708395069440.00000000\n",
      "Iteration 53, loss = 26272113047052200367906152698451696920902987816955728697718891256035820778949475829665978977346690964520960.00000000\n",
      "Iteration 54, loss = 26272106632563015634032635959638608125275240272389487175389717508114861831087008862680562346174733684310016.00000000\n",
      "Iteration 55, loss = 26272100218075370899357228092306738274587930217713000563166155207999383805404253807957766797552651052515328.00000000\n",
      "Iteration 56, loss = 26272093803589323200887266462066502885320333115514778302163227001904404513833792588173985704908216204263424.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 57, loss = 26272089656875257986931696624470655053469206499795180436035022204053297171903002680080550166612834865119232.00000000\n",
      "Iteration 58, loss = 26272088353057882081293088701726649004302634574406623260088990799557699204834013282901543539900203030020096.00000000\n",
      "Iteration 59, loss = 26272087070161142689502434375067312683714000038446549486793663256425350167735632030148772699483103188484096.00000000\n",
      "Iteration 60, loss = 26272085787264456260647164745046219342713264146318663051676571027635517670288933991309795705820364115279872.00000000\n",
      "Iteration 61, loss = 26272084504367847239158995825496404202648687810560896572358438104423209346179311418960209718952460011175936.00000000\n",
      "Iteration 62, loss = 26272083221471291180606211602584832042172010118635317431218540495553417561721372060524417578838916675403776.00000000\n",
      "Iteration 63, loss = 26272081938574800307204670083228020471957361526810891937067240196643646133757812042290217865499971208347648.00000000\n",
      "Iteration 64, loss = 26272080655678374618954371267425969492004742035087620089904537207693895062288631364257610578935623610007552.00000000\n",
      "Iteration 65, loss = 26272079372782018189927267824150851639205528462221823992667218860576665619594728735189195245819286247178240.00000000\n",
      "Iteration 66, loss = 26272078089885702501619691070597459155320083076919248924797773832184448899709813193746775179437072552296448.00000000\n",
      "Iteration 67, loss = 26272076806989447924391404351626654724805289972961505400980138781879751264038378283743348013156044359335936.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 68, loss = 26272075977647157177624622546340004387909572814378079424250633768364192902063434765489920225028433514594304.00000000\n",
      "Iteration 69, loss = 26272075716883806663098652632339682806952389083243824338927119842763612240445137374189664415892325571493888.00000000\n",
      "Iteration 70, loss = 26272075460304576932827149167200819112684589919985150569434886958439679329171523677754496521337864240234496.00000000\n",
      "Iteration 71, loss = 26272075203725347202555645702061955418416790756726476799942654074115746417897909981319328626783402908975104.00000000\n",
      "Iteration 72, loss = 26272074947146121546356094905895264261040368412224125133387208521664314778905194993646760258902353944510464.00000000\n",
      "Iteration 73, loss = 26272074690566883667940686102812055492989815611452807158021400973595379323069783879686393311001067879661568.00000000\n",
      "Iteration 74, loss = 26272074433987658011741135306645364335613393266950455491465955421143947684077068892013824943120018915196928.00000000\n",
      "Iteration 75, loss = 26272074177408448651829395186367363325802478197473392236657659196182521134207948739391654681932619417911296.00000000\n",
      "Iteration 76, loss = 26272073920829231143773749728145017242208809490483684775975788307476092039777031169244285367398395187036160.00000000\n",
      "Iteration 77, loss = 26272073664249997339430293594033981011049633508468688903546768091279657856222518764046517946170521488982016.00000000\n",
      "Iteration 78, loss = 26272073407670792053590506142728152538130095257747947751675259198190732578634297320186947211656534358491136.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 79, loss = 26272073241802338793123492984437429515020604008538849079853502993734622432976387067051381086039107029696512.00000000\n",
      "Iteration 80, loss = 26272073189649675208733423271992841257855370172322113427487659939610508336302165522811489166889345227948032.00000000\n",
      "Iteration 81, loss = 26272073138333813781205702436870812878814578428396354682429421501630216919380027690226577386619485967876096.00000000\n",
      "Iteration 82, loss = 26272073087017948279606028932776611962882409865714273834434395731777424230176991148879066079676214341009408.00000000\n",
      "Iteration 83, loss = 26272073035702099074294166104571101194515748578057481398186519289414636630097549442581952879426592181321728.00000000\n",
      "Iteration 84, loss = 26272072984386258017126208614309935499931840927913333167812217510796851574579905153810038732523794755223552.00000000\n",
      "Iteration 85, loss = 26272072933070388441454582441243562047108295546474930216880404409071557613095969903699927898907110761562112.00000000\n",
      "Iteration 86, loss = 26272072881754522939854908937149361131176126983792849368885378639218764923892933362352416591963839134695424.00000000\n",
      "Iteration 87, loss = 26272072830438665586399140770999505289026712058623412726763927533110974779251694238530104338367392241418240.00000000\n",
      "Iteration 88, loss = 26272072779122808232943372604849649446877297133453976084642476427003184634610455114707792084770945348141056.00000000\n",
      "Iteration 89, loss = 26272072727806954953559557107671966141619259027040861545457812652767895762250114699648079357847910821658624.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 90, loss = 26272072694633249634807124867714000404188404229676282240521027017135669152907297297475607836700140835438592.00000000\n",
      "Iteration 91, loss = 26272072684202720992001063594197255289646734281189257212984645738183347605853351697390228979543600841883648.00000000\n",
      "Iteration 92, loss = 26272072673939563373154549035472670746647532479926865034545432445328293902680159482418604919513913510330368.00000000\n",
      "Iteration 93, loss = 26272072663676405754308034476748086203648330678664472856106219152473240199506967267446980859484226178777088.00000000\n",
      "Iteration 94, loss = 26272072653413252209533472586995674197540505696158402780603793191490687768614673761237956326127951214018560.00000000\n",
      "Iteration 95, loss = 26272072643150086442543052690326744580758550257383366396291005234890631520879684128741133212751439148875776.00000000\n",
      "Iteration 96, loss = 26272072632886928823696538131602160037759348456120974217851791942035577817706491913769509152721751817322496.00000000\n",
      "Iteration 97, loss = 26272072622623767130778070903905402957868769836102259936475791317308022842252400990035285566018652118974464.00000000\n",
      "Iteration 98, loss = 26272072612360605437859603676208645877978191216083545655099790692580467866798310066301061979315552420626432.00000000\n",
      "Iteration 99, loss = 26272072602097443744941136448511888798087612596064831373723790067852912891344219142566838392612452722278400.00000000\n",
      "Iteration 100, loss = 26272072591834277977950716551842959181305657157289794989411002111252856643609229510070015279235940657135616.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 14073834740711524769025441705635202768788114851764360459630828772794774268937110139133069319960464386424832.00000000\n",
      "Iteration 2, loss = 18110196583836988240191794564931210162634238941506141228168399303589595145263443380191781450340339141312512.00000000\n",
      "Iteration 3, loss = 18114213847073601714216104001425412732809887584740694008355677144693188253837679764194643093031958490382336.00000000\n",
      "Iteration 4, loss = 18113004487994387291133228268648066843090555899703826995155292475404679560948064139055950615085258232561664.00000000\n",
      "Iteration 5, loss = 18111790217555802663482877735434683146725616164605745593207084722322301665507385973415384583630961558683648.00000000\n",
      "Iteration 6, loss = 18110576023752733871786074691960345122567537479837612312988888254620971504008777934869587239142520256462848.00000000\n",
      "Iteration 7, loss = 18109361911343283438456982735072629404265880125848963874020415968727834029632855800261861169912995366567936.00000000\n",
      "Iteration 8, loss = 18108147880326546919522109352949232801934990338736293424334880188947606796020106224295111454444841460563968.00000000\n",
      "Iteration 9, loss = 18106933930697065058564878122878955881129930985027983028637256206128584946766259465085972350365487033483264.00000000\n",
      "Iteration 10, loss = 18105720062449388784348594294581030549634206978143220008974487640800316806169292552657576931985891497345024.00000000\n",
      "Iteration 11, loss = 18104506275578064951564610448802516178339946412744869584456730781619849426246283808270458746943601897373696.00000000\n",
      "Iteration 12, loss = 18103292570077622081580492155915695722128081699092347510978598923817974133750267363753453472845809628217344.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 13, loss = 18102450056791876530711560475479615645599098663647627673755220817060247174458652626382400317531824949559296.00000000\n",
      "Iteration 14, loss = 18102187750029295487795580256904163928553209919053357128830037147815360410136624085988849119205445978816512.00000000\n",
      "Iteration 15, loss = 18101945015649314044209492354735644738261179746390952461095126293836839411317078334649124519961469156065280.00000000\n",
      "Iteration 16, loss = 18101702303212251545122867222073442208383956574524388506435382076730813154695738274259266706337749933228032.00000000\n",
      "Iteration 17, loss = 18101459594047351231081498796797263383397811870990435033915009566790391566788068299024748360641138334367744.00000000\n",
      "Iteration 18, loss = 18101216888136742185465004632472275189278330161182187511316377505288743787431882421802745729948287414304768.00000000\n",
      "Iteration 19, loss = 18100974185480340889798355015168940619752286660595042828435345588839593734868757112959968517454243653746688.00000000\n",
      "Iteration 20, loss = 18100731486078118825577881262082051916580043637934746264714402494335432503132401411158220036445120485130240.00000000\n",
      "Iteration 21, loss = 18100488789930039326156009352462056247739209724394398893722462234923748641694726937534104546860206607302656.00000000\n",
      "Iteration 22, loss = 18100246097036035169345520248268106754522067410494686017002533834708271157920904997504729858588197968150528.00000000\n",
      "Iteration 23, loss = 18100003407396094132930555942583685826254486239966641325744255298071496234968239464782297391608857467289600.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 24, loss = 18099834933308265813126800704854413855890457280830741431052113830896036675405118722527476702588098536538112.00000000\n",
      "Iteration 25, loss = 18099782479320744436192678891019553725292227013335086405150305845166053616273609680025339721613332971520000.00000000\n",
      "Iteration 26, loss = 18099733938668930721303661091348296611558146998050081032252492347989832539232236522368487882872946413273088.00000000\n",
      "Iteration 27, loss = 18099685401884407400089925447110734684366698918361516838577128497347567107745213364118033384117971755991040.00000000\n",
      "Iteration 28, loss = 18099636865233617527758525148922782485065014924324142596414327027495814932898754692578341703460643319840768.00000000\n",
      "Iteration 29, loss = 18099588328712988143206969508189125159915624966643474030201597886250960224344692922949627948318315425824768.00000000\n",
      "Iteration 30, loss = 18099539792322507024219400517993245098244398589050544831128579077995499165240331928944093538670750973558784.00000000\n",
      "Iteration 31, loss = 18099491256062180281903747181793401105388401019679838153600451600538183664007019773705637764528068513234944.00000000\n",
      "Iteration 32, loss = 18099442719932022175511843840992197060467451124178481357895971115432768173627901937903358969247211328634880.00000000\n",
      "Iteration 33, loss = 18099394183932022519863808823159201621253106855655669186673169292997999513400731649630758336144648502771712.00000000\n",
      "Iteration 34, loss = 18099345648062171129779760455863983445516926167220596382590077803552624502623262136981337048536849118658560.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 35, loss = 18099311954387253500461756496296857663185136369167879509402081326366117051493956829923199807354411233050624.00000000\n",
      "Iteration 36, loss = 18099301463884325812427052700259600075444645855289665769217108236799112964504971842934068437575321687949312.00000000\n",
      "Iteration 37, loss = 18099291756002914125003429887694822444547992139802351170551604404801113000149374600561894880698012209774592.00000000\n",
      "Iteration 38, loss = 18099282048874101675324393635590612220799690460940789981673130048090640213931348364437684176613808359342080.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 39, loss = 18099272341751229222552348744913960784678790528796854874630585561487022413263639504183583362364835295657984.00000000\n",
      "Iteration 40, loss = 18099262634633543063376051455812948811280580873450956806118314548577524226179986898718680003369805161693184.00000000\n",
      "Iteration 41, loss = 18099252927521069679263194116606697790399010816819189445225434666533403922506232154999871023005898341613568.00000000\n",
      "Iteration 42, loss = 18099243220413811107249753061781293990479768768279713843420339581290912138382824627408456184609821018816512.00000000\n",
      "Iteration 43, loss = 18099233513311746976975964946475874727065970634050919486019092633487542512405270772131437854814511359328256.00000000\n",
      "Iteration 44, loss = 18099223806214891547693664112093043879277435279779933733300449484677049497556716069837914376976912646930432.00000000\n",
      "Iteration 45, loss = 18099214099123236671258945220688456373331409067954112379390835471114430549275363103002686697750200148033536.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 46, loss = 18099207360433970751219414566906371197139160464287679320205959516137406086382452291977907767877927794900992.00000000\n",
      "Iteration 47, loss = 18099205262345164170442030340044158339939720749790500583008498807999683687519002069007833012091944762867712.00000000\n",
      "Iteration 48, loss = 18099203320778836827773652411034796707839646637766096189315243472056942513023877481945341727199601130209280.00000000\n",
      "Iteration 49, loss = 18099201379362199036790237857588785538706647270528397899062135795555947483309110250314659476824438270328832.00000000\n",
      "Iteration 50, loss = 18099199437945917727102681839207871347569119544468883615777919657898813778172980035411435810372857504989184.00000000\n",
      "Iteration 51, loss = 18099197496529840121012759269435584001000432756225474479333070113866743687081785258638188477591895079387136.00000000\n",
      "Iteration 52, loss = 18099195555113968255556446482758009767446275315176331541195980829395987846175975274376217241818257176920064.00000000\n",
      "Iteration 53, loss = 18099193613698308241841672482633407452243712449455937955771832802295298163876898145769421393062062347780096.00000000\n",
      "Iteration 54, loss = 18099191672282851931724531931117431981609990521551649517187051368819672095622756455292601877976485858377728.00000000\n",
      "Iteration 55, loss = 18099189730867601362241001162696169623990797940841627276910030194905360277553999557327058459898233892110336.00000000\n",
      "Iteration 56, loss = 18099187789452574866714867187744396795397330391729320698156312273978618434934671641304489008857662099554304.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 57, loss = 18099186441716548496570537824110145302252246201330865043174776694612784033111070488249160983241312899694592.00000000\n",
      "Iteration 58, loss = 18099186022099255698689617910537544473320692415408471133465827718322885865641731951354091599526538474684416.00000000\n",
      "Iteration 59, loss = 18099185633786396415129623421261274074970946423008903917524873638822714477148308297572766151887282717589504.00000000\n",
      "Iteration 60, loss = 18099185245503432671558313849787079385544296464500928051729360839736879085889609543746767433747974500384768.00000000\n",
      "Iteration 61, loss = 18099184857220511705742507302520696333477103102934334266770115025312307053580347231928063745679496134524928.00000000\n",
      "Iteration 62, loss = 18099184468937594813998653424226485818301286560124062584747656542760236293551983628871959584284430135459840.00000000\n",
      "Iteration 63, loss = 18099184080654688107434681218362706645353912064204596160067166389889418714225866797722354239572895053381632.00000000\n",
      "Iteration 64, loss = 18099183692371785474942661681471100009297914387041451838323463568891102407180648675335348421534772338098176.00000000\n",
      "Iteration 65, loss = 18099183304088895064666500151496010983916047166147273825390122743510289916978126679236141183516886723198976.00000000\n",
      "Iteration 66, loss = 18099182915806016876606196628437439569208310401522062121267143913746981243618300809424732525519238208684032.00000000\n",
      "Iteration 67, loss = 18099182527523144799653822108837126959837638865031333571549346081792424478679823002757223157531708244361216.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 68, loss = 18099182257976011636698518476917730564185991718938543662534174740062530117687009917244169174527837296656384.00000000\n",
      "Iteration 69, loss = 18099182174052573854889293105961290336545702737411307605570000337354306972825725624554412883819285482307584.00000000\n",
      "Iteration 70, loss = 18099182096390010961135590079844815838036782540195302788842741651573775494145018053075866752972941537837056.00000000\n",
      "Iteration 71, loss = 18099182018733434916116334108335884301406097505394628237724467152433863632244963008237325068706070598254592.00000000\n",
      "Iteration 72, loss = 18099181941076860908133054471313039033221100879972114738074586319230202406485357317780083147775905842069504.00000000\n",
      "Iteration 73, loss = 18099181863420293011257703837748452570373169482684084392829886483835293089147099690466740516855859636076544.00000000\n",
      "Iteration 74, loss = 18099181785763712892166495197267348496851107629127087738774824652822879954966145936865599305915576329699328.00000000\n",
      "Iteration 75, loss = 18099181708107144995291144563702762034003176231839057393530124817427970637627888309552256674995530123706368.00000000\n",
      "Iteration 76, loss = 18099181630450575061379817595652089302709556425172865996817031316096810684149181327857614280738777734316032.00000000\n",
      "Iteration 77, loss = 18099181552794011238576419631059675376753001846641157754509118812574402639091822409306871176492143895117824.00000000\n",
      "Iteration 78, loss = 18099181475137441304665092663009002645459382039974966357796025311243242685613115427612228782235391505727488.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 79, loss = 18099181421228019560960375139391730410598704793263994899517135841144265340151631261024737417642712156340224.00000000\n",
      "Iteration 80, loss = 18099181404443321819418648392770011022842204950067742430782332630921367530477127630580287342817470876483584.00000000\n",
      "Iteration 81, loss = 18099181388910806388817540919266195347316457137495115995381129761454510344144357020150758447976813430833152.00000000\n",
      "Iteration 82, loss = 18099181373379498920550399796011536860083936086171993080737370792184280389098053557831989211802922739826688.00000000\n",
      "Iteration 83, loss = 18099181357848185341175329669298619567514349806714387011688430825105298525630402032369320685618913498628096.00000000\n",
      "Iteration 84, loss = 18099181342316869724764283208099616006499075117878619891171097192090066026022301152525352396098198074032128.00000000\n",
      "Iteration 85, loss = 18099181326785550034281284077928439908592423610286530667716976227202332254133301563918784579904070282641408.00000000\n",
      "Iteration 86, loss = 18099181311254238491942190285701608884468525740207085650136429926059601026806099392837415817056767224840192.00000000\n",
      "Iteration 87, loss = 18099181295722920838495167490016519055007562641993157478150702627108117891057549158612147764199345616846848.00000000\n",
      "Iteration 88, loss = 18099181280191603185048144694331429225546599543779229306164975328156634755308998924386879711341924008853504.00000000\n",
      "Iteration 89, loss = 18099181264660289605673074567618511932977013264321623237116035361077652891841347398924211185157914767654912.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 90, loss = 18099181253878406886560912130483926500761428542481957786634972399806857931661410049111752722908743844495360.00000000\n",
      "Iteration 91, loss = 18099181250521467338252566781159582623210128573842707292888011757762278369726509323022862707943695588524032.00000000\n",
      "Iteration 92, loss = 18099181247414958955838806816794995190146189146944963271989947652434655278494786879545577544300128022560768.00000000\n",
      "Iteration 93, loss = 18099181244308699091814159659732932507456235664182867530235910791329609796397885670586863507734714831077376.00000000\n",
      "Iteration 94, loss = 18099181241202427005573654495754352214092151725151805479671511934607060497458288335340350891149064539209728.00000000\n",
      "Iteration 95, loss = 18099181238096158993405102000747944457619444604877065532043900409757012470799589708856437801236826614136832.00000000\n",
      "Iteration 96, loss = 18099181234989901166416431178171968043375179531493130841758257214588217624843137854279023528008119606050816.00000000\n",
      "Iteration 97, loss = 18099181231883633154247878683165560286902472411218390894130645689738169598184439227795110438095881680977920.00000000\n",
      "Iteration 98, loss = 18099181228777371253187255191617411335766830519078134100908215162696873479947088664455096638193762306097152.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 99, loss = 18099181225671105278054679031097089847739811808181555204748997303783076089428839392352483311618230564421632.00000000\n",
      "Iteration 100, loss = 18099181222564833191814173867118509554375727869150493154184598447060526790489242057105970695032580272553984.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 3568853670533496859825802525395755725002673771748020408114946545170267717334455820789424105870746600865792.00000000\n",
      "Iteration 2, loss = 4927793610517942133736319912585131311948074460790804719244959579920919053345730094448059537468985758973952.00000000\n",
      "Iteration 3, loss = 4934455194768194673893042153540201931767305636955475802380194984566941878384835000502747531478216719990784.00000000\n",
      "Iteration 4, loss = 4934205348142422472583913843178888773798805477892531561274353779285065756462203393421774329391131505197056.00000000\n",
      "Iteration 5, loss = 4933929524116043321160818023598786703028374663669591176580703115817968260973099662547544397413978036240384.00000000\n",
      "Iteration 6, loss = 4933653617861479301491689338589182067420109718907943029891772019433928562752392749556753313333041891901440.00000000\n",
      "Iteration 7, loss = 4933377726668776605645549618580198133699138376675221123450119720711641754251800679257121823134148504584192.00000000\n",
      "Iteration 8, loss = 4933101850902604395587810395492568914781663310673778803355625294271864380644520114741632582289896580841472.00000000\n",
      "Iteration 9, loss = 4932825990563471930312555290847861522089786865443878936706705224177255477042889651335226424976831970017280.00000000\n",
      "Iteration 10, loss = 4932550145650522636191735653246800074211532897468799381043822984234422546387955771701352868110004553515008.00000000\n",
      "Iteration 11, loss = 4932274316162887717381444824373591079060794806962851685097080052632469274778068832215662848584227112353792.00000000\n",
      "Iteration 12, loss = 4931998502099711618771622320072001789448440655098394232141136736146128483224499992732255764982902619635712.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 13, loss = 4931799333172997019720498839779444768429810527994791269563614151705670735676644741511309226718738998362112.00000000\n",
      "Iteration 14, loss = 4931736434966922372589308094001949504977963795236910110306634311572705552011169316664311938614926896005120.00000000\n",
      "Iteration 15, loss = 4931681248058843495873099215948922904149782312864977038129649338005111537441946587454994537202292399013888.00000000\n",
      "Iteration 16, loss = 4931626090737430879786186349196807075129809120902321010330772531025807417402468331470591654315315431473152.00000000\n",
      "Iteration 17, loss = 4931570934141752052469972673156507257296600881688391101409061999092515737184250823962222858071944355381248.00000000\n",
      "Iteration 18, loss = 4931515778163368459278256322754652147096216962886853968630302124806997514176587608008936563923565286522880.00000000\n",
      "Iteration 19, loss = 4931460622801864544871865062829642981608221851352855112442185057174122975727810389825581051182116811833344.00000000\n",
      "Iteration 20, loss = 4931405468057233179624881722680177821272706114262830852705332965417014895346346429077607148169127289421824.00000000\n",
      "Iteration 21, loss = 4931350313929469270947365466091040994975448728171378560748761684695046682681072339811765446542831260794880.00000000\n",
      "Iteration 22, loss = 4931295160418559578105470118902671757819475032120451402027912386422589202819067318549607484614638533869568.00000000\n",
      "Iteration 23, loss = 4931240007524504101099195681115070109804785026110049376542785070599642455760331365291133262384549108645888.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 24, loss = 4931200179459375147167687538066541788120408475696004014825154967708526036995719246390818221053904748019712.00000000\n",
      "Iteration 25, loss = 4931187601311461180907243310396718065839019388308497396247772682991521914629266974497946724653218832318464.00000000\n",
      "Iteration 26, loss = 4931176565116930972623290169839570246417911988316279460276951170646964849173317227226987378570455410540544.00000000\n",
      "Iteration 27, loss = 4931165534740262086779623507323769722035746014858899944022774392409876943042578033285357607157487689007104.00000000\n",
      "Iteration 28, loss = 4931154504410028454534489622387501551343716021271769175536409463974165176525096638262482977630323602358272.00000000\n",
      "Iteration 29, loss = 4931143474104549901991760579614610666880080995786816376704069825824309106818394361984982297425053252321280.00000000\n",
      "Iteration 30, loss = 4931132443823744947712382999561646330817304563277599488790008840510283288304497029200865033073429303001088.00000000\n",
      "Iteration 31, loss = 4931121413567613591696356882228608543155386723744118511794226508032087720983404639910131184575451754397696.00000000\n",
      "Iteration 32, loss = 4931110383336156852461670394858540438117171681875453971450919661357847722925341871303430633599473698209792.00000000\n",
      "Iteration 33, loss = 4931099353129370655936370868479269478811282618915283764823300968615062021849410014618163853472082767642624.00000000\n",
      "Iteration 34, loss = 4931088322947260094710399139306011336351940558309010520582354594644357208106732455807580252535044421189632.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 35, loss = 4931080357563020332941347913400590866306525419504007228230309605371080893870238529824096832417832695234560.00000000\n",
      "Iteration 36, loss = 4931077841993165675254960167850711844724667820024512114432351431826207407126512824974013305033369524895744.00000000\n",
      "Iteration 37, loss = 4931075634801740905170550075924113481391580535582017204026146437423373791769324149236005051101245649977344.00000000\n",
      "Iteration 38, loss = 4931073428769908031276439693793216034191129611519795408322047150408290989839139123104125152179198556635136.00000000\n",
      "Iteration 39, loss = 4931071222743413210158313832451385048917155462928608985543549449338000193956487253168275077095705056116736.00000000\n",
      "Iteration 40, loss = 4931069016717925703330485374479213810036099751838062513885719553743648969526041124785157972013419245600768.00000000\n",
      "Iteration 41, loss = 4931066810693421066361238306043667096199701565710223375727833472390229682862408485379176676891866924318720.00000000\n",
      "Iteration 42, loss = 4931064604669908465912466132332133115413558746746816302677662701990870196597611429666180126746225917558784.00000000\n",
      "Iteration 43, loss = 4931062398647380772358251682643309928117761862124277614595829411768693284240077217311619149898024583430144.00000000\n",
      "Iteration 44, loss = 4931060192625839004216583124220240668535155116531687837216530434691824263860030525506143628015616013631488.00000000\n",
      "Iteration 45, loss = 4931057986605284180005448624305968470888582714658127496273962603728388453527696031440403442767353299861504.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 46, loss = 4931056393537586389553735257175793608067265772658753928681061624917965669964852191604113690894416157343744.00000000\n",
      "Iteration 47, loss = 4931055890426003271587916992656141675786847725843239437172525200681986656450840256393679568704511701680128.00000000\n",
      "Iteration 48, loss = 4931055448989618872136955049789310462947516218779001475102574520323263452418650158998757111064956036448256.00000000\n",
      "Iteration 49, loss = 4931055007784992129785520257929412852969896483853911175116784739071915219590406730840508997161297857478656.00000000\n",
      "Iteration 50, loss = 4931054566581274923997518814107034103992151536277730355768766798356476023472800033932605213096950565437440.00000000\n",
      "Iteration 51, loss = 4931054125377600495965020394492466992373863185642931617257015842302300186304629779031996459103433124741120.00000000\n",
      "Iteration 52, loss = 4931053684173967827170036831842668383892187227260434433847335037941262390015671288948032853512392443691008.00000000\n",
      "Iteration 53, loss = 4931053242970372843540615457185465741655746842373916702602937053400861362325025854918114869650416155492352.00000000\n",
      "Iteration 54, loss = 4931052801766816563594744437763902199887386235672458949258018721649222421302918154132892389185857351843840.00000000\n",
      "Iteration 55, loss = 4931052360563301024368400108064064027032793816534222225280973708622596203089797540973665175455422216142848.00000000\n",
      "Iteration 56, loss = 4931051919359825207343594300842908088869125380270126004937605181352857389615439338249783346790757656690688.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 57, loss = 4931051600746652315728991834912401436528775679939240555729884854115887338183754358916482797023283239714816.00000000\n",
      "Iteration 58, loss = 4931051500124431840233911169751742920709184993225339286736358601459721561310161498671744802077834204938240.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 59, loss = 4931051411837231145489233690958003118010065202555715019240291571403750712156529333053371459342734330953728.00000000\n",
      "Iteration 60, loss = 4931051323596376074760130272355999857390791379117253234902715089954128012436383373576563671678366022369280.00000000\n",
      "Iteration 61, loss = 4931051235355695170607003452314372548877876557511561351112797046053934702724657213700885649302376394260480.00000000\n",
      "Iteration 62, loss = 4931051147115016303489852966758831508810650145284030518791272668089992029153380408206507390263092949549056.00000000\n",
      "Iteration 63, loss = 4931051058874339473408678815689376737189112142434660737938141956062299991722552957093428894560515688235008.00000000\n",
      "Iteration 64, loss = 4931050970633664680363480999106008234013262548963452008553404909970858590432174860361650162194644610318336.00000000\n",
      "Iteration 65, loss = 4931050882392992942872247684251769133505945569559484856371258362783793143352470795201821074833832807497728.00000000\n",
      "Iteration 66, loss = 4931050794152320186863026202154486898775784385466437178454914982628602378202542052851342105804667912978432.00000000\n",
      "Iteration 67, loss = 4931050705911653541961733723515463469382688429507872654943752600282163521473961373644762426785621568651264.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 68, loss = 4931050642189033019187049938283357391189868514151006820234124829794898900556724923009070683855399350697984.00000000\n",
      "Iteration 69, loss = 4931050622064591368531205406634529210160776468062019828197491978387166508550545576217682800870356963819520.00000000\n",
      "Iteration 70, loss = 4931050604407153062914648611913258891222072078368439921019832871718597911246223562037177919326372554080256.00000000\n",
      "Iteration 71, loss = 4931050586758985308026390063370596268611318768685805246501747440926674389126913337151895983132228785799168.00000000\n",
      "Iteration 72, loss = 4931050569110853201267717368334443343800112623120988972680551164019136999465466813939359905330443226972160.00000000\n",
      "Iteration 73, loss = 4931050551462718038955080171569161016320373863488931121656764388207223655593346259154874182523598393049088.00000000\n",
      "Iteration 74, loss = 4931050533814583895160431142046921823063479308545953796367174445363435629791450381561038341385106650824704.00000000\n",
      "Iteration 75, loss = 4931050516166451788401758447010768898252273162981137522545978168455898240130003858348502263583321091997696.00000000\n",
      "Iteration 76, loss = 4931050498518319681643085751974615973441067017416321248724781891548360850468557335135966185781535533170688.00000000\n",
      "Iteration 77, loss = 4931050480870183500812460387966290511738484053095182871966798282768322188526212103160830581306337607548928.00000000\n",
      "Iteration 78, loss = 4931050463222051394053787692930137586927277907530366598145602005860784798864765579948294503504552048721920.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 79, loss = 4931050450477529326534827270369802639734402333837154482672070117699582510821767644202455918255213788528640.00000000\n",
      "Iteration 80, loss = 4931050446452639570478474929899776615616602038054644348236867981262660587122217226777268507322510982774784.00000000\n",
      "Iteration 81, loss = 4931050442921153538983944638544391566585411887618457207976051092677947376573712307446207341683079047544832.00000000\n",
      "Iteration 82, loss = 4931050439391521210227878729527510803130674271308826903953470206081313053834119875097930812446274003927040.00000000\n",
      "Iteration 83, loss = 4931050435861893974061753656725845710790157678444599228601873484325305321445650828702903691551234418802688.00000000\n",
      "Iteration 84, loss = 4931050432332269793449593085653310021118173699647613130452867261473673543267855813879826215661254108774400.00000000\n",
      "Iteration 85, loss = 4931050428802643575801456180094688063000501311472465980835467372685791128949611444675448976434567615348736.00000000\n",
      "Iteration 86, loss = 4931050425273017358153319274536066104882828923297318831218067483897908714631367075471071737207881121923072.00000000\n",
      "Iteration 87, loss = 4931050421743391140505182368977444146765156535122171681600667595110026300313122706266694497981194628497408.00000000\n",
      "Iteration 88, loss = 4931050418213763904339057296175779054424639942257944006249070873354018567924653659871667377086155043373056.00000000\n",
      "Iteration 89, loss = 4931050414684137686690920390617157096306967554082796856631670984566136153606409290667290137859468549947392.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 90, loss = 4931050412135231236151151971619003838422704029965993382068570940997645059857360349136822657472894714511360.00000000\n",
      "Iteration 91, loss = 4931050411330257766419029439394388424179658471441445668411996578770012074626438845290644654627107756834816.00000000\n",
      "Iteration 92, loss = 4931050410623961374934513914917745921751695805105472660947190667427569686972917603176952326833903843147776.00000000\n",
      "Iteration 93, loss = 4931050409918031649925738597936631739547646826838488917793244624610241804600280149697217399647812940988416.00000000\n",
      "Iteration 94, loss = 4931050409212107017506904117170733228457818872016907803310282746633540512578766082170731880803487497322496.00000000\n",
      "Iteration 95, loss = 4931050408506183403606057803647877851590835121884407214561517701624964538627476691834896243627515145355264.00000000\n",
      "Iteration 96, loss = 4931050407800259789705211490125022474723851371751906625812752656616388564676187301499060606451542793388032.00000000\n",
      "Iteration 97, loss = 4931050407094332101732412507629994560965490802863083934127200279735311318443999202400625442602158074626048.00000000\n",
      "Iteration 98, loss = 4931050406388407469313578026864096049875662848041502819644238401758610026422485134874139923757832630960128.00000000\n",
      "Iteration 99, loss = 4931050405682485892448708047827326941454367507287163282363867022686284688611645098919604049918566462390272.00000000\n",
      "Iteration 100, loss = 4931050404976557185957920898089255893473162733709260064944117812837082124309232322630519004400828651929600.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 13784568855725746062704765431358884309121190953552769411767869810288370173291379665389948448246088945631232.00000000\n",
      "Iteration 2, loss = 16010546136680154914415627103958022550807218216289783222170758817191199187343980883788434899248198038061056.00000000\n",
      "Iteration 3, loss = 16008891248932431558332589092425495493753620546390614707145803481233662615520733947656364853257705007087616.00000000\n",
      "Iteration 4, loss = 16007193894628478689398222237970481315734417186485790235829462667720268090323015115279863988358241014775808.00000000\n",
      "Iteration 5, loss = 16005496719685897980975237480031120711710744138962780782299306595626650819216757389411318923493791890407424.00000000\n",
      "Iteration 6, loss = 16003799724687568944367961001818352999409749280203014564187299929646086666520672789546310587108381142548480.00000000\n",
      "Iteration 7, loss = 16002102909614439182089736354967309406307739696266177197607485168026375810611949787388152490989075455541248.00000000\n",
      "Iteration 8, loss = 16000406274447411481862427732419223254075877466892411166369244158417804434777891058251563353515405478985728.00000000\n",
      "Iteration 9, loss = 15998709819167423261019497013378794427962027631250596829244651069386919536693438301933357869790906980237312.00000000\n",
      "Iteration 10, loss = 15997013543755399714678548070134205202539924772240648236195418073882764297190837091942552154898878626267136.00000000\n",
      "Iteration 11, loss = 15995317448192272149065113778431896657720368700896962591588438342663133805523681064932061613932737634238464.00000000\n",
      "Iteration 12, loss = 15993621532458980018548632351962654947196912865766581305664179710230825695507361275080000705332725954904064.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 13, loss = 15992503611754276718441072821087061259223254275083852633153547826698454819881630749113274531454135037853696.00000000\n",
      "Iteration 14, loss = 15992154668544588591156465625553627342041624655172540553189781596223337036501424828735960805765370611761152.00000000\n",
      "Iteration 15, loss = 15991815539110739233358058601316507125398025964130909076395576838860703119928409508681182493093932293947392.00000000\n",
      "Iteration 16, loss = 15991476417006886799506518268408656209881950910203083614506705112082134095256023099968003503831408491626496.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17, loss = 15991137302094437472879975196978628356191090631445605412423317111203098874731541562104425698499199397527552.00000000\n",
      "Iteration 18, loss = 15990798194373281253535707324777765068258271021437777690852154875666063106770699758500261856915171108192256.00000000\n",
      "Iteration 19, loss = 15990459093843216474912057537683525769960339551741656354422245477782213813469011605406835408745411467280384.00000000\n",
      "Iteration 20, loss = 15990120000504114803742516763072475549218926431533095160624787963567760917978167776802261263777430920757248.00000000\n",
      "Iteration 21, loss = 15989780914355821425292883580002058004161712548071854197861863721867656073624017339707757408421559530422272.00000000\n",
      "Iteration 22, loss = 15989441835398187635936885570987975538253444016752176708940735139335602842153757424288441119098245908267008.00000000\n",
      "Iteration 23, loss = 15989102763631040287616534304712895333610606040430373319047940611390297151629192908133832512187464465514496.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 24, loss = 15988879238294764488970255926480127794674309461017700399892224626973272540302301371244028205369895243743232.00000000\n",
      "Iteration 25, loss = 15988809464941219421231369212429456364884236970155702133773282385009119698999413162995596604863027859161088.00000000\n",
      "Iteration 26, loss = 15988741652713582437241045472502122966833007915549247799428435704807166540223881048892519838624068227039232.00000000\n",
      "Iteration 27, loss = 15988673840801225659451915482090901981759651956441373434244840641861171383356723946079013747747937740390400.00000000\n",
      "Iteration 28, loss = 15988606029176480028197427916686009111878504509270516943031332784078743532674361293360392929754547312656384.00000000\n",
      "Iteration 29, loss = 15988538217839325173117819431426581672732681480255067811103975472097376626772299546923659751276835109863424.00000000\n",
      "Iteration 30, loss = 15988470406789789612716758709117827422561820600689280759020280029024579571616829668107010899028687699574784.00000000\n",
      "Iteration 31, loss = 15988402596027848902562529735926711140017660958035223169159522463625344733522559404334849212969630881021952.00000000\n",
      "Iteration 32, loss = 15988334785553507116727085180825405361991579371049217144458490107772173384770387464369774219773077020999680.00000000\n",
      "Iteration 33, loss = 15988266975366754070030543371383478746255133792840457427575214631783812344658067076305287102755495202521088.00000000\n",
      "Iteration 34, loss = 15988199165467618280976572990406139051047961954703198739067207358767770519151889201479584548630771993149440.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 35, loss = 15988154462752171674417694596611445547982402278332978492139550478462805521949288093334630045306303767642112.00000000\n",
      "Iteration 36, loss = 15988140508692917305443068340933207082179404731509604844880393715299924469002875861177344788295953261002752.00000000\n",
      "Iteration 37, loss = 15988126946793688735293510341918688226312988865829212136523614893951136018605065349347420315589884629745664.00000000\n",
      "Iteration 38, loss = 15988113384911499971085990319015804918630117448466014961279851629338918883066104237090016154431040117538816.00000000\n",
      "Iteration 39, loss = 15988099823040816386072807473528165792195382198956436479523513572670294668785096670413675318988719433711616.00000000\n",
      "Iteration 40, loss = 15988086261181627795074080133025339504780341070409671433912632394264010195059795877411898992579391661277184.00000000\n",
      "Iteration 41, loss = 15988072699333952531413595307882102472396189747229169287662751087546321187154246047516385045233412450811904.00000000\n",
      "Iteration 42, loss = 15988059137497778372875494991181937084368797773145963731963507656899723828225751054439334896930544701931520.00000000\n",
      "Iteration 43, loss = 15988045575673101245387826513952670803806788329403732663878114770451716845993412189418149020997376047841280.00000000\n",
      "Iteration 44, loss = 15988032013859927260058518879652562436047226644136959237811753426011052148878577515596726707444025038733312.00000000\n",
      "Iteration 45, loss = 15988018452058266602067453760712043323318554764236448711106391953258982917583493804881566772954022591594496.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 46, loss = 15988009511609242713256461446650374400326535729932174948616208715005365357357080461160527329372583397163008.00000000\n",
      "Iteration 47, loss = 15988006720821848493393408035466465666100979214769084148698730592997926649002746716614028898464951164731392.00000000\n",
      "Iteration 48, loss = 15988004008463859360575174937574787809786053305549823442304052858322994491488577383841984701446402818965504.00000000\n",
      "Iteration 49, loss = 15988001296107424486206885052566932777531383751867445006293742233007297709137265444001659930334672405397504.00000000\n",
      "Iteration 50, loss = 15987998583751460167149128433845005756230736764540269459482368438965497875229754366241580490002070356623360.00000000\n",
      "Iteration 51, loss = 15987995871395950107114094405520316598318605068543008390122782148707589900642449315511348273754947205464064.00000000\n",
      "Iteration 52, loss = 15987993159040906528317640974509382914469119120144628107025345357851077602218046418098761861613540052303872.00000000\n",
      "Iteration 53, loss = 15987990446686315171507933799409600825562460053698001249911302404842206526973400193334722910220905613361152.00000000\n",
      "Iteration 54, loss = 15987987734332194370008759890595746747609823553606577281996196283107232400172554830650929289607399539212288.00000000\n",
      "Iteration 55, loss = 15987985021978527827532308572179130533045702344845067791532877665156150132691915494996982893079372362678272.00000000\n",
      "Iteration 56, loss = 15987982309625321655186508847618010987207161655547955932926527548797711632952830249516783010646942633951232.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 57, loss = 15987980521539278468058209646812580514416974606399298821964234419027412817861325381280718099492293056659456.00000000\n",
      "Iteration 58, loss = 15987979963382792068013269126197028754311256352406744937382132838767235003817384086940663110954019161374720.00000000\n",
      "Iteration 59, loss = 15987979420912046944709316122494405154413439336261108940772785852746764860706649964398335204295517862363136.00000000\n",
      "Iteration 60, loss = 15987978878441566636082286601982996452455115539276409635054615438438877415854331911424976531408820405010432.00000000\n",
      "Iteration 61, loss = 15987978335971092438563186084929846555833856970426193483741626021939741879423361921595517148532241497849856.00000000\n",
      "Iteration 62, loss = 15987977793500644722511777916195818149006547723492071001517754262611864612818233538722954689032842974855168.00000000\n",
      "Iteration 63, loss = 15987977251030203117568298750920048547516303704692431673699063501092739254634453218994291519543563002052608.00000000\n",
      "Iteration 64, loss = 15987976708559800216308369940879918046494139464077852323779852392362375983119210632510323853451700513800192.00000000\n",
      "Iteration 65, loss = 15987976166089399352084417465325873813917663632841434025329034949568263347744417400407655950696544208945152.00000000\n",
      "Iteration 66, loss = 15987975623619018858220228334632692265798071895386626241562154166136657073774117712117985681308449738063872.00000000\n",
      "Iteration 67, loss = 15987975081148652623607873545342114596798299023578945818074029044258805252786963504497413755277298550964224.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 68, loss = 15987974723531597171287634058534715889356029998986925466304774096710793327530453980323942975966673626988544.00000000\n",
      "Iteration 69, loss = 15987974611900345113477320580002720696829169036383590031986693164443521887039641388720786724333896119353344.00000000\n",
      "Iteration 70, loss = 15987974503406232348056908733114531555182859320085729548802231020904689181717493072199456930395565924024320.00000000\n",
      "Iteration 71, loss = 15987974394912174582607857917350671661570136656998217455264397857644623652187477323973220746548302680424448.00000000\n",
      "Iteration 72, loss = 15987974286418098483835020091212035351946218309507255898511021700958302397393417386315286692670683786248192.00000000\n",
      "Iteration 73, loss = 15987974177924069236889637958253383216573133377713998525530699860805745773829692599427247195537307110211584.00000000\n",
      "Iteration 74, loss = 15987974069430007397368634473517350786069033895870164329056079365673178972018778142438411485016631499816960.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 75, loss = 15987973960935937409703725650836973281782180776513685926707884206795609625646066267924376721149131155832832.00000000\n",
      "Iteration 76, loss = 15987973852441881681290651169559199656615146522804334884638444709471794732256499874079440300638574095630336.00000000\n",
      "Iteration 77, loss = 15987973743947807619553789677906649615436916584691534379353462218721724113602889290802806010097661384851456.00000000\n",
      "Iteration 78, loss = 15987973635453786520752312882892342553846585290410921212246715042314170034600961921439965566311109442404352.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 79, loss = 15987973563930373393252288651044776543912443076114356090424470386868317013409210662223971647112278274211840.00000000\n",
      "Iteration 80, loss = 15987973541604104648366438944963601089395875199190239540345311206988607000047003954471642526755367122108416.00000000\n",
      "Iteration 81, loss = 15987973519905295947126995650091349886497294439702162593693495706647344784737629900960214958657303130144768.00000000\n",
      "Iteration 82, loss = 15987973498206515764391221038024306441838351411508340367599191529413591475394546808786984077273125705744384.00000000\n",
      "Iteration 83, loss = 15987973476507739655727399094929435534070785202070840244441674684052339438332362425376352722562360648138752.00000000\n",
      "Iteration 84, loss = 15987973454808910584128192455196321646715320348801152783105922524348570861618494828051927521097234822201344.00000000\n",
      "Iteration 85, loss = 15987973433110112068068630832754501786045181636203881093796075353688561827011367546446998769682701747224576.00000000\n",
      "Iteration 86, loss = 15987973411411297255721258534423991777809535648581320992739078855538547703280645429791671911574519205068800.00000000\n",
      "Iteration 87, loss = 15987973389712519110021460256843034601596281029765659818113168344241045030078011691999740793527047964065792.00000000\n",
      "Iteration 88, loss = 15987973368013690038422253617109920714240816176495972356777416184537276453364144094675315592061922138128384.00000000\n",
      "Iteration 89, loss = 15987973346314893559398668329154187122016365873276861718935962679813518054897466167451686603984095246548992.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 90, loss = 15987973332010236193144770030412143648756073706706745732779595206333855338800687909936604885519485687037952.00000000\n",
      "Iteration 91, loss = 15987973327544986110832357491270863841054999268202612315406871969043164481181055406272478635454174586732544.00000000\n",
      "Iteration 92, loss = 15987973323205207259482267622613288945531500477528444093742002075110406694539406018767275109806229847801856.00000000\n",
      "Iteration 93, loss = 15987973318865442667384012095358317929127820552501403232355887842731403360880902111931169927515228392652800.00000000\n",
      "Iteration 94, loss = 15987973314525678075285756568103346912724140627474362370969773610352400027222398205095064745224226937503744.00000000\n",
      "Iteration 95, loss = 15987973310185917557259453709820548433211837521203643612520446709845897965844793007021559089606637849149440.00000000\n",
      "Iteration 96, loss = 15987973305846136668873387506676887269242650321151314339387183149976889543062694265135055800621986926821376.00000000\n",
      "Iteration 97, loss = 15987973301506376150847084648394088789730347214880595580937856249470387481685089067061550145004397838467072.00000000\n",
      "Iteration 98, loss = 15987973297166603410604923783194772699543913652340910513678167353346381603464787742700245909366571649728512.00000000\n",
      "Iteration 99, loss = 15987973292826842892578620924911974220031610546070191755228840452839879542087182544626740253748982561374208.00000000\n",
      "Iteration 100, loss = 15987973288487062004192554721768313056062423346017862482095576892970871119305083802740236964764331639046144.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 13482668385485480182247064070531028830862210455390153120550968620744838176678680659436633630551934398103552.00000000\n",
      "Iteration 2, loss = 19862774411446074319051747736640687449339279628806759073665091656163888325669863524992610186690037423276032.00000000\n",
      "Iteration 3, loss = 19937705324589393556610717473710531016879690926423678310975862197431011050977440991766187962206820497358848.00000000\n",
      "Iteration 4, loss = 19937470324699854881002529646520293077124887615822838413412766472711817695788577888195339587530705747312640.00000000\n",
      "Iteration 5, loss = 19936577784243264843964731890556369466542545843906408372867404441539366596814609539279746956010101191737344.00000000\n",
      "Iteration 6, loss = 19935679544815551570579065722903458193507131435989385898798048075263154562288497898624889808760517697732608.00000000\n",
      "Iteration 7, loss = 19934781295768832194783376186725770273061328788897666581734520777308049157484036497609570934710560790413312.00000000\n",
      "Iteration 8, loss = 19933883086757683823965557334349965887520545381093094632964743335810543817256891043609710496769281833828352.00000000\n",
      "Iteration 9, loss = 19932984918213652529712070043152015255974307975994423631686844345467904895802787214980171376141633562083328.00000000\n",
      "Iteration 10, loss = 19932086790138714236919958764635598770740373670417873502242679764443249449357598761581724009432613870632960.00000000\n",
      "Iteration 11, loss = 19931188702531076353930049151044800199612942211581718952445823568836017674325893827870576660340781369786368.00000000\n",
      "Iteration 12, loss = 19930290655388909622435592833874150478363821977897335763678763747893138316584152179439541852503983368699904.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 13, loss = 19929642712392866647286170703733173993847420028520234766459605726999343378412851444112458397917345510064128.00000000\n",
      "Iteration 14, loss = 19929434748634421872255384160052341690978287882967941026812246361010323435357325758000272772222577035706368.00000000\n",
      "Iteration 15, loss = 19929254904819448499015509945556252927270649895109924919437438189554809628703067015022986279282349736722432.00000000\n",
      "Iteration 16, loss = 19929075308038309085677938816887647763816784120563641600434204243097322934321942130768653889310853603262464.00000000\n",
      "Iteration 17, loss = 19928895715017586973517261031136663783863152256394234171379736597784166776819789631028635379034857427435520.00000000\n",
      "Iteration 18, loss = 19928716123633985860745130354225731697878992613942348758009921266509762703373867585519815786110921687957504.00000000\n",
      "Iteration 19, loss = 19928536533868948349617139617908945965642895758160806483258461570030815474490557580601351113145715634733056.00000000\n",
      "Iteration 20, loss = 19928356945722311477255182063299405111499788938796723229653864233447274198933911265769260293202744595972096.00000000\n",
      "Iteration 21, loss = 19928177359194042651083636338619728840318657605799522173701830601779128698456738970922747112894709637316608.00000000\n",
      "Iteration 22, loss = 19927997774284137797030549774897744615208124940412881212465573343153877700778141987299212045548198391971840.00000000\n",
      "Iteration 23, loss = 19927818190992592841023969703161279899276814123880478243008305125699019933617221606136055564489798493143040.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 24, loss = 19927688617495926604189657363416746552570344504974894843661136632262593005774997844384982135812690452414464.00000000\n",
      "Iteration 25, loss = 19927647028743627788888097298322697712986809775826903948814956705516247734718424979477294972718435794944000.00000000\n",
      "Iteration 26, loss = 19927611063102988006920983653950366886008495839743496331166065227833716728402913277366241786678699785977856.00000000\n",
      "Iteration 27, loss = 19927575146604814487891274259816808995279471712433593770637478164877753447975375823870605915613509040734208.00000000\n",
      "Iteration 28, loss = 19927539230599713675077232560808873457212691206518843340669305635133270507875863781507884747626834363940864.00000000\n",
      "Iteration 29, loss = 19927503314663098011787556284021283063190233994378643278096181882135174609683646038443206959702045506928640.00000000\n",
      "Iteration 30, loss = 19927467398791235648113600650943994020710934095221947292820910910672300344095505368135406118993414485704704.00000000\n",
      "Iteration 31, loss = 19927431482984118435911460323632661255992037871536111178969918056999645166549644353059283172154116566679552.00000000\n",
      "Iteration 32, loss = 19927395567241726004821371957226422084576661229539524421859266661754702715641569449401840485817089915879424.00000000\n",
      "Iteration 33, loss = 19927359651564066502987240889669621580247557806744831227362531388682475535933078074688277113329159266893824.00000000\n",
      "Iteration 34, loss = 19927323735951131782265161783017914669221973965639387389606137574037961082862372811393394001343499886133248.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 35, loss = 19927297821855871775907604758418701480239176770502910614830047673982820094075812932457757951810575089532928.00000000\n",
      "Iteration 36, loss = 19927289504265369855481762998426904156315678562894246781179750599527503841421961280163876581377434200309760.00000000\n",
      "Iteration 37, loss = 19927282311262134204426188679090425410832945478234159626040345788266499920021135540735195928294956038356992.00000000\n",
      "Iteration 38, loss = 19927275128076813070725794956658491960451022772662800996663015919814005939216317478063667778024835309895680.00000000\n",
      "Iteration 39, loss = 19927267944979744483664116509428052651073747966540836183972869073484072107239328630823086427213404089352192.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 40, loss = 19927260761886020709675579288351266129516841359359317882385121694477682817880179677676716475273526007300096.00000000\n",
      "Iteration 41, loss = 19927253578794892119520892202548385607766968299954979151530904718254603971453508206306245014297325573505024.00000000\n",
      "Iteration 42, loss = 19927246395706354639128102583047238548932751969571497888473430812942334295678415507949072517611390421172224.00000000\n",
      "Iteration 43, loss = 19927239212620404194425257760875652416122815549452551990275912646668372518274002873842599458542308183506944.00000000\n",
      "Iteration 44, loss = 19927232029537040785412357736033627209337159039598141456938350219432718639240270303986825837090078860509184.00000000\n",
      "Iteration 45, loss = 19927224846456268486161355177493335465467159258764588391397530863107873930858116507144351179928114818973696.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 46, loss = 19927219663661371657497218108584464056643758225971041348654403555156889086549248784809817616683452585738240.00000000\n",
      "Iteration 47, loss = 19927218000149674899707254847046898077725142305636389977966672377463325607135068887404977367866382535884800.00000000\n",
      "Iteration 48, loss = 19927216561554025840967674278240870586969676938960328429789490156395685657061439649476340560192178487296000.00000000\n",
      "Iteration 49, loss = 19927215124921529463700927985354332259504981587434398516639980678153613342244108301568624236368099991879680.00000000\n",
      "Iteration 50, loss = 19927213688306274558937876782701970056346983212663608231974459684336925320190112436982104794425157771853824.00000000\n",
      "Iteration 51, loss = 19927212251691272246635891056324305140454347600784788329389753266615316179551836515676756006233782293102592.00000000\n",
      "Iteration 52, loss = 19927210815076379934276627392195298720628886095326665206098304809451241390497825730961594438224540717809664.00000000\n",
      "Iteration 53, loss = 19927209378461593547788133121342778259979221877532916759163326980972199680747181374074020563724020679180288.00000000\n",
      "Iteration 54, loss = 19927207941846896790882597567878053610939847672378254576837670453688185961176308609963636276038572710035456.00000000\n",
      "Iteration 55, loss = 19927206505232318182063689414606332531750401211156933379678846550706709137751498399968638261882083377938432.00000000\n",
      "Iteration 56, loss = 19927205068617829202827739978722407264171244762574698447129343948920260304506459782750829834540666115325952.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 57, loss = 19927204032059821910662819381701000284689073511274442799298175872108866901867118149040170186167924385906688.00000000\n",
      "Iteration 58, loss = 19927203699357739225637844874640356913062089908855805010178231544537734359680900821602972316829489484005376.00000000\n",
      "Iteration 59, loss = 19927203411638813117487562209487778259479837773336697847382161693949269983711110412147221288965267014025216.00000000\n",
      "Iteration 60, loss = 19927203124312493101200130385686062217207478728309684393970902400690911501107187328120057197830595453911040.00000000\n",
      "Iteration 61, loss = 19927202836989607527568798505425794774365777894862203716271363875951125551300875730964294092382549101772800.00000000\n",
      "Iteration 62, loss = 19927202549666750472441135307970735089763714792708977759129336674318848507460855095146727673648389317197824.00000000\n",
      "Iteration 63, loss = 19927202262343905639529330117432193015835782146824718110797671468304075280463530585616959834934466633007104.00000000\n",
      "Iteration 64, loss = 19927201975021064880689477595865823478799226319696780565402793594161803325747104784849791522893956315611136.00000000\n",
      "Iteration 65, loss = 19927201687698232269993530412243799015545424130081487225881490383764533915592476401607822264200270731804672.00000000\n",
      "Iteration 66, loss = 19927201400375407807441488566566119626074375577978838092233761837112267049999645435891052058853409881587712.00000000\n",
      "Iteration 67, loss = 19927201113052571122673588713971922625929196569607222649775671294842496367564118343886483273486311930986496.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 68, loss = 19927200905741007960516959682906063076811704415656599287815238599081729646476697879512786894541839832973312.00000000\n",
      "Iteration 69, loss = 19927200839200604460542213322204886520538713515193102459388969195559507209338330282065665806029072426336256.00000000\n",
      "Iteration 70, loss = 19927200781656835535199967465063060937387770363114569438576904552931819423267967035224913707149877399519232.00000000\n",
      "Iteration 71, loss = 19927200724191569902313700032713848714176747826606637906719937761531147217834822934914441078253578140778496.00000000\n",
      "Iteration 72, loss = 19927200666727000935731338994606140299391161297429785977053604720328192572435358033008487510510793603940352.00000000\n",
      "Iteration 73, loss = 19927200609262440117292883294442776958388328405765578253260846342870240471597690548627732996114833800691712.00000000\n",
      "Iteration 74, loss = 19927200551797879298854427594279413617385495514101370529468087965412288370760023064246978481718873997443072.00000000\n",
      "Iteration 75, loss = 19927200494333310332272066556171705202599908984924518599801754924209333725360558162341024913976089460604928.00000000\n",
      "Iteration 76, loss = 19927200436868745439761658187036169324705699274503988773072209214878880352241991969197670872906717290561536.00000000\n",
      "Iteration 77, loss = 19927200379404176473179297148928460909920112745327136843405876173675925706842527067291717305163932753723392.00000000\n",
      "Iteration 78, loss = 19927200321939615654740841448765097568917279853662929119613117796217973606004859582910962790767972950474752.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 79, loss = 19927200280477300577866344041168622136958955331619011185458958857942319498418836264778663798975031110795264.00000000\n",
      "Iteration 80, loss = 19927200267169230470458471708356035421621936880292749287409352040106378318921499388071998350623349783134208.00000000\n",
      "Iteration 81, loss = 19927200255660466092802945597600021709074168521110605215611292048712337453777090095921089161496638624104448.00000000\n",
      "Iteration 82, loss = 19927200244167415410668863712513482786566790105062812171001971089555703776059000501116554351721426192433152.00000000\n",
      "Iteration 83, loss = 19927200232674499172909219903508637581474846707973648523306632082191612083610568295477803922168821864988672.00000000\n",
      "Iteration 84, loss = 19927200221181587009221528763475964913274280129640806978548080406700021663443034798601653019289629904338944.00000000\n",
      "Iteration 85, loss = 19927200209688678919605790292415464781965090370064287536726316063080932515556400010488101643083850310483968.00000000\n",
      "Iteration 86, loss = 19927200198195762681846146483410619576873146972975123889030977055716840823107967804849351213531245983039488.00000000\n",
      "Iteration 87, loss = 19927200186702850518158455343377946908672580394642282344272425380225250402940434307973200310652054022389760.00000000\n",
      "Iteration 88, loss = 19927200175209942428542716872317446777363390635065762902450661036606161255053799519859648934446274428534784.00000000\n",
      "Iteration 89, loss = 19927200163717030264855025732284774109162824056732921357692109361114570834886266022983498031567082467885056.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 90, loss = 19927200155424558286521830379026699441610130151060229144400345443339937214351084200079319274526986893000704.00000000\n",
      "Iteration 91, loss = 19927200152762942635411474844875313083786175733292447923615709147023748469539257341232946374187285680750592.00000000\n",
      "Iteration 92, loss = 19927200150461197908024274960668455415059375698968663315129671812489942841072172900327963589708768182534144.00000000\n",
      "Iteration 93, loss = 19927200148162586141968677516062278615801349288256575865033092687909615596616195497862016817084360749481984.00000000\n",
      "Iteration 94, loss = 19927200145863998820344796085289137037891583790082421032557237554564295985845610347971667204500427517198336.00000000\n",
      "Iteration 95, loss = 19927200143565415572792867323488167996873195110664588303018169753091477647355923906843917118589906651709440.00000000\n",
      "Iteration 96, loss = 19927200141266832325240938561687198955854806431246755573479101951618659308866237465716167032679385786220544.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 97, loss = 19927200138968249077689009799886229914836417751828922843940034150145840970376551024588416946768864920731648.00000000\n",
      "Iteration 98, loss = 19927200136669673978280986376029605947600782709923734320274541012418025176448662000985865914205168788832256.00000000\n",
      "Iteration 99, loss = 19927200134371082582585152276284291832799640392993257384861898547200204293397178142332916774947823189753856.00000000\n",
      "Iteration 100, loss = 19927200132072507483177128852427667865564005351088068861196405409472388499469289118730365742384127057854464.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 10811827949550361989678646895594818488596725616403179088902423061235594032609082694631617101661338179993600.00000000\n",
      "Iteration 2, loss = 13854159990544455584118288254813615757043036477142203085799712295326051859002147282227196556966069115289600.00000000\n",
      "Iteration 3, loss = 13856717328809955202690556134511525683287614721813955211531811952353463841189500665261041516603367722319872.00000000\n",
      "Iteration 4, loss = 13855728885330169935630086031587247927365638884653516574372241535881826322462936252938961938143490294677504.00000000\n",
      "Iteration 5, loss = 13854737464439079847640606570867004409512417550015746531928316370274974376510767769343791463694020638670848.00000000\n",
      "Iteration 6, loss = 13853746111867593444291798315233040891776836645536013151116724643838466475814766724322834203692258425831424.00000000\n",
      "Iteration 7, loss = 13852754830228411031794296784447179545667076191200768742185002525701965724576413218571246054152322947219456.00000000\n",
      "Iteration 8, loss = 13851763619518733722716618394626887526807261701416728587560253019457098065818294332183152190439918214840320.00000000\n",
      "Iteration 9, loss = 13850772479733487260441713940931270136987565415184713479473854277903528873681068301340842140819941385830400.00000000\n",
      "Iteration 10, loss = 13849781410867587203172652546089001335769717524614738952815216124159670341603146590320106616873758700339200.00000000\n",
      "Iteration 11, loss = 13848790412915965405400314008717445230280955496842108954220897708833939752146535498447134436876385865695232.00000000\n",
      "Iteration 12, loss = 13847799485873547610507649123975707122311451570867644275922277184726002479451893261904215129092720039034880.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 13, loss = 13847097840610183063510937481961434888940018814369875986949904106369055654339750721233120140582805250768896.00000000\n",
      "Iteration 14, loss = 13846880209254605439493005219248481829514218400434671947518645919488554107388968215796224462587454784798720.00000000\n",
      "Iteration 15, loss = 13846682029516788816637207456785201259115916944297383271770820294646093875831307148039241252314740590379008.00000000\n",
      "Iteration 16, loss = 13846483869331158494267500554942742630507458769666617303834827547033282962474721183246980679174144783286272.00000000\n",
      "Iteration 17, loss = 13846285711995793724776485943700991114926547524298609044620873451908073760944671329472175618732894619435008.00000000\n",
      "Iteration 18, loss = 13846087557496253960127928451194389700888049126409664634686260187189706671595684377680803777080855994826752.00000000\n",
      "Iteration 19, loss = 13845889405832508644782183060131644361706637435327368302005082763834422152321020012153368704167436158500864.00000000\n",
      "Iteration 20, loss = 13845691257004523149127652084249288533805609491622982171614648860925959388733039208407774423268629992701952.00000000\n",
      "Iteration 21, loss = 13845493111012246547264927161395165506042755060842479956805116830058052477320508106911526850966782912495616.00000000\n",
      "Iteration 22, loss = 13845294967855646246618386939791894983287059592935284834082188016250691239836237037563829773874595983523840.00000000\n",
      "Iteration 23, loss = 13845096827534675395360575726259492791287689672203692619672808102970111045049890849594788635247826987646976.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 24, loss = 13844956523954696962845898390751700440214842917294331897204155726086270465638850077764086314973351366361088.00000000\n",
      "Iteration 25, loss = 13844913004230261029128663285794100815016778694202319761353404058534364999263577282646172485128185765167104.00000000\n",
      "Iteration 26, loss = 13844873373716381986301413839968299586080235904910221607601823387954743504476771791375809191970403996663808.00000000\n",
      "Iteration 27, loss = 13844833746658623736434259536521166586817957297540808892216404129526924613963482497298513014665033918971904.00000000\n",
      "Iteration 28, loss = 13844794119717147648240183036822581541232839633116858198615011176030419164861138705336907149895436897091584.00000000\n",
      "Iteration 29, loss = 13844754492889112056532197732782199967589551829103702728388480546395589741242891053577821742956487091683328.00000000\n",
      "Iteration 30, loss = 13844714866174476220590776934678296496974325697938121452168938921897423620299752454395261527114060834799616.00000000\n",
      "Iteration 31, loss = 13844675239573270695955565659802165156072487380292530141982291291579680344138463223508722952418750877401088.00000000\n",
      "Iteration 32, loss = 13844635613085487334482658570209460871101283238654284591954962991697357368197225943393006965523732485898240.00000000\n",
      "Iteration 33, loss = 13844595986711124099136079331414097373615024863645223750618560356314204056335591259666813803092299476893696.00000000\n",
      "Iteration 34, loss = 13844556360450168767699969936499557052939581798996381309162721389812716591710863046042344885104214750003200.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 35, loss = 13844528300752977476622550141275950770393491887109687197958108470390858141853024767568619499281907699417088.00000000\n",
      "Iteration 36, loss = 13844519597069928449833966360430073332007194420059047256347705183413745778569443155559056317521641057812480.00000000\n",
      "Iteration 37, loss = 13844511671184482379954861724220024541959731690700679947342892855129190795241341514671694241403426423439360.00000000\n",
      "Iteration 38, loss = 13844503745972089292944481966801760354928234780779006207352455191544821705503522290276536719701855749275648.00000000\n",
      "Iteration 39, loss = 13844495820764807129198725434973943698128957000663410601561725362013298692153133208562485409796099219128320.00000000\n",
      "Iteration 40, loss = 13844487895562057370500313134688074332986390086956154512946902240639441091302557625240407524061600748142592.00000000\n",
      "Iteration 41, loss = 13844479970363846127957174069402411064837599267791721095913166825232000811373143603454202352508478886510592.00000000\n",
      "Iteration 42, loss = 13844472045170179512677237242575212699019649771304593504865700113599729760786239206347769185146852184424448.00000000\n",
      "Iteration 43, loss = 13844464119981045302444644647289961624858411141225805430994140110125124122699148307633309441956483541499904.00000000\n",
      "Iteration 44, loss = 13844456194796445534295372618032744110799571786933517925766880480744434533252320261692122886274079141134336.00000000\n",
      "Iteration 45, loss = 13844448269616386319337350158261818962180196936562214143589102223266412900867103131668108808109757533519872.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 46, loss = 13844442657717691632498922789222987395263130290535330161277249496705514910479252198102450133740352727678976.00000000\n",
      "Iteration 47, loss = 13844440916991559525389080095687142284828773274634386505784828949008864489837835071354000196070224322101248.00000000\n",
      "Iteration 48, loss = 13844439331823168455032207424033498789908788773510402814024820031142169812890957951326517228581984502022144.00000000\n",
      "Iteration 49, loss = 13844437746788651389040037177969417545631068605130721625096536443667282286275650770319480749424117557493760.00000000\n",
      "Iteration 50, loss = 13844436161754431730300411766873931494423856205962553950553728082884987636165949328982209717425353388982272.00000000\n",
      "Iteration 51, loss = 13844434576720403552942561797470554677111354288341525114039924320110252783258487199487116439076970459824128.00000000\n",
      "Iteration 52, loss = 13844432991686548523642700259384510677682367167864185652339582161916822002289220192402503044348613119442944.00000000\n",
      "Iteration 53, loss = 13844431406652872753508756156074058301473960072665018719857882606113447201679496370872268823250399918030848.00000000\n",
      "Iteration 54, loss = 13844429821619386427720611159969628890714575049634829573936793982381381562131562506802912592465861772574720.00000000\n",
      "Iteration 55, loss = 13844428236586073249990454595182532297838704823748329802829166963230619994521823765144036245301349215895552.00000000\n",
      "Iteration 56, loss = 13844426651552945442534144468629286133520479851274485715345363544278666315692976272183438361777099348377600.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 57, loss = 13844425529174844689498627188532096904159685343986226509765179144899248301764776878911576302282331075379200.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 58, loss = 13844425181030033008601440351192092137614974090199627857631645419980547735831982005594518130101684333969408.00000000\n",
      "Iteration 59, loss = 13844424863996698646202871078112725552263180693008416607144494446446316180950457601152421587840040127430656.00000000\n",
      "Iteration 60, loss = 13844424546990120751353455279776495001028644477962616394008145545564190330871202995082716473214114845425664.00000000\n",
      "Iteration 61, loss = 13844424229983563226863802826301127134250992356698426695555733304044570842196441932826008991955251397394432.00000000\n",
      "Iteration 62, loss = 13844423912977015887554032045256190609701782282325042254445289392206204534223927642475800327379918866350080.00000000\n",
      "Iteration 63, loss = 13844423595970474659352190267669512890489637436086140967740026478176590134672761415269490952814704885497856.00000000\n",
      "Iteration 64, loss = 13844423278963941579294253828027180245060246227359883886908338227891978279683392605588380631596315638235136.00000000\n",
      "Iteration 65, loss = 13844422961957416647380222726329192673413608656146271011950224641352368969255821213432469363724751124561920.00000000\n",
      "Iteration 66, loss = 13844422644950895789538144293603377638658347903688980239928898386685260931109148530039157622526598977683456.00000000\n",
      "Iteration 67, loss = 13844422327944389190947900202280166483022906016878816828186327793571907345945621327314944224685390114586624.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 68, loss = 13844422103468830151420086780843316690521399396765996531122100891783542827373462080099564712887621961908224.00000000\n",
      "Iteration 69, loss = 13844422033839889000414803292030612929047616603541551735966688272536809330047576391001670617153236920958976.00000000\n",
      "Iteration 70, loss = 13844421970433231090893385309153519193138286925367218112330190207949465818089248669390970267382415286599680.00000000\n",
      "Iteration 71, loss = 13844421907031930585989727024683311469389473911756449850569033555701295355512722970598647493148855987339264.00000000\n",
      "Iteration 72, loss = 13844421843630619895906187067782672403412218851254876331465908573771871712233950499899825902231765771091968.00000000\n",
      "Iteration 73, loss = 13844421780229329576182410455742896021891847884534913327046720251204954430359671573014001944681737388818432.00000000\n",
      "Iteration 74, loss = 13844421716828022960170823167814429492805969642789661910880382601148032059361797811077779880438059539365888.00000000\n",
      "Iteration 75, loss = 13844421653426724492303141217830308037502845038557054700587619614836112232925721466666756869541206423502848.00000000\n",
      "Iteration 76, loss = 13844421590025417876291553929901841508416966796811803284421281964779189861927847704730534805297528574050304.00000000\n",
      "Iteration 77, loss = 13844421526624129593603753652348151395342284239470001331470487308148523216194018132226010611084206375174144.00000000\n",
      "Iteration 78, loss = 13844421463222822977592166364419684866256405997724749915304149658091600845196144370289788546840528525721600.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 79, loss = 13844421418327719317830509018076659981538858311214830061764878941478930486043509938371911697827799628775424.00000000\n",
      "Iteration 80, loss = 13844421404401926198743109117547512184974449570062354579209651619382582259841254350037213446672827780431872.00000000\n",
      "Iteration 81, loss = 13844421391720597061281997122355396959927409725681281116244424405588614320818128030972633092722710873636864.00000000\n",
      "Iteration 82, loss = 13844421379040341441780413401330745205758161623591081777122659140198731627811811470853028017216752617259008.00000000\n",
      "Iteration 83, loss = 13844421366360077674134924342361748377806159883988238232127319211063846390243697493208223888363969627291648.00000000\n",
      "Iteration 84, loss = 13844421353679820017597364286851010355191223372519877841537160279737713061096931578707319049521305187516416.00000000\n",
      "Iteration 85, loss = 13844421340999562361059804231340272332576286861051517450947001348411579731950165664206414210678640747741184.00000000\n",
      "Iteration 86, loss = 13844421328319302667486267841343448041515661940204996008888448751149195766662950395324209608499270124568576.00000000\n",
      "Iteration 87, loss = 13844421315639036862804802447888364945117971791223991412424715156078059892954387063298105716309780951203840.00000000\n",
      "Iteration 88, loss = 13844421302958779206267242392377626922503035279755631021834556224751926563807621148797200877467116511428608.00000000\n",
      "Iteration 89, loss = 13844421290278513401585776998922543826105345130774626425370822629680790690099057816771096985277627338063872.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 90, loss = 13844421281299486965932711793092897297513908047213791510551466221736754837075272738119882278132304245161984.00000000\n",
      "Iteration 91, loss = 13844421278514339342109504019211933587807743709625366091969746553373238626993248134111961349919523265839104.00000000\n",
      "Iteration 92, loss = 13844421275978066588694962082920817230082995148863403824384162646431192876311095065402626083784698860929024.00000000\n",
      "Iteration 93, loss = 13844421273442017909237816940099190401383971619699157218321881992476717101078370978636264784687554629730304.00000000\n",
      "Iteration 94, loss = 13844421270905965155708719128305391035793571271778588509322814006649740053564748183107303958916998031736832.00000000\n",
      "Iteration 95, loss = 13844421268369912402179621316511591670203170923858019800323746020822763006051125387578343133146441433743360.00000000\n",
      "Iteration 96, loss = 13844421265833865759758452508176051109949835804071934245729859032804537866958850655193281597386003385942016.00000000\n",
      "Iteration 97, loss = 13844421263297806895121425692923992939022370228016882382325610049168808911023879796520421481605328237756416.00000000\n",
      "Iteration 98, loss = 13844421260761758215664280550102366110323346698852635776263329395214333135791155709754060182508184006557696.00000000\n",
      "Iteration 99, loss = 13844421258225701388063230069336394207841569532175744964327474077514854815996634205462499830064215041769472.00000000\n",
      "Iteration 100, loss = 13844421255689654745642061261000853647588234412389659409733587089496629676904359473077438294303776993968128.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 7573762716751973621748137866359506833826897912675802717822669841268316571894941806668843067669390286651392.00000000\n",
      "Iteration 2, loss = 9926094619683000487174324521676320228618344263875586231793025689614519593554901138185133857863910004097024.00000000\n",
      "Iteration 3, loss = 9930124785340404631275381295588232076760880591561172441929433920343915454624161808248571896188700230418432.00000000\n",
      "Iteration 4, loss = 9929513795390937842692172054705464741034823802764871979593896149795727797301034252299399828470340127817728.00000000\n",
      "Iteration 5, loss = 9928896085675556391625090831344278718448698244113911821292247114992537802429924126671929663476305106042880.00000000\n",
      "Iteration 6, loss = 9928278404551726719352169486794918795980227241038814830910566063043126095034000311140686639333569978171392.00000000\n",
      "Iteration 7, loss = 9927660761839797601220308328835021061327870379530766038941358730575512803546248968116040446218381337559040.00000000\n",
      "Iteration 8, loss = 9927043157551687734727040435555342190214510931210077586955964510592169985735842582582906367198618242252800.00000000\n",
      "Iteration 9, loss = 9926425591685021935923959796179293174967463561140963462807368921424855901838833945945760351674870850912256.00000000\n",
      "Iteration 10, loss = 9925808064237408724574849724041594860348535659362349242601408153915323722967681014558680242354079855017984.00000000\n",
      "Iteration 11, loss = 9925190575206454583407517197990881822673846206534999450975524732969075984094392390394444118606479762653184.00000000\n",
      "Iteration 12, loss = 9924573124589776180329650869306219980487956230210483869909129513172868400893223447332328876485835998887936.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 13, loss = 9924143907790537217222674908815133708260138598743541762070378255388327089919398572951543361501177160138752.00000000\n",
      "Iteration 14, loss = 9924009866140211846560481301826354055340321710588905301947302601158042398918081523060658069876097478033408.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15, loss = 9923886372687244307120912273443158084872384506560543256685190990155749164750812102311452393035332621172736.00000000\n",
      "Iteration 16, loss = 9923762896122103529596956018771194102028048733887986370524007943937309622787510574069815238443755903123456.00000000\n",
      "Iteration 17, loss = 9923639421115645644156811151701429243475293667463503658080976670490626940737599693730689957995941722062848.00000000\n",
      "Iteration 18, loss = 9923515947645538625391922701270102539132087294507487871356337510699977110854807379104771091382001492557824.00000000\n",
      "Iteration 19, loss = 9923392475711735621474834974297229814747596199322234826577036148031595501908798479422164081857692996468736.00000000\n",
      "Iteration 20, loss = 9923269005314216262045784625921948385864936288126134009059135923122975752495079450869871296055954399821824.00000000\n",
      "Iteration 21, loss = 9923145536452960176745008311283395568027223467137574904118700176611611501209156749634895100609723868643328.00000000\n",
      "Iteration 22, loss = 9923022069127944958176766351034622408331885233196785945603398583198745750506087477522938098815233385562112.00000000\n",
      "Iteration 23, loss = 9922898603339150235981295400314766222322037492522156618829294483521872138981378090721002657305421116604416.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 24, loss = 9922812773516913505317852504276991963477077104379321549489324677080337779450909224057782883392399389753344.00000000\n",
      "Iteration 25, loss = 9922785968684312498913998387967591382570196965952148088055036067341744424856168663256409670041152140607488.00000000\n",
      "Iteration 26, loss = 9922761272933926608182263970853104491362771092105403292297541947277725253565774629404994558295142830178304.00000000\n",
      "Iteration 27, loss = 9922736580314829642516398541342660675018784730485593630447058047030317822073499087542485601223422114791424.00000000\n",
      "Iteration 28, loss = 9922711887761655235116669750556036298043102664913731588752391175725997167803230033302917746637208505286656.00000000\n",
      "Iteration 29, loss = 9922687195269925980907094398075613316812601082191826039684263605485865054047286536589411180456310894231552.00000000\n",
      "Iteration 30, loss = 9922662502839643916923648818387477999772968391698038034711069002246172116946117951783265666017435465023488.00000000\n",
      "Iteration 31, loss = 9922637810470809043166333011491630346924204593432367573832807366006918356499724278884481203320582217662464.00000000\n",
      "Iteration 32, loss = 9922613118163423396671123311874156626711998096772975708517872362704354408848554872274357555702457335545856.00000000\n",
      "Iteration 33, loss = 9922588425917484940402043385048970570690660492341701387297870326402229637852160377571594959826354635276288.00000000\n",
      "Iteration 34, loss = 9922563733732989600287140562043899641968814961382222507236013925228042771229642086013593889018861750059008.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 35, loss = 9922546568309991305550501587333104698448687781468722315928570617605823557499021312673164262747899267383296.00000000\n",
      "Iteration 36, loss = 9922541207483360883279744193970754806716586662112807322933612468985290686886676498387797194368723502235648.00000000\n",
      "Iteration 37, loss = 9922536268450879347604015590924831275932148797327166372386375745598210477401286898795291884063441750589440.00000000\n",
      "Iteration 38, loss = 9922531330034813046511007815528084484243142401588343963636025995398628912452330498036371314396080778510336.00000000\n",
      "Iteration 39, loss = 9922526391622095632563093935257163017265881023546290168924863044395093162402112700134261670273685311717376.00000000\n",
      "Iteration 40, loss = 9922521453211832846966663110720195027343152946188303393628067546573573961593366931698366846882240838762496.00000000\n",
      "Iteration 41, loss = 9922516514804036911937573348833698125149088625783349946556001497551575126868789319016485424241984460029952.00000000\n",
      "Iteration 42, loss = 9922511576398691531188013973708982163118180787306141415961515569839091569104785027038219295659266708340736.00000000\n",
      "Iteration 43, loss = 9922506637995806889897866657776478483478871477647483059186578093117376469003600827670067277817618500681728.00000000\n",
      "Iteration 44, loss = 9922501699595372802887249728605755744002718649916569618889220737705176645862989949005530554033508920066048.00000000\n",
      "Iteration 45, loss = 9922496761197407603479950196571590360700917988516850558284986497028747824946996580476306994337293617070080.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 46, loss = 9922493328134464488811424857006985725488648015515553101103917870159317045774138595158525006941219291398144.00000000\n",
      "Iteration 47, loss = 9922492255974735364405850012305146928515701399079675117063362766877458331150315730360681337199293640998912.00000000\n",
      "Iteration 48, loss = 9922491268172937684453717417302549019183698897788828243950745270755759610813718626348214387584719915057152.00000000\n",
      "Iteration 49, loss = 9922490280494038459025797036856698975124810999710538561964781040507440516067570054913069068127602919604224.00000000\n",
      "Iteration 50, loss = 9922489292815412196418705477546408902788169958305829776743568045716706664141634970572092035789114499399680.00000000\n",
      "Iteration 51, loss = 9922488305136883711538477973568259715844572567052851462005251015866003346957268896533503643612522882269184.00000000\n",
      "Iteration 52, loss = 9922487317458448930313161855950078877402642007195281514813042619082829292233573124034704364924415701417984.00000000\n",
      "Iteration 53, loss = 9922486329780120074958615131608383998136508735002086243977304850984688316813243779363492779745030057230336.00000000\n",
      "Iteration 54, loss = 9922485342101888997330932462598830004263419112960621443624463047826577876134483444994669834727541216116736.00000000\n",
      "Iteration 55, loss = 9922484354423753660394137514435330627337684731692726062286123543672247334056842766546935766535242994679808.00000000\n",
      "Iteration 56, loss = 9922483366745716101184206621603972135804994000576561151430680004457947326720771098401590338504841576316928.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 57, loss = 9922482680133991588911662642688846283423563264192219692887059369241581021664815629885393548456389708349440.00000000\n",
      "Iteration 58, loss = 9922482465702270652802335001012402560432974336254024178189609067947279508645659780621318686880367225339904.00000000\n",
      "Iteration 59, loss = 9922482268142096487085754920245733407124219089408510487190909168921747653359231608517103760597715169312768.00000000\n",
      "Iteration 60, loss = 9922482070606495086351697745137720514154746171319760659425001458887639560313365338031933965001753435832320.00000000\n",
      "Iteration 61, loss = 9922481873070916093013380249376656574087845756390782397811424074152288464812441965741061566109559719723008.00000000\n",
      "Iteration 62, loss = 9922481675535359507070802432962541586923517844621575702350177014715694366856461491644486563921134020984832.00000000\n",
      "Iteration 63, loss = 9922481477999796810020295613090167794422124704717885852483748957470348360479132954404012271722589772054528.00000000\n",
      "Iteration 64, loss = 9922481280464236150005765127703880270366419974192357054085714566161252990242253771544837742860751706521600.00000000\n",
      "Iteration 65, loss = 9922481082928691786279045318206282893876222518692116667434829502342162709128969423736061320692563108167680.00000000\n",
      "Iteration 66, loss = 9922480885393143348480372839736512980494648244435554177847157106650571155734786367164685371850962143019008.00000000\n",
      "Iteration 67, loss = 9922480687857598984753653030238915604004450788935313791196272042831480874621502019355908949682773544665088.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 68, loss = 9922480550535289934132327721411008758172280646714080005331276436266218809682219562763545426399111998865408.00000000\n",
      "Iteration 69, loss = 9922480507648952265425586463431196072600365771136556267090646107003360542727826326930889696761367289135104.00000000\n",
      "Iteration 70, loss = 9922480468136925987833371052119424569410506041155729945058159524130506843460427980911505717519002848198656.00000000\n",
      "Iteration 71, loss = 9922480428629813041016074421247732557221089731299359764791195179494187514956872402587150906411952761733120.00000000\n",
      "Iteration 72, loss = 9922480389122712316414635797292558155705803877711955893334592830475372003296012950550594675325139775651840.00000000\n",
      "Iteration 73, loss = 9922480349615601406633315500906952411962075977233746764536022151775303310932906726607539627554795872583680.00000000\n",
      "Iteration 74, loss = 9922480310108494570923947873493519205109724895511859738674238804947735890850699211427084106457864336310272.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 75, loss = 9922480270601389772250556580566172266703062223168133764280849124056419106908941050627928348697638983434240.00000000\n",
      "Iteration 76, loss = 9922480231094278862469236284180566522959334322689924635482278445356350414545834826684873300927295080366080.00000000\n",
      "Iteration 77, loss = 9922480191587180174903773994711478389889736878480681815494069762273785539025424729029616833177188277682176.00000000\n",
      "Iteration 78, loss = 9922480152080069265122453698325872646146008978002472686695499083573716846662318505086561785406844374614016.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 79, loss = 9922480124615605417962212302074205008533886540180064878054106296324413797534012659386789317413405882056704.00000000\n",
      "Iteration 80, loss = 9922480116038349699029526790497542828404496339457894228922663492902095833757740267631796798838752803815424.00000000\n",
      "Iteration 81, loss = 9922480108135935480552787836496408946605495392197820338055234046208022294886283439150201044308772708679680.00000000\n",
      "Iteration 82, loss = 9922480100234517372668476446191460334748126630858500615232307242340509828694560903124189561428116294860800.00000000\n",
      "Iteration 83, loss = 9922480092333099264784165055886511722890757869519180892409380438472997362502838367098178078547459881041920.00000000\n",
      "Iteration 84, loss = 9922480084431679119863877331095476842587700698801700118118059968669234260170666476690866832330097283825664.00000000\n",
      "Iteration 85, loss = 9922480076530256937907613271818355693838955118706058292358345832929220521698045231902255822776028503212032.00000000\n",
      "Iteration 86, loss = 9922480068628834755951349212541234545090209538610416466598631697189206783225423987113644813221959722598400.00000000\n",
      "Iteration 87, loss = 9922480060727410536959108818778027127895775549136613589370523895512942408612353387943734040331184758587392.00000000\n",
      "Iteration 88, loss = 9922480052825988355002844759500905979147029969040971763610809759772928670139732143155123030777115977973760.00000000\n",
      "Iteration 89, loss = 9922480044924564136010604365737698561952595979567168886382701958096664295526661543985212257886341013962752.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 90, loss = 9922480039431672181392946620281799541808446855753951745241780867021303940157180116597777669622335788810240.00000000\n",
      "Iteration 91, loss = 9922480037716219000570433183480380837336880406231356563947098640400589711261476283865479402570698989764608.00000000\n",
      "Iteration 92, loss = 9922480036135735749467890125782936807287942534903709575479934017874524876259095047292900298997361734057984.00000000\n",
      "Iteration 93, loss = 9922480034555450090855051513235860816470780373257684579446954991164771746880301185706398239084524267896832.00000000\n",
      "Iteration 94, loss = 9922480032975170543350141904147043630990683439746142737819156962263770525922855387263795469181805351927808.00000000\n",
      "Iteration 95, loss = 9922480031394884884737303291599967640173521278100117741786177935554017396544061525677293409268967885766656.00000000\n",
      "Iteration 96, loss = 9922480029814603300196417348025064186247735935210414848689986240716765539446166372853390876029542786400256.00000000\n",
      "Iteration 97, loss = 9922480028234319678619555069964074463876262182942550904125400879943263046207821865648188579453411503636480.00000000\n",
      "Iteration 98, loss = 9922480026654031982970740122930912204613411611918364856624028187297259280688578649680386756203867854077952.00000000\n",
      "Iteration 99, loss = 9922480025073750398429854179356008750687626269028661963527836492460007423590683496856484222964442754711552.00000000\n",
      "Iteration 100, loss = 9922480023493464739817015566808932759870464107382636967494857465750254294211889635269982163051605288550400.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 15876544169537803091987113879219519642268485654249629356147770786405282735438615926299294164971916623872000.00000000\n",
      "Iteration 2, loss = 18209930517773439898201206453970922491636568359462690563924315492097159912079361026837945439774427236532224.00000000\n",
      "Iteration 3, loss = 18207847072639076439292896218749836307159601727782594709516580604521257031721492484104787150191619534225408.00000000\n",
      "Iteration 4, loss = 18205746548020335121837542099309353209866919249855942961504264898570674017107994957900431155045863052541952.00000000\n",
      "Iteration 5, loss = 18203646265631166006646588014133934099879092195662061008543340405751558219004645233323089464813660911173632.00000000\n",
      "Iteration 6, loss = 18201546225538357392276112758534062505981998846967343344856932177913516774324425410048102600080648159887360.00000000\n",
      "Iteration 7, loss = 18199446427713957071058854514433962816439285716645841721147156237825320563821294667880118054564576219889664.00000000\n",
      "Iteration 8, loss = 18197346872130018946435480467216118224851664545706091042521309606064492376670560249767682611993315062579200.00000000\n",
      "Iteration 9, loss = 18195247558758596921846657802263011924819847075156626214086689303208555002047529398659343056094734659354624.00000000\n",
      "Iteration 10, loss = 18193148487571742863697077370471040841498856636627821089482198683898780592987060003122346407259998798217216.00000000\n",
      "Iteration 11, loss = 18191049658541512712463382691708774436935093380506371625283528434648942574804908660486539212553683633963008.00000000\n",
      "Iteration 12, loss = 18188951071639980741946004296219558589186153141582423241281912235399070098080876157513465889070720971964416.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 13, loss = 18187574452461380494228571640784847587851918361558624336624892956144515428309184180573404856669183257083904.00000000\n",
      "Iteration 14, loss = 18187144170114761556200248078728818403752053824631177944562410284563068014304616820829751473763102387863552.00000000\n",
      "Iteration 15, loss = 18186724524869707119155084402686307293148258951213153702488008780965766031554475225256667156659150182678528.00000000\n",
      "Iteration 16, loss = 18186304889365569276595634083973480499069088371774273532363503495839178516555195047690980978210529173569536.00000000\n",
      "Iteration 17, loss = 18185885263543986947799914096218747052541613402000412864710365355602580045432773263894473341785085025714176.00000000\n",
      "Iteration 18, loss = 18185465647404729947702598642494358619203043782159372883600110109459648734316432828780270990335019015208960.00000000\n",
      "Iteration 19, loss = 18185046040947580313454219932789084475364719708787921081914615502231566516178092823549299246832769518534656.00000000\n",
      "Iteration 20, loss = 18184626444172305822953475835689090018218162512775697592257003617185760871006526848733385090893831628390400.00000000\n",
      "Iteration 21, loss = 18184206857078702772602732902585748403194531256306597267787907860697168184756799466202552188847587005038592.00000000\n",
      "Iteration 22, loss = 18183787279666555236588499677953915175050854545295548652857599643523221026541279111539025627003180208357376.00000000\n",
      "Iteration 23, loss = 18183367711935602474305805347574547972739015979337937159511689725823837970382448423936435698262219763482624.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 24, loss = 18183092466724049336318869878282397250029895639006402785808843888603255230087226578145770253483873203650560.00000000\n",
      "Iteration 25, loss = 18183006430744558286884474171309727144307189735032530485931574488122680132640940308814046241888969519267840.00000000\n",
      "Iteration 26, loss = 18182922520073630570586618608393595421293115794269292885230463327176461567145917636992899578896382016094208.00000000\n",
      "Iteration 27, loss = 18182838609801566720670913428085342974743728553708899104819504479722303118477178949953212822994740819525632.00000000\n",
      "Iteration 28, loss = 18182754699916698595064914694082824147755819095244846333739779462916560974140822351609941581531027429392384.00000000\n",
      "Iteration 29, loss = 18182670790419079156704007103024281919917286062709321910169523591101751673788531055876879701259602614026240.00000000\n",
      "Iteration 30, loss = 18182586881308673775976592968646249727651426496673587959146044543361614403032666038271931205456461255671808.00000000\n",
      "Iteration 31, loss = 18182502972585488563990601294406986376295305625272127635074523317504901070294575361938995384131721904521216.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 32, loss = 18182419064249533705925913752736923208077365495395746195296928243212864856276505798784571053968915477561344.00000000\n",
      "Iteration 33, loss = 18182335156300805127710577674663887686106229288288121536876471988613004488697558640046058688294629607997440.00000000\n",
      "Iteration 34, loss = 18182251248739278384912877046354844589033636091411321042192430562470312333872341633147861127068390095060992.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 35, loss = 18182196202841391082949568439487499203464977524078776959267550838599659232000762935723248235201138687737856.00000000\n",
      "Iteration 36, loss = 18182178996464963927838384367793553124527891446180753513506391789026954609320482317413497103065852147662848.00000000\n",
      "Iteration 37, loss = 18182162215065802890733976403513004545052107943631571754440006800529249709018198929195392459494172395569152.00000000\n",
      "Iteration 38, loss = 18182145433684455733242613520056873523121464453100785086476230424543357856945519605443718195418066453528576.00000000\n",
      "Iteration 39, loss = 18182128652318602271387250737772900299096862692862955887114677159679804489130629705858043870329191431929856.00000000\n",
      "Iteration 40, loss = 18182111870968232319988006384230653530749860616027278899013378676257336424871282458531870667544016413786112.00000000\n",
      "Iteration 41, loss = 18182095089633339767936951455971874412743392994459270967767153976467201755746129800321299297052422848905216.00000000\n",
      "Iteration 42, loss = 18182078308313957207809707304773943240208474378209508916870301715289410660002361401327125972241709671645184.00000000\n",
      "Iteration 43, loss = 18182061527010031676670889233998617033557206123445805408144586578381446597988294047635556846357516113674240.00000000\n",
      "Iteration 44, loss = 18182044745721632433743692616172828919942994149025636191515393207575831198479205788210783872847852410503168.00000000\n",
      "Iteration 45, loss = 18182027964448698367948827416713990845995186173604169722930911624785045377261615991613814151611533060210688.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 46, loss = 18182016955394902175394086474606556738492806015323738135543304978866194648441675105121638277502130253201408.00000000\n",
      "Iteration 47, loss = 18182013514152387764344728338629219769562243511847865049321013284977387696998652526057760706669468968812544.00000000\n",
      "Iteration 48, loss = 18182010157901956504577477650353439783977037048212116751127195478370528259271817504589883969302819229401088.00000000\n",
      "Iteration 49, loss = 18182006801652613022021589577647727148389441192514369937055595281721508520544937722736080853737271424188416.00000000\n",
      "Iteration 50, loss = 18182003445403878613222625516281808778062679740886777512033701200011428987812414900890906975846872454791168.00000000\n",
      "Iteration 51, loss = 18182000089155769574468396142144374820562259968354627887808662560730294750197843874104760442325271788388352.00000000\n",
      "Iteration 52, loss = 18181996732908279794650972451777166470551116646783437909975298366069353899279876579233741963162350874787840.00000000\n",
      "Iteration 53, loss = 18181993376661419458950236117610615070257691823064012835875576945709859615760759788184350355041640630976512.00000000\n",
      "Iteration 54, loss = 18181990020415162085898494791325599129888036175280258996420380642480553629814651893999688694585960672788480.00000000\n",
      "Iteration 55, loss = 18181986664169536193999417155727326407681787434726431112167220779488944847407843858017953668509197567787008.00000000\n",
      "Iteration 56, loss = 18181983307924521412893239865954934219182061507620918668432160697372526907135842136426147643444289481998336.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 57, loss = 18181981106118787542135908854708271653194891411888146323481813232919076119860411217794228620276563361923072.00000000\n",
      "Iteration 58, loss = 18181980417871604251831506707599488958525730506365700847462771687644476821354898470187439795628296709865472.00000000\n",
      "Iteration 59, loss = 18181979746622689702971644166341154571368662287956787992444044772854470841796000106017488319428363452153856.00000000\n",
      "Iteration 60, loss = 18181979075373901450342314363220168827844275450993860328465725146112004302944961713488122170104213565079552.00000000\n",
      "Iteration 61, loss = 18181978404125137642144700573932218305668149526568865282108129510604545397779315573534353180820537878773760.00000000\n",
      "Iteration 62, loss = 18181977732876390130234897460532957931057530877169158647497683202587091581737264268630982298230511659646976.00000000\n",
      "Iteration 63, loss = 18181977061627671136828763029938905314686549959063706733444748217677146671661503925065808102354372008083456.00000000\n",
      "Iteration 64, loss = 18181976390378982698962273616636146725000895181630670591417718221810961303692483897220130356528825107480576.00000000\n",
      "Iteration 65, loss = 18181975719130314631455547548194250819772124497979244964074624885307282297127957413187450244070340040851456.00000000\n",
      "Iteration 66, loss = 18181975047881671008380537493585390135891614726865751954352255540038610924248823181730367291652329174990848.00000000\n",
      "Iteration 67, loss = 18181974376633051829737243452809564673359365868290191562250610186004947185055081202848881499274792509898752.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 68, loss = 18181973936272090018452428421896865335030439420680660566590685560125814789152796396944516205614168738365440.00000000\n",
      "Iteration 69, loss = 18181973798622720582578767030515955654804324749055486169843868226967165922086522542006050630795819460067328.00000000\n",
      "Iteration 70, loss = 18181973664372984932041445482341490205312882202947039992906855893730179484633167890818214844967416263344128.00000000\n",
      "Iteration 71, loss = 18181973530123273725935839948000059977169700569376526433590567551728200680865205492205976219179487267389440.00000000\n",
      "Iteration 72, loss = 18181973395873562519830234413658629749026518935806012874274279209726221877097243093593737593391558271434752.00000000\n",
      "Iteration 73, loss = 18181973261623855387796581548289372057774714120991821417894778199596744345610179403744098494277041642274816.00000000\n",
      "Iteration 74, loss = 18181973127374142144654999679461855561185844078043146807110096191658514905701767650750560105152406462922752.00000000\n",
      "Iteration 75, loss = 18181972993124447234837204821009115480608169719497921659540957177146541191057400087188719586058126934147072.00000000\n",
      "Iteration 76, loss = 18181972858874729917623670283209426447127922857792924945819487837335810478868089625432581670260079388000256.00000000\n",
      "Iteration 77, loss = 18181972724625022785590017417840168755876118042978733489439986827206332947381025935582942571145562758840320.00000000\n",
      "Iteration 78, loss = 18181972590375325838736246224901342406852755275055347290402454146758108596596209017639802288714577046667264.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 79, loss = 18181972502303136328329650086999323315010933758662866563326220353893033008012381152592748898653840949116928.00000000\n",
      "Iteration 80, loss = 18181972474773271404113213680461920960126739825601740310437789017380806033617103540882774742371678300405760.00000000\n",
      "Iteration 81, loss = 18181972447923322236969773036340941601782762907001890023581992884797158109985983256263907821869291477663744.00000000\n",
      "Iteration 82, loss = 18181972421073373069826332392219962243438785988402039736726196752213510186354862971645040901366904654921728.00000000\n",
      "Iteration 83, loss = 18181972394223436124898749755015500495768939526071155758680762615247366079566438813313972560884754932563968.00000000\n",
      "Iteration 84, loss = 18181972367373486957755309110894521137424962607471305471824966482663718155935318528695105640382368109821952.00000000\n",
      "Iteration 85, loss = 18181972340523545938755773804717886852863739326384099390842745013825072776865995661601437773226806020669440.00000000\n",
      "Iteration 86, loss = 18181972313673598808648309495082993762965450817162410155455342547177675489375324731363870616061125381324800.00000000\n",
      "Iteration 87, loss = 18181972286823657789648774188906359478404227536075204074473121078339030110306001864270202748905563292172288.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 88, loss = 18181972259973712696577286213757552656951627436231675890554112277627883458955780288413935355076588836225024.00000000\n",
      "Iteration 89, loss = 18181972233123767603505798238608745835499027336388147706635103476916736807605558712557667961247614380277760.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 90, loss = 18181972215509328071795697943439473002374112305607122720045141785594721180976433656043217472566102214049792.00000000\n",
      "Iteration 91, loss = 18181972210003358346209972797309730560910374973999955151816885383790276803922097100711302262648399577743360.00000000\n",
      "Iteration 92, loss = 18181972204633366883152503600896665674485028862777456253271011224524546710283513560282489067878557266477056.00000000\n",
      "Iteration 93, loss = 18181972199263373383059058069997514519613994342176796303256743399322565980504480665472376109772008771813376.00000000\n",
      "Iteration 94, loss = 18181972193893381920001588873584449633188648230954297404710869240056835886865897125043562915002166460547072.00000000\n",
      "Iteration 95, loss = 18181972188523382308800214339227039672980548482219154300291420417046103248665516167089550666885499415691264.00000000\n",
      "Iteration 96, loss = 18181972183153390845742745142813974786555202370996655401745546257780373155026932626660737472115657104424960.00000000\n",
      "Iteration 97, loss = 18181972177783405493793204949859168705466921487908639657604853096323394969809697149375823567355933343350784.00000000\n",
      "Iteration 98, loss = 18181972172413414030735735753446103819041575376686140759058978937057664876171113608947010372586091032084480.00000000\n",
      "Iteration 99, loss = 18181972167043416456570337553574780127279164037329158706107923779983182874111182005374297887806130170626048.00000000\n",
      "Iteration 100, loss = 18181972161673420919440915688189542703962441107350337704625262288844951508191699756182885166362875492564992.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 22381530472650963079467391045338189326241059272521787534124506785220946387999780283007887152667723487510528.00000000\n",
      "Iteration 2, loss = 28341229952258206453972966804626302933829125061770259370914679188194256333773698973963559949582008735236096.00000000\n",
      "Iteration 3, loss = 28343633624826751522986313898427555832741993596520994487898203779722247953071913188093531491757806155988992.00000000\n",
      "Iteration 4, loss = 28340972605803633831341686616645046815534605025230338951263521851349276461853739133252218605471008817152000.00000000\n",
      "Iteration 5, loss = 28338308663110604075892864746885818991756823826802670994235052560433844571625550323451727832904348699983872.00000000\n",
      "Iteration 6, loss = 28335644968829532377559821172841611403595165086495529943818548009904310436434012370628280398487498308714496.00000000\n",
      "Iteration 7, loss = 28332981524924479935860835351226388941117429402118876389945255950154025410175835218862491234241540991746048.00000000\n",
      "Iteration 8, loss = 28330318331373080095775754624812924070664881801464365209652724401151004670717107761482958903132583045890048.00000000\n",
      "Iteration 9, loss = 28327655388151866202857205713887404297907046248116683487045921777287919880081267525916409766305391733374976.00000000\n",
      "Iteration 10, loss = 28324992695237269750858998614431703706229026236752465732810133196144910893269284320524582018069425146560512.00000000\n",
      "Iteration 11, loss = 28322330252605787418686186025982456969277954362149500104619241085262137921776507293870806279508739246522368.00000000\n",
      "Iteration 12, loss = 28319668060233891440812104634243263539352702306547642142526403880944753543412893341942816011666915793567744.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 13, loss = 28317630374474601598722574506619597487182464180399669048765690485108939455378805076964761213978634740563968.00000000\n",
      "Iteration 14, loss = 28317008950947425999160987957773812076689975532444087386383795393243270177409699190431473157854379139661824.00000000\n",
      "Iteration 15, loss = 28316476539644712803208827782198456196713011088535436479812581327584159303868341669985304891475428716314624.00000000\n",
      "Iteration 16, loss = 28315944194116856468821245413299387225065155500872885858770466499350700380631684293196999092711638398140416.00000000\n",
      "Iteration 17, loss = 28315411858631974534598183002060146501956332029044674197496213914641034739397504056688898289406171989147648.00000000\n",
      "Iteration 18, loss = 28314879533154948500307634008353466023718362993554274180883022832494195318818931426853082556744427718574080.00000000\n",
      "Iteration 19, loss = 28314347217685546143848296300765511187542769725291325941534015336177609598884740004221378874341900679118848.00000000\n",
      "Iteration 20, loss = 28313814912223628946773779134242415739122740386540877979598422142026234322044373690865403335302570399760384.00000000\n",
      "Iteration 21, loss = 28313282616768919872191301018676447169844651301873027295374704682709982973196720290928388125834395938455552.00000000\n",
      "Iteration 22, loss = 28312750331321321142373997898735464594315458821136043418379966993288825017600210794107944605775480492130304.00000000\n",
      "Iteration 23, loss = 28312218055880612757436425649922151020400814731488532790027693152647691752086314927223698334761556253868032.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 24, loss = 28311810617596379826295026802263183556878298619272191978349585434679131852880907058751184659307386443399168.00000000\n",
      "Iteration 25, loss = 28311686357892527761912568884759526330685738570630412331390726580600174672754937791407877555460889798246400.00000000\n",
      "Iteration 26, loss = 28311579894964654315290494486150730583760129144585815469381694356520699413003965198276970085848776649474048.00000000\n",
      "Iteration 27, loss = 28311473443587878927984129488142779387678811774666334295327887610995985289215884649468653468791278023999488.00000000\n",
      "Iteration 28, loss = 28311366992618335550850696941585098242950166830435296373725412845080944547220035883821224546013727101550592.00000000\n",
      "Iteration 29, loss = 28311260542049057520851132904062649065319834238581905682667932556798401586679626917289492705980976663101440.00000000\n",
      "Iteration 30, loss = 28311154091880036689841532037631086781005060361593518016281872082403353863032860332348258895346201975062528.00000000\n",
      "Iteration 31, loss = 28311047642111264909677989004346066316223091561957489168693656758150798831717938711472324060762578303844352.00000000\n",
      "Iteration 32, loss = 28310941192742729958144645797291070060299797383404852831092924588423232675892165928373889622209868549062656.00000000\n",
      "Iteration 33, loss = 28310834743774448131529313092354788160800685100960897415226824900710660484679136818103353686381722177896448.00000000\n",
      "Iteration 34, loss = 28310728295206411281688085551592875543943001077112978715221783031268079713517053963135517199931314456756224.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 35, loss = 28310646811503689451327079143483011286897507080639821516500123931513190647077311084017463369269737022291968.00000000\n",
      "Iteration 36, loss = 28310621960562862517771222225187472490698492077613468074540516812907668852655914889986708292170473690628096.00000000\n",
      "Iteration 37, loss = 28310600668750510833230194776858096999989454137284051598530571248355241519208734875639813577189239877009408.00000000\n",
      "Iteration 38, loss = 28310579379184235756915098376970035094234364744893205202744560312479235368028682770037969181179943678115840.00000000\n",
      "Iteration 39, loss = 28310558089635377338197661833119568399115175535779348861724393131546168217690610624549101314008515526787072.00000000\n",
      "Iteration 40, loss = 28310536800102513725966403674018481539541376763986068650531290882053096042160869081025975166654039411654656.00000000\n",
      "Iteration 41, loss = 28310515510585653068365229237611119589295722067026008775038828227745021386001255556993789792463340066308096.00000000\n",
      "Iteration 42, loss = 28310494221084823883897807206702690306617849176193423955804516491729453155178061013790741878150304058310656.00000000\n",
      "Iteration 43, loss = 28310472931600001728132421567460158470159497178950381575207631682771383716005893198841234263674457186893824.00000000\n",
      "Iteration 44, loss = 28310451642131170304781261643994833932355158800271593221501024473380807979361157277094868842342149984878592.00000000\n",
      "Iteration 45, loss = 28310430352678362206419948788084096988335848590207635718178993518537736123491042918652441827540681386622976.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 46, loss = 28310414056095971683921575938611137245418303183834511443344170056337147745778609023437416712607079925284864.00000000\n",
      "Iteration 47, loss = 28310409085947810424528051726508603729985758915307233911854417755080536167494930866348538008757939289980928.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 48, loss = 28310404827616253330782307863893504056232976686462203280407345424335204513808717212331847558280049292476416.00000000\n",
      "Iteration 49, loss = 28310400569731373411855237441303176616143226082016216535386552262756864465576033580964662054060642304786432.00000000\n",
      "Iteration 50, loss = 28310396311847402010973612199507324902830506060230058745269334108746308135983762003657170998012193112326144.00000000\n",
      "Iteration 51, loss = 28310392053964062091244650648398216407681192945673826910354152394973449009930790284552606576342660773052416.00000000\n",
      "Iteration 52, loss = 28310387796081374023028116132836713815152170832129131545324943780800793448821611967463966422419107120939008.00000000\n",
      "Iteration 53, loss = 28310383538199325584108150645906299514569309263327006341371346270610837635813530926103451956221295055601664.00000000\n",
      "Iteration 54, loss = 28310379280317904552268896180690455895258477782998484989682997868786077754063851034183264597728987476656128.00000000\n",
      "Iteration 55, loss = 28310375022437151668229879426910908326133444578706788519627771894051526526381559379329399613676308052049920.00000000\n",
      "Iteration 56, loss = 28310370764557005820911810349985068753823557369107085387153858368319664868553175330102864103962071279861760.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 57, loss = 28310367505246871861256831903557551252492030455177239225352126867738528388826157836179842016813088560381952.00000000\n",
      "Iteration 58, loss = 28310366511218818719666981554751119848503176551422230817352946241268699209245761721145642814657893802967040.00000000\n",
      "Iteration 59, loss = 28310365659553762929893645359451675783674955646351696816181387458224524995481501030975478845308007249608704.00000000\n",
      "Iteration 60, loss = 28310364807977921167739854316786444566216311867222572924778822019213211188837164824209749970342128767729664.00000000\n",
      "Iteration 61, loss = 28310363956402148664809258646648146475911074006950924783301641222034419010968106666408213048824260521361408.00000000\n",
      "Iteration 62, loss = 28310363104826429124814047673148091365193734790511463980002695739198143372750731722520469974060753043324928.00000000\n",
      "Iteration 63, loss = 28310362253250697362602978692731518643802265117803036867893388260744363917690660652344928319277008464904192.00000000\n",
      "Iteration 64, loss = 28310361401675030785543152415869706512672824545195763402772678092250604819124968922370979091267861755199488.00000000\n",
      "Iteration 65, loss = 28310360550099376430699184145924411992217514428857456246462329919374349537401973318684828443278952145879040.00000000\n",
      "Iteration 66, loss = 28310359698523709853639357869062599861088073856250182781341619750880590438836281588710879215269805436174336.00000000\n",
      "Iteration 67, loss = 28310358846948112535802726964727720857112039202500385066146294224219352969045867907701121940708668961980416.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 68, loss = 28310358195086329373374500879978135062950067581342477589405830370078701755498207192919969218348931952410624.00000000\n",
      "Iteration 69, loss = 28310357996280792893166069385510388953575354901956538181255523684864259075094484469392440763373998076592128.00000000\n",
      "Iteration 70, loss = 28310357825947841216661911113444219179223812274784734083898306973593942807642753479292361058935841321123840.00000000\n",
      "Iteration 71, loss = 28310357655632717679022632263605070841537228525278452437922454536389194041403772034327810077350201659490304.00000000\n",
      "Iteration 72, loss = 28310357485317598215455306082738095040742021594528492894883389431056946547445689298125858622437974364651520.00000000\n",
      "Iteration 73, loss = 28310357315002486900031885239815464313729568301291177557717898989469701598049403979449106220872571803402240.00000000\n",
      "Iteration 74, loss = 28310357144687379658680417065865006123608491826810184323489195879754957920934017369534953345980581608947712.00000000\n",
      "Iteration 75, loss = 28310356974372276491400901560886720470378792171085513192197280101912715516099529468383399997762003781287936.00000000\n",
      "Iteration 76, loss = 28310356804057165175977480717964089743366338877848197855031789660325470566703244149706647596196601220038656.00000000\n",
      "Iteration 77, loss = 28310356633742057934626012544013631553245262403367204620803086550610726889587857539792494721304611025584128.00000000\n",
      "Iteration 78, loss = 28310356463426950693274544370063173363124185928886211386574383440895983212472470929878341846412620831129600.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 79, loss = 28310356333054590801531337017935518174778690149649572208876860804569851951938219819912031680601943535779840.00000000\n",
      "Iteration 80, loss = 28310356293293473727716964313508754864364443248757211280198509871032960362383318374176287125590767080308736.00000000\n",
      "Iteration 81, loss = 28310356259226890725745647463245431475898612997084230246013283726149399398998589851928950332715277989445632.00000000\n",
      "Iteration 82, loss = 28310356225163860314517057956716560256713368700924122972706610974086947864557535370668400799055372743606272.00000000\n",
      "Iteration 83, loss = 28310356191100858421792137132992896795767762136058270419957449545132005236082771850746047952109354065330176.00000000\n",
      "Iteration 84, loss = 28310356157037819862419642288519680502799764202385518940777202129324551157079919951960299365102624085901312.00000000\n",
      "Iteration 85, loss = 28310356122974797599334958119935154357397273543738055873344104041007102167200662888224948884789543573651456.00000000\n",
      "Iteration 86, loss = 28310356088911807928825895303128008507125797435141169629405304607669663355568595494590394617863761995759616.00000000\n",
      "Iteration 87, loss = 28310356054848765295381447789682619677266422682712096047288269859989708004284844887042046504183619649536000.00000000\n",
      "Iteration 88, loss = 28310356020785751180440668959042438605646685661577277185728746435417261558967385240831895077217363870875648.00000000\n",
      "Iteration 89, loss = 28310355986722720769212079452513567386461441365417169912422073683354810024526330759571345543557458625036288.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 90, loss = 28310355960648269161223201326948899033249226303351452591566505815452090133823974081391081143762384999940096.00000000\n",
      "Iteration 91, loss = 28310355952696031079801297177763725238357420375650220835258401234003707235701758440698573936735865188384768.00000000\n",
      "Iteration 92, loss = 28310355945882716109035814875299929575420805052818153469596070937775995551937172219754146388830132316930048.00000000\n",
      "Iteration 93, loss = 28310355939070101878646191636049810257801002556073487809061161723618502700487163905976837428751326534172672.00000000\n",
      "Iteration 94, loss = 28310355932257508018616331741660553624638084153110432663210189168823516210441649136012526102039582585389056.00000000\n",
      "Iteration 95, loss = 28310355925444889714154755833438261770126904837609444899738492622793522086710742113472617615287364435836928.00000000\n",
      "Iteration 96, loss = 28310355918632283631909037932132487526289855978377423445077158072381031779822531217220507708555383386669056.00000000\n",
      "Iteration 97, loss = 28310355911819669401519414692882368208670053481632757784542248858223538928372522903443198748476577603911680.00000000\n",
      "Iteration 98, loss = 28310355905007067393345649460548766501724381441157058432817701639683549893765210715953688368418008921538560.00000000\n",
      "Iteration 99, loss = 28310355898194457237027978890270819720995955763168714875219579757398558314596101110938978935012615505575936.00000000\n",
      "Iteration 100, loss = 28310355891381851154782260988965045477158906903936693420558245206986068007707890214686869028280634456408064.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 18842246178465812321359701819482936992970851077650745994042648603232192500544519209690201948226338588459008.00000000\n",
      "Iteration 2, loss = 23524147074323755369168470283592281595454953331419743323839970184562189295733096189957057285062650429964288.00000000\n",
      "Iteration 3, loss = 23524889770839719723399054421359713940645226836813079530569151411968655779985402853388874284218967245455360.00000000\n",
      "Iteration 4, loss = 23523021819652356485585525607499612359276838285430220243356918483597717284057534754174816655932751580495872.00000000\n",
      "Iteration 5, loss = 23521152943550926710513977188981465139146241461915630339255743527587869344840166053969481455249388076007424.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, loss = 23519284215488287628119721506140075479791328007223221227253588590151154505475045394158766551810132884324352.00000000\n",
      "Iteration 7, loss = 23517415635893720125983535487823483088696113456256657253694825744874131731978216381092197996402662801670144.00000000\n",
      "Iteration 8, loss = 23515547204755625321256170570256475436110794809766911357545921150745678840631055167648923349821969563385856.00000000\n",
      "Iteration 9, loss = 23513678922062184331202934065166523000151220855663562919186825045639604944546407633527715732498777097895936.00000000\n",
      "Iteration 10, loss = 23511810787801631236024517980919339238521139025688378657175722981772235696488802872341142111618170101956608.00000000\n",
      "Iteration 11, loss = 23509942801962139004842324291298049555553644470238293746018990533272377665009289346262776554264047770402816.00000000\n",
      "Iteration 12, loss = 23508074964531949866000950342612712482735238258567719110148387916101359047434193566430385080968319533580288.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 13, loss = 23506761915777448494835821057325612826042238042940145050711676233328666178964975509963181434153747563413504.00000000\n",
      "Iteration 14, loss = 23506356603714851754719227059087806766818219923331504925112971356134603862167641695281964099134715240382464.00000000\n",
      "Iteration 15, loss = 23505983069676564954857225057672137117660208577373281028224944737628707740471803824755198634044921921667072.00000000\n",
      "Iteration 16, loss = 23505609554635023818778213261248386055987766960369514900779466492955253884511132553805387023075513844367360.00000000\n",
      "Iteration 17, loss = 23505236045534094774555415907181475692314971693196468152718745467884477693283542176117993775022159901491200.00000000\n",
      "Iteration 18, loss = 23504862542368302269484445896871516444631378367357234437000607625774669221261168114759255040818639120891904.00000000\n",
      "Iteration 19, loss = 23504489045137564822126249850875057575109450607725371694889306329175803022826036194477180286996704166674432.00000000\n",
      "Iteration 20, loss = 23504115553841756136250295031054750440116507032854894735344434290040339657270286443631184186680571668201472.00000000\n",
      "Iteration 21, loss = 23503742068480815100777291402828006986281895361400972014314181530280760040380438230782273839769056123551744.00000000\n",
      "Iteration 22, loss = 23503368589054623567620611566001823643755687849430262546631715425594527276010429001815062972733198895677440.00000000\n",
      "Iteration 23, loss = 23502995115563124499772918154965784896058609034354256891182013329766623552227676834053158212145226849452032.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 24, loss = 23502732558517476188858999118563863498536605568696438403984987506795265403816777265691449560887809410072576.00000000\n",
      "Iteration 25, loss = 23502651509600804998614018248133286862477773693743018736379766684648534818857032956436655446485082210041856.00000000\n",
      "Iteration 26, loss = 23502576814149815502902543754283281386801440517711713510306118022625898651326960182106602149424627630211072.00000000\n",
      "Iteration 27, loss = 23502502121548020079146209611352605821720550938030452920579439730623029585747063114230328849956066226601984.00000000\n",
      "Iteration 28, loss = 23502427429184667864563732402763996898250732709612910912204933940502123304325893795016553165293096219443200.00000000\n",
      "Iteration 29, loss = 23502352737058691452303512857808249950851259318302696515744319701667846469467990528664199107001677508509696.00000000\n",
      "Iteration 30, loss = 23502278045170090842365550976485364979522130764099809731197597014120199081173353315173266675081810093801472.00000000\n",
      "Iteration 31, loss = 23502203353518866034749846758795341984263347047004250558564765877859181139441982154543755869533493975318528.00000000\n",
      "Iteration 32, loss = 23502128662105012955384447535766008428183531348259696894909038961012291371992978338013067163683316786266112.00000000\n",
      "Iteration 33, loss = 23502053970928535678341305976369536848174060486622470843167203595452031051107240574343800084204690893438976.00000000\n",
      "Iteration 34, loss = 23501979279989438277692374749578099781126311280848894506276047113050901449065667572298554157771028663631872.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 35, loss = 23501926770688264406767357428556814380362398931321438284921736502845917625710231614217069653796058768080896.00000000\n",
      "Iteration 36, loss = 23501910561444708035799426046582284328556726400202494549291079457982637277269105417939999317993402331561984.00000000\n",
      "Iteration 37, loss = 23501895622808734122360928591889462804670910453606340870607482455366458422353388997562856814668862676008960.00000000\n",
      "Iteration 38, loss = 23501880684704597709767744102543952154897768594810148766015762752811365660516911864453125311941509140971520.00000000\n",
      "Iteration 39, loss = 23501865746610173884709722442857769454166962651085850062725042234299306016342956421380665398629238044622848.00000000\n",
      "Iteration 40, loss = 23501850808525250795445324826277942784126898047104695408022379642460213331224789812690301687714606313635840.00000000\n",
      "Iteration 41, loss = 23501835870449808071614787907943609460320690689085074287223838317931581243757918494569036545830552114036736.00000000\n",
      "Iteration 42, loss = 23501820932383862009505922363743459630313847852052275112076567588203414843065937302067268079670724913004544.00000000\n",
      "Iteration 43, loss = 23501805994327428905406538869566183441671876811031586294327716780765719218272441070235394395928774177718272.00000000\n",
      "Iteration 44, loss = 23501791056280472092669063404662228062372386197216108907546199908765982918849341420210019754543988607025152.00000000\n",
      "Iteration 45, loss = 23501776118243007867581306644920283639980883285631131363479166299694211033920233187041542262210017668104192.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 46, loss = 23501765616467088456899959161209981049006191069065353657784209678190544885816463062680009583688001926463488.00000000\n",
      "Iteration 47, loss = 23501762374639978727013814309068300144103224972612619102091869323594134703909371424479805844294521858293760.00000000\n",
      "Iteration 48, loss = 23501759386930964083007704839552464463405977801505140511474939670920576359208626122628505171064959512084480.00000000\n",
      "Iteration 49, loss = 23501756399326771236271815355063830460942900411809048381459271765832252529750758573700426278062182428049408.00000000\n",
      "Iteration 50, loss = 23501753411723014315234861450597657905857142629039421265679848371101564834349052862370496739114528591052800.00000000\n",
      "Iteration 51, loss = 23501750424119644431033411098487876355452182628120393928895221504258498005632724483487522234141049599557632.00000000\n",
      "Iteration 52, loss = 23501747436516653435523558960790140735945266771539322165231816501558049499039976019526303709794920719974400.00000000\n",
      "Iteration 53, loss = 23501744448914037254633352368532278510445018240539883871752846031127718042289908761724241639402729585508352.00000000\n",
      "Iteration 54, loss = 23501741461311804036506696659658634752734190672634723254331884756712506179944320127606535076311300929748992.00000000\n",
      "Iteration 55, loss = 23501738473709945632999686496224864389030030430311196107095358014567411367441412699647984967173810019106816.00000000\n",
      "Iteration 56, loss = 23501735486108462044112321878230967419332537513569302430043265804692433604781186477848591311990256853581824.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 57, loss = 23501733385756661271325548695980981535736909365506021167612474867316756877218720209438931725887483091615744.00000000\n",
      "Iteration 58, loss = 23501732737392094065643989675914443594567172721291851452611989023248241765369850980192271674090701631520768.00000000\n",
      "Iteration 59, loss = 23501732139851012247578390190085815488201420206939367954299960834146255290148773370802127760638778084950016.00000000\n",
      "Iteration 60, loss = 23501731542330846714917793207390991782164255187544560933454094478465800705061666548597953788485945662636032.00000000\n",
      "Iteration 61, loss = 23501730944810697478545006900584858223692597443175042324355377450275351209098154561444177923026762707501056.00000000\n",
      "Iteration 62, loss = 23501730347290564538460031269667414812786446973830812127003809749574906802258237409340800164261229219545088.00000000\n",
      "Iteration 63, loss = 23501729749770460116878724321555179160119934235780836650209753371981971301384611218575619092209582299152384.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 64, loss = 23501729152250367917513275380359461118127551953999827482226058990006539617353681154098236600178172479143936.00000000\n",
      "Iteration 65, loss = 23501728554730292014435637115052433223700676947244106725989513935521113022446345924671252214840412126314496.00000000\n",
      "Iteration 66, loss = 23501727957210220185429951518717577866165178759244708072689756212908187699819909404006867356176064140279808.00000000\n",
      "Iteration 67, loss = 23501727359690164652712076598271412656195187846270597831137147817785267466317067718392880604205365621424128.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 68, loss = 23501726939619938942529160037903109196891497235616570975564971582102674106074231853876733067207418973257728.00000000\n",
      "Iteration 69, loss = 23501726809947060538411641187050485425923390548078107117821245467392482025320186903385756986239409035673600.00000000\n",
      "Iteration 70, loss = 23501726690438881656260485844428747144050906777765773765177283282799096435260239502123643848944418100871168.00000000\n",
      "Iteration 71, loss = 23501726570934870549716910860339514102056908595170951716866761603774512388559671165000846498550278397100032.00000000\n",
      "Iteration 72, loss = 23501726451430871665389193883166798670737040868845095977366601920367432158701798954165847728176375793713152.00000000\n",
      "Iteration 73, loss = 23501726331926872781061476905994083239417173142519240237866442236960351928843926743330848957802473190326272.00000000\n",
      "Iteration 74, loss = 23501726212422873896733759928821367808097305416193384498366282553553271698986054532495850187428570586939392.00000000\n",
      "Iteration 75, loss = 23501726092918875012406042951648652376777437689867528758866122870146191469128182321660851417054667983552512.00000000\n",
      "Iteration 76, loss = 23501725973414880202150278643448109482348946782297995122302750518611612511551208819588452173354177746960384.00000000\n",
      "Iteration 77, loss = 23501725853910869095606703659358876440354948599703173073992228839587028464850640482465654822960038043189248.00000000\n",
      "Iteration 78, loss = 23501725734406870211278986682186161009035080873377317334492069156179948234992768271630656052586135439802368.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 79, loss = 23501725650392833217386308708056845390957096388759156169251208572788432107505998516252625598533370843758592.00000000\n",
      "Iteration 80, loss = 23501725624458268943964272411008403740059330143769165285925467879089397253741705910689709057025323483267072.00000000\n",
      "Iteration 81, loss = 23501725600556621760132573869361972980388978297188996727173670912927716573343200045902007754880770669281280.00000000\n",
      "Iteration 82, loss = 23501725576655826057338983142899602431016381570680147682210426308118801799652524312497607527479402515398656.00000000\n",
      "Iteration 83, loss = 23501725552755014058257581740548541734078277569146010225500032375819881936838253744042809193384384894337024.00000000\n",
      "Iteration 84, loss = 23501725528854210207320085676141826110922927205124516974663213107265964618585780593113209912636192006864896.00000000\n",
      "Iteration 85, loss = 23501725504953410430454542280707283024658953659859345826763181170584548572614206150946210158561411486187520.00000000\n",
      "Iteration 86, loss = 23501725481052602505445093547328394864612226477081530472989574570158129982080834291254011351139806231920640.00000000\n",
      "Iteration 87, loss = 23501725457151794580435644813949506704565499294303715119215967969731711391547462431561812543718200977653760.00000000\n",
      "Iteration 88, loss = 23501725433251002951714006756459308692084279386551188177189510696795297890137685406920011842990245190565888.00000000\n",
      "Iteration 89, loss = 23501725409350195026704558023080420532037552203773372823415904096368879299604313547227813035568639936299008.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 90, loss = 23501725392547377850153336022721343319882650941834567543319442383196573020632802695121968080741897336782848.00000000\n",
      "Iteration 91, loss = 23501725387360464995468928763311654989703097692836569366654294244456766049879944174009384772440287864684544.00000000\n",
      "Iteration 92, loss = 23501725382580140447588932257748975882038679506028122178428079649471431440537321451566963944019472142041088.00000000\n",
      "Iteration 93, loss = 23501725377799978862587042511073198250029334069472559107673358329386147722430647079628524182535151091187712.00000000\n",
      "Iteration 94, loss = 23501725373019821351657105433369593154911365451673318139855424341173365276604871416452683947724242407129088.00000000\n",
      "Iteration 95, loss = 23501725368239671988871073693610333133576150471386721377911065016705585375340893170802042766260158456659968.00000000\n",
      "Iteration 96, loss = 23501725363459522626085041953851073112240935491100124615966705692237805474076914925151401584796074506190848.00000000\n",
      "Iteration 97, loss = 23501725358679365115155104876147468017122966873300883648148771704025023028251139261975561349985165822132224.00000000\n",
      "Iteration 98, loss = 23501725353899199456081262460499517848222244617988998474457263052067238037863566181274522061827432404484096.00000000\n",
      "Iteration 99, loss = 23501725349119041945151325382795912753104276000189757506639329063854455592037790518098681827016523720425472.00000000\n",
      "Iteration 100, loss = 23501725344338888508293340974064480194877684201146838641758182407514174418492913563685441118879027403161600.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 23014726006896270196705890981161872451940445404367532822019256949810385803022371780124626614378504191475712.00000000\n",
      "Iteration 2, loss = 26363639282460564141932206133898267522143905526154869316768449202327835868221965697278656089472072270479360.00000000\n",
      "Iteration 3, loss = 26360459928962964066794486884351111419030476375663374213436781604875325787615515747294533143554976399753216.00000000\n",
      "Iteration 4, loss = 26357258588526182043548475866474842070650558475429630827110852719238612749839460096148105576939197223141376.00000000\n",
      "Iteration 5, loss = 26354057636765381337003844827613056249036242318584489412580767709140469497058740109278815859206926219870208.00000000\n",
      "Iteration 6, loss = 26350857073743510432901282056803621462850754509060762446147591336598054041439304877611950621252642735652864.00000000\n",
      "Iteration 7, loss = 26347656899413342689165448828622490067254012023572632684572572535576618102803322425093796664880190885724160.00000000\n",
      "Iteration 8, loss = 26344457113727651463721006417645614417405931838834282884616960240041413400972960775670640791893414785318912.00000000\n",
      "Iteration 9, loss = 26341257716639299744075574815836742680076720944198982067651324685152719645950159546065959390911230619156480.00000000\n",
      "Iteration 10, loss = 26338058708101073110369673304688344821100426772649879299247276800493292374399782886513837843757719602855936.00000000\n",
      "Iteration 11, loss = 26334860088065732698312105151859855584962835844632191027355703524410879489301302694672764372216488751267840.00000000\n",
      "Iteration 12, loss = 26331661856486141495410490349315023138434155149499186273347175092065760700656658587266215364906454249111552.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 13, loss = 26329519233731501788359674083772028276453396017445926892364071479882307365104255751213472354499860692467712.00000000\n",
      "Iteration 14, loss = 26328856962018513976235041963672755177752096562546350835464722975652295680717746453846317638328711710244864.00000000\n",
      "Iteration 15, loss = 26328217432662588463187337801067478234830002225833027561967879705266391358562607801324249809634428358492160.00000000\n",
      "Iteration 16, loss = 26327577918952704432737838356039843714094398755985890108919635597531546966123208783448821393753109371551744.00000000\n",
      "Iteration 17, loss = 26326938420776657869238087465986013090136320785137901494260083551890222615168211369466468278234760855158784.00000000\n",
      "Iteration 18, loss = 26326298938134041365492818233688732673818086437656851424310490381092291077607744683117237795738146129838080.00000000\n",
      "Iteration 19, loss = 26325659471024471958738479775763783997350274750448462223012846889122632759037330100716774438962502716882944.00000000\n",
      "Iteration 20, loss = 26325020019447623723218858574437364109422740223006965655424166526181145876985070921257116074034841272713216.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21, loss = 26324380583403060733235019049687011562658163248405896707308204781910194297394805307140113346900038550290432.00000000\n",
      "Iteration 22, loss = 26323741162890436692670984338878060721289514232357876629038038447147169876390143013544806490318043372060672.00000000\n",
      "Iteration 23, loss = 26323101757909376786907108896570638191310125849281271950429232989621955564128402834312039050334917992906752.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 24, loss = 26322673362520536404476639439210677696622052317769106129327817535175943487815457397914210323472709960335360.00000000\n",
      "Iteration 25, loss = 26322540941468324766714194084375810875966461041467533911886560832024105630763812159711174922581752678449152.00000000\n",
      "Iteration 26, loss = 26322413065131152211410460199900582221347107175951655736232144491141521575299526983719133195519783845494784.00000000\n",
      "Iteration 27, loss = 26322285189437585246901559863354041518872073159176311102493268040519927367089657288149917227449871780282368.00000000\n",
      "Iteration 28, loss = 26322157314365232773735624403675926013534363106395222270086755508892330550314899713756528421297648578854912.00000000\n",
      "Iteration 29, loss = 26322029439914111088200464496754925852899484292633677650759756223748736214098849095589364883756763708391424.00000000\n",
      "Iteration 30, loss = 26321901566084212042152174804646695963184683080379033038638695521344141813879708016123227561480392435302400.00000000\n",
      "Iteration 31, loss = 26321773692875523413374897320434718733715829013362322124913211406061043532814780349070317874448297659203584.00000000\n",
      "Iteration 32, loss = 26321645820288061498156442720007684312058429366608833321330453205389446460027660929481033929354128847273984.00000000\n",
      "Iteration 33, loss = 26321517948321814074280952996449075087538353683849600319080058923711846778675653631067577146177648899129344.00000000\n",
      "Iteration 34, loss = 26321390076976777067676475480786718523264225146328301015225241229155743216477859745067347998245445447974912.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 35, loss = 26321304403064537857009619947585203119047517400840412133060758744621669823986986625767969957767782481264640.00000000\n",
      "Iteration 36, loss = 26321277920185493280631055110864506275699739710379428805211631207287580851256704036374960790069552342892544.00000000\n",
      "Iteration 37, loss = 26321252346099269932281043578047337101844162869017581623991669594520441275793893634955199068463630372569088.00000000\n",
      "Iteration 38, loss = 26321226772042355457558532631039398324553420160636942970019773472527454489116394071676432235386275123691520.00000000\n",
      "Iteration 39, loss = 26321201198010288747675349745311762047769895047064810127513814440919727343640119251492178583450944956006400.00000000\n",
      "Iteration 40, loss = 26321175624003053506343684244975738123928080253275894684726643172207254750241474339352040005963990402334720.00000000\n",
      "Iteration 41, loss = 26321150050020686400211110150780879385050367148077095568089345653242548159448547714119412242986122763829248.00000000\n",
      "Iteration 42, loss = 26321124476063162984845911448894150609788494818930480159981197892790599937575947123218698134476867839721472.00000000\n",
      "Iteration 43, loss = 26321098902130479186176135470343379261251086447079726357465412558978908812342773857887298153762813263216640.00000000\n",
      "Iteration 44, loss = 26321073328222663522705450897933773097677779763819088881099500974914983689715318879463408987557845601878016.00000000\n",
      "Iteration 45, loss = 26321047754339687475930189048860124360828937037854313010325951817491315663727291226608833949148078288142336.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 46, loss = 26321030619763852933618862730772291696647857260627127990713161368988523693182412530973873961821997797212160.00000000\n",
      "Iteration 47, loss = 26321025323241302731351610169050858094190902596431254176589410497981837917912816183053844519132857930612736.00000000\n",
      "Iteration 48, loss = 26321020208471291222272070858266947963286073161454817348283028972459160318647524012843764615685289208184832.00000000\n",
      "Iteration 49, loss = 26321015093703186378866380626459785097545594904437124694393118763267078146842827543530263195394708145700864.00000000\n",
      "Iteration 50, loss = 26321009978936067460873236285918376159518306786449380951205742867220303867015618594765847230069919847546880.00000000\n",
      "Iteration 51, loss = 26321004864169930394220685167670548612312831988735264015784113952446336206884998457787917193037511946928128.00000000\n",
      "Iteration 52, loss = 26320999749404803697412395954521510214168808242589028608685743342052684072417258093934669771011371011407872.00000000\n",
      "Iteration 53, loss = 26320994634640667000088605301610398280629351454229064215226694376676841102207903959393107330624435207012352.00000000\n",
      "Iteration 54, loss = 26320989519877536598537123884825902959259968898680659247154116383808812385380530889213627978570354000920576.00000000\n",
      "Iteration 55, loss = 26320984405115388048326235690334989028712399663405881086847285372213590288249746630820634554808653192364032.00000000\n",
      "Iteration 56, loss = 26320979290354225423527893387109829025878020567161051837242988673763676083096449892976726586012745148137472.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 57, loss = 26320975863447339473716623076330160978454326415815921262634361348109203727142189584709532504925505796964352.00000000\n",
      "Iteration 58, loss = 26320974804144961802523199504020980066909562420035735176924100675975032483910654481470118877251710604017664.00000000\n",
      "Iteration 59, loss = 26320973781192839277506253105624606562409860707207466106296499296842583994464260270491524503674662898630656.00000000\n",
      "Iteration 60, loss = 26320972758240965270878419514530757808284144938514845314812925161932713114152687294031501257175769567723520.00000000\n",
      "Iteration 61, loss = 26320971735289119782754254606242116812398066901116479243886862350130351139807405278909674697390762804379648.00000000\n",
      "Iteration 62, loss = 26320970712337298739061805711786511037860249776256045790581523529562996799147515516363445297646230241804288.00000000\n",
      "Iteration 63, loss = 26320969689385526584232788844996975706018954476471477572517632691465657725858410258968410217982646080765952.00000000\n",
      "Iteration 64, loss = 26320968666433815540483062012790028427548311458031740898505551831455837736782785633012368038420247421648896.00000000\n",
      "Iteration 65, loss = 26320967643482112644877240518527426222860422077104648430367045635191020292268958424581524912204673496121344.00000000\n",
      "Iteration 66, loss = 26320966620530454564062898382958721923977677702497099094533200089523716842845017012539276579396635605336064.00000000\n",
      "Iteration 67, loss = 26320965597578849446183940944028260604682831971721737096877589858198929933072758814410822093342958482882560.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 68, loss = 26320964912197794107905947730673957409616861823202157113962063610995635972072904745002745884325087589433344.00000000\n",
      "Iteration 69, loss = 26320964700337395166219973192888964920865793216664975432031613315771825642307493449091734260250481046585344.00000000\n",
      "Iteration 70, loss = 26320964495747051327841246758858706450415113885474499256054482211020861135580009040395486013668636368044032.00000000\n",
      "Iteration 71, loss = 26320964291156756378325952352494518422660956379359888315318799088739911896223309136850432087167740091039744.00000000\n",
      "Iteration 72, loss = 26320964086566457354738705277158157858015422054488955271646328634586461384585710524542778633993431447240704.00000000\n",
      "Iteration 73, loss = 26320963881976170553367316208738314904044018185886988536784220176050514689790808038522923760839359903825920.00000000\n",
      "Iteration 74, loss = 26320963677385867455708116464429781802507107042259733390174962390024562905872310717452670780991638893232128.00000000\n",
      "Iteration 75, loss = 26320963472795556209905011382176903627187442261119834037692129940253608577392015978857218747797093149048832.00000000\n",
      "Iteration 76, loss = 26320963268205261260389716975812715599433284755005223096956446817972659338035316075312164821296196872044544.00000000\n",
      "Iteration 77, loss = 26320963063614966310874422569448527571679127248890612156220763695691710098678616171767110894795300595040256.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 78, loss = 26320962859024675435431080832056512080816346561532323318421867905283262131602814976984656494967816684830720.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 79, loss = 26320962721948474960362559128713300037720732260594844789474409718711106647333180805885800022515114659807232.00000000\n",
      "Iteration 80, loss = 26320962679576407394241222228072819150644648995556374761898681655283848398222794672991396277720430451621888.00000000\n",
      "Iteration 81, loss = 26320962638658331293235962137116856890150034855556899741417038236963153206771680115479467480391919255683072.00000000\n",
      "Iteration 82, loss = 26320962597740275562590465391021757314112304809339035235619331478004964376725059101780536316430469893718016.00000000\n",
      "Iteration 83, loss = 26320962556822215757873015975954485201183197944364848626884837387174274274397539379319005625795608164958208.00000000\n",
      "Iteration 84, loss = 26320962515904151879083613891915040551362714260634339915213555964471082899789120948094875408487334069403648.00000000\n",
      "Iteration 85, loss = 26320962474986104296582022483764286049107737851929119615289423869257896614304297351921143297872709441028096.00000000\n",
      "Iteration 86, loss = 26320962434068032269648715061780496325504500530685966697744567782809702695134081503171814027217610611884032.00000000\n",
      "Iteration 87, loss = 26320962393149980613075170984657569286358147303224424294883648355724015137368359198235482389929573616713728.00000000\n",
      "Iteration 88, loss = 26320962352231920808357721569590297173429040438250237686149154264893325035040839475773951699294711887953920.00000000\n",
      "Iteration 89, loss = 26320962311313865077712224823495197597391310392032373180351447505935136204994218462075020535333262525988864.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 90, loss = 26320962283898620093812177280059948144502535349337290951037811070373703581403213177340129808834627280830464.00000000\n",
      "Iteration 91, loss = 26320962275424204136144738298548548444952492605075803683760593058564751168212596725503689343871643019116544.00000000\n",
      "Iteration 92, loss = 26320962267240579138170999874824141904314265412060735632615974778406609076448216912971064720389751099621376.00000000\n",
      "Iteration 93, loss = 26320962259056945992053356113155390289893284581533023375597781834503464440122039682913241043561034446536704.00000000\n",
      "Iteration 94, loss = 26320962250873337290367428365319673896820564663543243736200312881835327437481254705431014526772791994220544.00000000\n",
      "Iteration 95, loss = 26320962242689687847961973927762232134834076557990243067434970610442177712031482640322792743250425873956864.00000000\n",
      "Iteration 96, loss = 26320962234506075072204093510954343204869979821244141325100714325901539437109798954077966699788771054845952.00000000\n",
      "Iteration 97, loss = 26320962226322441926086449749285591590448998990716429068082521381998394800783621724020143022960054401761280.00000000\n",
      "Iteration 98, loss = 26320962218138833224400522001449875197376279072726649428685052429330257798142836746537916506171811949445120.00000000\n",
      "Iteration 99, loss = 26320962209955183781995067563892433435389790967173648759919710157937108072693064681429694722649445829181440.00000000\n",
      "Iteration 100, loss = 26320962201771562858093281809140199431642940592914902811711879209651467253209583577659669625840966276481024.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 5676148550660796974080875276467112667182370823078362658112673250532756551229858489175452346733388059639808.00000000\n",
      "Iteration 2, loss = 7063336155328146165804852718536238798581202735432921786025383103123977760639255498543847230159677797957632.00000000\n",
      "Iteration 3, loss = 7063497162146023499496051540053395302129112524391891164367458375380923500525412989392155391914954622763008.00000000\n",
      "Iteration 4, loss = 7062962011984067747939202281740642203543785787516597521236718077974625795678350133685964458706130626936832.00000000\n",
      "Iteration 5, loss = 7062426644796418244310820959823732165896081720840282727838159095064586830831790832499917003915461920817152.00000000\n",
      "Iteration 6, loss = 7061891318093910530083366899219022223916814039527932800012831600223168173524385399615146751283087691218944.00000000\n",
      "Iteration 7, loss = 7061356031968728631329880933274504618789260742879770888447693872536657723651217006371143740110631402995712.00000000\n",
      "Iteration 8, loss = 7060820786417836345927636510478596232214847652746749779501986834023482313872522947440610717037526702358528.00000000\n",
      "Iteration 9, loss = 7060285581438162842142309393056247382318297631551084384572259085785807962460901493014154451979202117763072.00000000\n",
      "Iteration 10, loss = 7059750417026633214167622674260235850332956722958667512118271897053299415408052204519782188177673810870272.00000000\n",
      "Iteration 11, loss = 7059215293180163389535405942155951209486573128433667238992015040342493556073652548669652233859780118052864.00000000\n",
      "Iteration 12, loss = 7058680209895683555029323126211386912126713915087379002324233949723681720800525472845021240609302659465216.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 13, loss = 7058314448671451882517980257655456862517848509190555856636424003951965377357849805490036173263870183342080.00000000\n",
      "Iteration 14, loss = 7058200860116899645718964243915442304555180604296050682538478387581703702363383411588574655221029408866304.00000000\n",
      "Iteration 15, loss = 7058093853530922342209221751966396858420409568012252143543933093061586255609523474812136881298352289873920.00000000\n",
      "Iteration 16, loss = 7057986851001803018850436857882231288814491403681424958220586305527749002697156938388722442906867123355648.00000000\n",
      "Iteration 17, loss = 7057879850095774609593195299718943693499125868983877800020517696106407416496092554994386940303604243759104.00000000\n",
      "Iteration 18, loss = 7057772850811899059370395046633807453234800445276446467748444101154143554329402632040589356935366196592640.00000000\n",
      "Iteration 19, loss = 7057665853150167201520142593439434360015917290357406229796594023957829553565065074811480757786975156568064.00000000\n",
      "Iteration 20, loss = 7057558857110550517538769257330616655602838672932502365607456141409956508236788921968864456144544556122112.00000000\n",
      "Iteration 21, loss = 7057451862693017433368641853773017179087394247640238577420928631498639558167609180602594120289128552595456.00000000\n",
      "Iteration 22, loss = 7057344869897546560132008870662730111789855716009923824818878001893247023882807629709022235185312220315648.00000000\n",
      "Iteration 23, loss = 7057237878724112434879166126923677098139117960814544964446383428390645953626767339521901759124268266815488.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 24, loss = 7057164740415102214234821737044159575714749536997271767338500560235537210783894765069696199438979005677568.00000000\n",
      "Iteration 25, loss = 7057142026293193916568548307194548530854782580707478293168564768770159639149117232495542826036272247603200.00000000\n",
      "Iteration 26, loss = 7057120628071643936529360859217083134369187109429023440764817476949991894855795579460292489242240186580992.00000000\n",
      "Iteration 27, loss = 7057099230401820058090428131490210581475050689625956174114504853773996200918984715512692490976215846354944.00000000\n",
      "Iteration 28, loss = 7057077832797050960591713551414407110085956170774228602084307570698316288372755372754334413758864484728832.00000000\n",
      "Iteration 29, loss = 7057056435257163495975228687672339902318388755730151349860764023141648085278912428774738373970160512925696.00000000\n",
      "Iteration 30, loss = 7057035037782148497579080035076620750166750602291999685836102714390863729005433788858055436594926105657344.00000000\n",
      "Iteration 31, loss = 7057013640372020224655101935029853532750860576106900970289079305999717672535464933673383944990104546705408.00000000\n",
      "Iteration 32, loss = 7056992243026765436469448213372477505173744016216808368675134969382580780956085059742275437467105643986944.00000000\n",
      "Iteration 33, loss = 7056970845746394318202000542534924009663842969512527138336238034220706234969540938971228730709460314488832.00000000\n",
      "Iteration 34, loss = 7056949448530899740226841751815891106661248003170493599133010669737216808084259831025694653038696916320256.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 35, loss = 7056934821426462313280721451577371503433350528632217009978536076292127946636009032660648795632362428825600.00000000\n",
      "Iteration 36, loss = 7056930278745629764185367466312352154745290652632290872258829979206880995318877034587620553301882616938496.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 37, loss = 7056925999225132037040725841731906965457106608632204781626132325718262444656241612008965948946926807285760.00000000\n",
      "Iteration 38, loss = 7056921719804601850434699121835085748327612797885719502412590493729612486545672451716197093447921223860224.00000000\n",
      "Iteration 39, loss = 7056917440386695366166191220017378289244790266210668514490090387631781877333871734537523415626479856254976.00000000\n",
      "Iteration 40, loss = 7056913160971391195357450624174878769528910715136360777440498515094138937546121239469297400447187778797568.00000000\n",
      "Iteration 41, loss = 7056908881558676097274631160148026444282999483704749456719256047531054532269500163033070586221454799405056.00000000\n",
      "Iteration 42, loss = 7056904602148554145989685496908993850398433390672156655263150316815029933784907213991442499622693284872192.00000000\n",
      "Iteration 43, loss = 7056900322741032471128530805159082927435121868862146053211559153722942368583915132678962312329374877089792.00000000\n",
      "Iteration 44, loss = 7056896043336101906029273579710905467387467076072992918956711061541663974034501824379781089326321750769664.00000000\n",
      "Iteration 45, loss = 7056891763933772635871795492994892812483911059195502509840574369952447930838914061000397647297064822898688.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 46, loss = 7056888838535178879614771315572528015141421480758026672403638190777642206855088121148258385395610262962176.00000000\n",
      "Iteration 47, loss = 7056887930004753348287804000656454505887453656029332798233565679495760651220077174350775748768550969409536.00000000\n",
      "Iteration 48, loss = 7056887074105607263242528245556344447410321932103544435799903605978708843818245138841679333765985974878208.00000000\n",
      "Iteration 49, loss = 7056886218226037093929826901745274151998804332305460684629371179830270346134708707623308584509843429130240.00000000\n",
      "Iteration 50, loss = 7056885362346576924559847620182862352654460838928073712752096714239366200035437412995125055435834786840576.00000000\n",
      "Iteration 51, loss = 7056884506467220644024661397410850244040226223836900365762899211397244497099083191813229456533841497817088.00000000\n",
      "Iteration 52, loss = 7056883650587970289360244567915324094601788896410101695130172337240155873466095398458921551140569745457152.00000000\n",
      "Iteration 53, loss = 7056882794708824842048608964453240770116304651958597175119719258799975011066249355741551457587666438062080.00000000\n",
      "Iteration 54, loss = 7056881938829780228017801918052427733692396671726064702794752644204200637618646354898519649201719208837120.00000000\n",
      "Iteration 55, loss = 7056881082950840521339776097685057522221441774468826381092059825325334025404185104692425652656140424577024.00000000\n",
      "Iteration 56, loss = 7056880227072003684978555168865043867257751550808721158543247136227124538282416250741969704614223901884416.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 57, loss = 7056879641993176951781187204837747860156208101817950429065446213876316959388423346342708217377570699608064.00000000\n",
      "Iteration 58, loss = 7056879460287323049099107706025324626891049001293490995894112795384387850202422879260734828768310656499712.00000000\n",
      "Iteration 59, loss = 7056879289107694683837319135333408683940499821195012998190995841995290212170362814155072210767027340574720.00000000\n",
      "Iteration 60, loss = 7056879117931959094326305767552351740700500962762304356588174492781158238537018971713257329211260497035264.00000000\n",
      "Iteration 61, loss = 7056878946756230634441209570472596737020411537153159395124730974343903491395247869605991619333965295386624.00000000\n",
      "Iteration 62, loss = 7056878775580509304182030544094143672900231544367578113800665286683525970745049507833275081135141735628800.00000000\n",
      "Iteration 63, loss = 7056878604404791029476816019444820011448584165649238409679190097927524404305525177632508187941377450967040.00000000\n",
      "Iteration 64, loss = 7056878433229076828843554163767668886888313605687220808494502241044024110146899556194340821421025533100032.00000000\n",
      "Iteration 65, loss = 7056878262053367720800233144305733433442264069170605835980798549001150406339397320709422863242439073726464.00000000\n",
      "Iteration 66, loss = 7056878090877663705346852961059013651110435556099393492138079021798903292883018471177754313405618072846336.00000000\n",
      "Iteration 67, loss = 7056877919701957652857496443326207600332918633650020096826965828660405543286190267264786000232090888568832.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 68, loss = 7056877802686228769161999237821692604090432471720948772215652264449130414421435129810199466511800930926592.00000000\n",
      "Iteration 69, loss = 7056877766345067970101867377041030672821273857569046037776514543838372709672436872862173629139809220952064.00000000\n",
      "Iteration 70, loss = 7056877732109155334079758203613599602283569841569581167633610615152557253364900727881359590894472131510272.00000000\n",
      "Iteration 71, loss = 7056877697874011679138715298683734869993240365825913226809315577401356940076995861841206212255719274577920.00000000\n",
      "Iteration 72, loss = 7056877663638873116787613229969085808817131913527647914656004704490783217140214381754302241958731876139008.00000000\n",
      "Iteration 73, loss = 7056877629403733535918522994011393613418179256540302076768496998612084176133208224476748389993391386001408.00000000\n",
      "Iteration 74, loss = 7056877595168602103193338095998046491801980237065600444754563956478387679687999484724393591374875629453312.00000000\n",
      "Iteration 75, loss = 7056877560933461503806259692797311162180183375389174081132859417631563320610768650256189857741182047617024.00000000\n",
      "Iteration 76, loss = 7056877526698319885901193122353532698335542309023667191776958045816613643463313138597336242439135374082048.00000000\n",
      "Iteration 77, loss = 7056877492463182342068079220881926771382278061414482405357844005874165238596756335701082153810501067341824.00000000\n",
      "Iteration 78, loss = 7056877458228050909342894322868579649766079041939780773343910963740468742151547595948727355191985310793728.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 79, loss = 7056877434824905132603794881767676650517581809553966508421648250898213716378596568457810048447927319265280.00000000\n",
      "Iteration 80, loss = 7056877427556672157977377975817109756885474722972321540946463240401561920972617175315684975638846503911424.00000000\n",
      "Iteration 81, loss = 7056877420709489427069358507683014915933365078834612461771043088070773766097065010881392191656108467683328.00000000\n",
      "Iteration 82, loss = 7056877413862460492377552293248433342630730342748062768459344713926908639825439102235231539594687277957120.00000000\n",
      "Iteration 83, loss = 7056877407015433594721722413299938037773784016039674126616040005719294149694262547970370650869972271628288.00000000\n",
      "Iteration 84, loss = 7056877400168404660029916198865356464471149279953124433304341631575429023422636639324209998808551081902080.00000000\n",
      "Iteration 85, loss = 7056877393321380817928050820645990562282735567311977368663627422272190487502134116631298755088895350669312.00000000\n",
      "Iteration 86, loss = 7056877386474351883236244606211408988980100831225427675351929048128325361230508207985138103027474160943104.00000000\n",
      "Iteration 87, loss = 7056877379627324985580414726262913684123154504517039033508624339920710871099331653720277214302759154614272.00000000\n",
      "Iteration 88, loss = 7056877372780298087924584846314418379266208177808650391665319631713096380968155099455416325578044148285440.00000000\n",
      "Iteration 89, loss = 7056877365933271190268754966365923074409261851100261749822014923505481890836978545190555436853329141956608.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 90, loss = 7056877361252644275660509046080437369849819654939076053452795413466906585436882629511801715174894345388032.00000000\n",
      "Iteration 91, loss = 7056877359798997069624432764544498110589691714809298744517240311586701035513551944568986771612066327298048.00000000\n",
      "Iteration 92, loss = 7056877358429560930850024137814896396088407467857389138975835014307793531766531382558388167482859956731904.00000000\n",
      "Iteration 93, loss = 7056877357060155347615260528376588708272449361577895305460334706072645570126251136267286013404246337126400.00000000\n",
      "Iteration 94, loss = 7056877355690751801416473253424367288902179664676562523413228063773748244626420244357483622662338900918272.00000000\n",
      "Iteration 95, loss = 7056877354321347236699697811229102735309065763086149215631924588506725601056364675257031350252078373011456.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 96, loss = 7056877352951939616428957867304708779047419247428494330648030614335327003275635074584629432836758570008576.00000000\n",
      "Iteration 97, loss = 7056877351582535051712182425109444225454305345838081022866727139068304359705579505484177160426498042101760.00000000\n",
      "Iteration 98, loss = 7056877350213129468477418815671136537638347239558587189351226830833156398065299259193075006347884422496256.00000000\n",
      "Iteration 99, loss = 7056877348843725922278631540718915118268077542657254407304120188534259072565468367283272615605976986288128.00000000\n",
      "Iteration 100, loss = 7056877347474320339043867931280607430452119436377760573788619880299111110925188120992170461527363366682624.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 2579993437955525348040565582390881203239532912561745654212198553032809473657168719891902536265608028225536.00000000\n",
      "Iteration 2, loss = 3210514087360918622702290639905713373304584930344671048176501599365269686768961787569765388331375529033728.00000000\n",
      "Iteration 3, loss = 3210586075618275231654993447218957256626366299432850572130459336865424025306910347512753428493145930203136.00000000\n",
      "Iteration 4, loss = 3210341638085459847693688558702755526714519372195744304815094767366728652879919295802529734912595070025728.00000000\n",
      "Iteration 5, loss = 3210097102088944716128425172507218533357818414064992634233589301963446097052313275566190627053358003781632.00000000\n",
      "Iteration 6, loss = 3209852584675748102510192284043466165684139737867957702391063702160685263026043059618012431276878218657792.00000000\n",
      "Iteration 7, loss = 3209608085887755031325385512595828636827134875007639539530978532659963811685621097648403936101522185650176.00000000\n",
      "Iteration 8, loss = 3209363605723562494045304480872388554818911866275613946797196379868656101296554559537153143370906089816064.00000000\n",
      "Iteration 9, loss = 3209119144181753732148408553800146215683181989160869627921922585124444696176317473090274652405879378280448.00000000\n",
      "Iteration 10, loss = 3208874701260910459336174845441537214109390214118774498036067240312824183537046850327808240024761860620288.00000000\n",
      "Iteration 11, loss = 3208630276959611843015110051751389309229870999881995157935048354898975855415318010293168978874990617165824.00000000\n",
      "Iteration 12, loss = 3208385871276441124663673537656702797068335623939520311351071270220582276128604980792371468277415095042048.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 13, loss = 3208218427797117361649912590068478476979768835289247551301185681487937019480537781802974981659206321438720.00000000\n",
      "Iteration 14, loss = 3208166456479652941208532488537417408886482866315388717580340048296241618545470673194048189038971735506944.00000000\n",
      "Iteration 15, loss = 3208117579945529863933211500464960709443632835443709944966141551246993621364472039765577925256511741755392.00000000\n",
      "Iteration 16, loss = 3208068705300767330629013946215397161117500993493533101951968839859956412626796495178776332308507185381376.00000000\n",
      "Iteration 17, loss = 3208019831401017094575545158998094193973240459208273474692616824194952076923071563688969044591428193222656.00000000\n",
      "Iteration 18, loss = 3207970958245843739332863642412111927744953728006006311818939410378407139232247746293331648884328064090112.00000000\n",
      "Iteration 19, loss = 3207922085835240135275052225756148422872731367063167933191558767633444373062752302657314973508735156092928.00000000\n",
      "Iteration 20, loss = 3207873214169191513891282484006078233125332408388090715664620817922246666396327413516495734273529639600128.00000000\n",
      "Iteration 21, loss = 3207824343247686162224690493866905314940048498056348613294861982111372861425389291178400291992650960076800.00000000\n",
      "Iteration 22, loss = 3207775473070712876577406415665155191865593384488055843006117097551444459377466486545879948314215108837376.00000000\n",
      "Iteration 23, loss = 3207726603638260962510554493348874954562102918447866884589319418077145620515199889117110945720514623045632.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 24, loss = 3207693121355691500078162036281797760932714702178455274784954230837657190032925009532713571964570973503488.00000000\n",
      "Iteration 25, loss = 3207682728743027747129377232208876633121396219967093327707192652939913569249049883279819516990010356137984.00000000\n",
      "Iteration 26, loss = 3207672954857402187939759653813545854277712328257329936930363990280261374904369966374880241069880420335616.00000000\n",
      "Iteration 27, loss = 3207663181230471031082728303186919469856421620799378300541337734738271154995674379824035791819864125997056.00000000\n",
      "Iteration 28, loss = 3207653407633399766084796016855859096558679783115199613600863605726230799863839991555772213677437162094592.00000000\n",
      "Iteration 29, loss = 3207643634066111494837856167970608100559749361179214183177080714150678795206903673676023440681941105377280.00000000\n",
      "Iteration 30, loss = 3207633860528603161787944254802037079191097740924180432067398561107239186814191394612839827828316680749056.00000000\n",
      "Iteration 31, loss = 3207624087020875276194054360971667599564147024694638623138915563079974633720815492961546315950740434059264.00000000\n",
      "Iteration 32, loss = 3207614313542925291761216068371891826121786700767887442056139637648571840751214275745518200878329636061184.00000000\n",
      "Iteration 33, loss = 3207604540094752189971441209759666624641172564454846363084873951844905489835163065774105600942731195056128.00000000\n",
      "Iteration 34, loss = 3207594766676361572673664704971729233347947741545458277763201086993664830358897587595882865319887115386880.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 35, loss = 3207588070476348296625059016536411386460330802732843331303571926618306527241494741092318091773087817138176.00000000\n",
      "Iteration 36, loss = 3207585992019842094988030569960350604418764701093640239505333286027439555276665744630033301602533461983232.00000000\n",
      "Iteration 37, loss = 3207584037299559657255330354724170918206816256584564366287858038195010361190784029666421451869872705241088.00000000\n",
      "Iteration 38, loss = 3207582082626250759877909305015820022458165970124795091361220863818187906831902008880027205747624546140160.00000000\n",
      "Iteration 39, loss = 3207580127954150334093448689178147882114164547259069600059126006122055342794599474799717559126319687532544.00000000\n",
      "Iteration 40, loss = 3207578173283237500283191078728770245606505791861237114830538389260043648639270545017169937804719749595136.00000000\n",
      "Iteration 41, loss = 3207576218613517860296071393504424351160832829721240527213540594556842073752150944080958690958766736670720.00000000\n",
      "Iteration 42, loss = 3207574263944990395614101466262067064554301456149999311473935789044325300063015994800433936920107557060608.00000000\n",
      "Iteration 43, loss = 3207572309277652050683316795272568983118379057080271890409133473818117373361191665603646030683682935668736.00000000\n",
      "Iteration 44, loss = 3207570354611505881057681882265059509521598246579299841221724147782594247857351988062544617254552147591168.00000000\n",
      "Iteration 45, loss = 3207568399946552395996190810861060210875381126991623426778806227421818582586609300772454637466891738677248.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 46, loss = 3207567060716808355815088961828748847406971788117063684890801733601763057073099863416414834576706777382912.00000000\n",
      "Iteration 47, loss = 3207566645028149762259782001313252745590232054077555136478256824581539927685081502984140853251527627767808.00000000\n",
      "Iteration 48, loss = 3207566254086367217973624145084118274269813754016921716002611059651639212451519455836344296070108002910208.00000000\n",
      "Iteration 49, loss = 3207565863153785455933575078885136805012518512765217515394055912281799284592578810424253789956342238150656.00000000\n",
      "Iteration 50, loss = 3207565472221258184605892960188963016677388222379321441565031328706663873490658394711931953099466879270912.00000000\n",
      "Iteration 51, loss = 3207565081288777255846672451051251835481669245346589288641962645181230434583960791174179732152657192681472.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 52, loss = 3207564690356344197432895802336567962759627888700641845226145111157686944977823015596971949618442815930368.00000000\n",
      "Iteration 53, loss = 3207564299423959518623557097666432965622686254786019374184677143120096063707357406575633546331000294866944.00000000\n",
      "Iteration 54, loss = 3207563908491621691641674086176282142736578036569101086916263491616269813667226948324189699787799991943168.00000000\n",
      "Iteration 55, loss = 3207563517559333262782217185973723329658413745772588297756396239066521490032993333819265114159724636405760.00000000\n",
      "Iteration 56, loss = 3207563126627092195009210062572670257942504973018319955236681719534600456664207208679560026110068044857344.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 57, loss = 3207562858781553747870422274988286763632753174473951825166984823628280002055027761321189389711491697999872.00000000\n",
      "Iteration 58, loss = 3207562775643926529104946842021413114533220628765712055813070904353893010182475968995412452619483076362240.00000000\n",
      "Iteration 59, loss = 3207562697455661177607656239027946733213693288426292424928558302015128834420872168129017550500800858423296.00000000\n",
      "Iteration 60, loss = 3207562619269228649230072589890600385902312286087278852731246625817874526028574971837084710583507152273408.00000000\n",
      "Iteration 61, loss = 3207562541082798157888465275239340307036619693126426332002328615556870853776727129926451634002919629520896.00000000\n",
      "Iteration 62, loss = 3207562462896368175805852044209601795282349202510114074140509021779929840559991626611143498256508652617728.00000000\n",
      "Iteration 63, loss = 3207562384709941758536197398530514253308033428305583656348378343391427440589042493463109948349333496659968.00000000\n",
      "Iteration 64, loss = 3207562306523517887561513170959034546890828165823754552891739747423238335793655053291701102613041069948928.00000000\n",
      "Iteration 65, loss = 3207562228337095035104817110630597974696467108031005975169297984423174549068492290310942138545101734936576.00000000\n",
      "Iteration 66, loss = 3207562150150674219684097384788247670947794459616418448915249887359361398483778881711482937813868583321600.00000000\n",
      "Iteration 67, loss = 3207562071964256459817342160675026769867654425269072499863792289199924202109739504683973382087694706802688.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 68, loss = 3207562018395164862973797645598231591726642499647671180450162870915040136697453514824567385167958286270464.00000000\n",
      "Iteration 69, loss = 3207562001767643798848051678149942339064966070669069487236426468823101606024909268279206488923474856247296.00000000\n",
      "Iteration 70, loss = 3207561986129994395213350959626204346003299739481875453702632547040599915925397345992267082505809542774784.00000000\n",
      "Iteration 71, loss = 3207561970492712167313384532219516240276969198708210947346796910267275390141881550934610017529433786679296.00000000\n",
      "Iteration 72, loss = 3207561954855428411636435853948263433216372350900925652389666024041762887253028740090978130050528393035776.00000000\n",
      "Iteration 73, loss = 3207561939218147711513451677406140028824308117160881934635125636720626338574849960819295887576682274488320.00000000\n",
      "Iteration 74, loss = 3207561923580863955836502999134887221763711269353596639677994750495113835685997149975664000097776880844800.00000000\n",
      "Iteration 75, loss = 3207561907943580709418548404485155981814536523890851607587962280753663991832256677727357053453048033050624.00000000\n",
      "Iteration 76, loss = 3207561892306297972259587893456946308976783880772646838365028227496276807013628544074375047642495731105792.00000000\n",
      "Iteration 77, loss = 3207561876669017272136603716914822904584719647032603120610487840175140258335449764802692805168649612558336.00000000\n",
      "Iteration 78, loss = 3207561861031732497941666871400526963301278594536237299919160120981502437376372276768411036021391127216128.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 79, loss = 3207561850317914789683750868730993808206782732225405351476952337105400815136049885110919765638455698128896.00000000\n",
      "Iteration 80, loss = 3207561846992411289821193392311466151630438389712041380848142839764700831650698309835302503557406176313344.00000000\n",
      "Iteration 81, loss = 3207561843864881714649649698779631493284958384881326731861643105298638089051863328535109586774379041128448.00000000\n",
      "Iteration 82, loss = 3207561840737424963514259963125380931872839015319869672870216928053535588474092766366383209278597962399744.00000000\n",
      "Iteration 83, loss = 3207561837609967703119876143849608803349297543413872351011692334324370428861209865602331890948640337821696.00000000\n",
      "Iteration 84, loss = 3207561834482511461243480491816879809048600276196955554887364573563330587318551642028930454287035804942336.00000000\n",
      "Iteration 85, loss = 3207561831355056237885073007027193948970747213669119284497233645770416063846118095646178899293784363761664.00000000\n",
      "Iteration 86, loss = 3207561828227598977490689187751421820447205741763121962638709052041250904233235194882127580963826739183616.00000000\n",
      "Iteration 87, loss = 3207561825100143244873287619340214393257930576890745429381479707764273721725689309904051085136398752153600.00000000\n",
      "Iteration 88, loss = 3207561821972686493737897883685963831845811207329288370390053530519171221147918747735324707640617673424896.00000000\n",
      "Iteration 89, loss = 3207561818845229233343514064410191703322269735423291048531528936790006061535035846971273389310660048846848.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 90, loss = 3207561816702467015765315481292241146793068029056929342297543262873348650578263448987619981402931982237696.00000000\n",
      "Iteration 91, loss = 3207561816037366621348200436181248555744652421960980705892040413295646249302260537089691493487228005384192.00000000\n",
      "Iteration 92, loss = 3207561815411859585944104713507534176430427795836849197787123950137495850905246395919938040295434177478656.00000000\n",
      "Iteration 93, loss = 3207561814786369356086813750344031511793132547082546364296455230953413200666939428395907634631466362601472.00000000\n",
      "Iteration 94, loss = 3207561814160876070675558285451399444487304684261001953603196012864954596217958429299927583962439272628224.00000000\n",
      "Iteration 95, loss = 3207561813535385840818267322287896779850009435506699120112527293680871945979651461775897178298471457751040.00000000\n",
      "Iteration 96, loss = 3207561812909894083183994108259829413878447879718775498020563325044601318636007478465891950131974005325824.00000000\n",
      "Iteration 97, loss = 3207561812284402325549720894231762047906886323930851875928599356408330691292363495155886721965476552900608.00000000\n",
      "Iteration 98, loss = 3207561811658911586433435847446737816158168972832008779570832220740185382018944189036531375467332192174080.00000000\n",
      "Iteration 99, loss = 3207561811033419319540168549797148883075185314699544894611769835619852095640187867131201206466658193899520.00000000\n",
      "Iteration 100, loss = 3207561810407928580423883503012124651326467963600701798254002699951706786366768561011845859968513833172992.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 15764520694059880481935494223113075889686632329316010647666679209963700873410052271752742553851055795339264.00000000\n",
      "Iteration 2, loss = 19748474834127717032321069894998623989396042409265123439133325143132822376511934692465357727421340003074048.00000000\n",
      "Iteration 3, loss = 19749491847397175573409334028718131247198048455202300772212681275925468362932646457375604427611810635448320.00000000\n",
      "Iteration 4, loss = 19748040675279309215465098196584947597504570606748199807199578849669127741794829864046546307210282570612736.00000000\n",
      "Iteration 5, loss = 19746588482369979948170189343927295563658957548355189679099609634095085172155419051822267897439962196344832.00000000\n",
      "Iteration 6, loss = 19745136395734190528493469055516600026524591072897834338248865455241418603996945501247305938197450537828352.00000000\n",
      "Iteration 7, loss = 19743684415878926618368969566447696822815550854611267444336499190911453216916525047731958722264496267591680.00000000\n",
      "Iteration 8, loss = 19742232542796565629173247229785769428765809000416834402633412907655308573352673596452511843686561112653824.00000000\n",
      "Iteration 9, loss = 19740780776479252750181556267182166717800858948125520751013630755290531715732680653117077876124601892732928.00000000\n",
      "Iteration 10, loss = 19739329116919129096597198231316065026454817316791989924414389551762168414202937014671169866566163060752384.00000000\n",
      "Iteration 11, loss = 19737877564108364302127143357671848449501438456765160078330437438122773344876124439398497548712675637198848.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, loss = 19736426118039103556046645867900865860366215805856016750635798564189893550179532433007173496225096441790464.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 13, loss = 19735432776669485885526702671467291629073108139274531316470335984603252317257766744996926459961637102157824.00000000\n",
      "Iteration 14, loss = 19735124070363123456157986672386416663993568163184311466214620872829224400066290924220614941683179426152448.00000000\n",
      "Iteration 15, loss = 19734833805688198677454705417812258080429546527257487881687018489219843625456558643340852079182232483266560.00000000\n",
      "Iteration 16, loss = 19734543553703955068379887784052273156068052190849125051519963632731101310672938646072883049449490424528896.00000000\n",
      "Iteration 17, loss = 19734253305992484049249613433628148874755518559885303441676531363146691578589905017835702347751496431435776.00000000\n",
      "Iteration 18, loss = 19733963062549850066557604139421214599421938715758871615220159091630385405859305093958167207571904180256768.00000000\n",
      "Iteration 19, loss = 19733672823376000157368475204793227350479414014637642233972611503839666252829455660526483782156352902660096.00000000\n",
      "Iteration 20, loss = 19733382588470873210602936595161599074557292175176783753882078621687015035286876086101659171403657096724480.00000000\n",
      "Iteration 21, loss = 19733092357834408115181698275943741718284920916031464630896750467084912669018085739244700475212631260528640.00000000\n",
      "Iteration 22, loss = 19732802131466535611881564874612722154508894318344209115091242398200837525247806570991415740135265158561792.00000000\n",
      "Iteration 23, loss = 19732511909367198663695199025558124866749936919526507765350531768819771792043456658665411592743785655697408.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 24, loss = 19732313277845360066531380954533703560923388423967462451871428462577152636564788377278096764922990608515072.00000000\n",
      "Iteration 25, loss = 19732251546052282131966932712742638092465618037646302072583077514772643892464981224831577507692725898575872.00000000\n",
      "Iteration 26, loss = 19732193501265916933168292542851341438303138785405232553358062403303478137369529151180596475380690455625728.00000000\n",
      "Iteration 27, loss = 19732135458334338175973444658062225173699427448902678653554479557998538617905620591165725940823236192239616.00000000\n",
      "Iteration 28, loss = 19732077415574261551698149825848221888494278478535369980750419217507157035153756101540530250232774520733696.00000000\n",
      "Iteration 29, loss = 19732019372984921134815306279440894647108849948114751182829862989799094200304978434936298389007780483694592.00000000\n",
      "Iteration 30, loss = 19731961330566320999396866687812415986434518676397144362729598206746851385640186300115629883821666447917056.00000000\n",
      "Iteration 31, loss = 19731903288318457071370878381990613369579907844626227417512837536477927318878480988315925208001020046606336.00000000\n",
      "Iteration 32, loss = 19731845246241333424809294030947659333436394271558322450116368310864823272300761208299783888219253646557184.00000000\n",
      "Iteration 33, loss = 19731787204334921541208444951878346119764340225899174739982679206800030339940735998729009237762480680206336.00000000\n",
      "Iteration 34, loss = 19731729162599258087215905165532226560586137076455683213542856211136059972326493738466996996691412448706560.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 35, loss = 19731689437764805750533420305645438619637730293343440696668085551852572036010724727068518934293003305484288.00000000\n",
      "Iteration 36, loss = 19731677091784883299765016958668874653204233253731320801066487575029931341287540197730418429873268669284352.00000000\n",
      "Iteration 37, loss = 19731665483153521349559777090193745499708281361528582269635599958096876081953915439416997800375920640393216.00000000\n",
      "Iteration 38, loss = 19731653874865782924130449010639067912498730652557293838595431795411130082740982553798054913171237548064768.00000000\n",
      "Iteration 39, loss = 19731642266585051902459711563221153778457308204460022458829474453427572406673828739850297904235825342709760.00000000\n",
      "Iteration 40, loss = 19731630658311132729093836637275721326797926716933307189371936002266141984269315976968949493245890418180096.00000000\n",
      "Iteration 41, loss = 19731619050044053922536492915607978315760223921271402750780327765034347721493735226492206366915319342039040.00000000\n",
      "Iteration 42, loss = 19731607441783795112427917053357062060887315723692698628370713082369683256942592944607070891877050280312832.00000000\n",
      "Iteration 43, loss = 19731595833530360372840061719495145099070578942953516925079879286144649862896787840076142594804495599796224.00000000\n",
      "Iteration 44, loss = 19731584225283766000060737589910917577875520854079146052654975703849252628479914747949819582391304767668224.00000000\n",
      "Iteration 45, loss = 19731572617043999771874086657687861886628011000800619702285640339865987736849277541940303274617240683544576.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 46, loss = 19731564672135903051700946156926818685275476480413703100276029926449650689691569634486896991486106543325184.00000000\n",
      "Iteration 47, loss = 19731562202955064331438507658480129039371238480994328998956295300295852382215972424459277251679974412255232.00000000\n",
      "Iteration 48, loss = 19731559881241823267945266659176179709429940576525659746278077504291810848031857320847439143357301236170752.00000000\n",
      "Iteration 49, loss = 19731557559596215872938283266908515313235210845760221347206356123681390449047206385150343340353329219764224.00000000\n",
      "Iteration 50, loss = 19731555237950918107399702716525963720785119340475262771330471965381066743410857315410811564528697079758848.00000000\n",
      "Iteration 51, loss = 19731552916305893304681950987278972100057274691863885092219339042538328280594721732765448075822693515001856.00000000\n",
      "Iteration 52, loss = 19731550594661149612928933417111885524834430537438732515746532018898177605160597054739451927582143259082752.00000000\n",
      "Iteration 53, loss = 19731548273016670735852839330136013847551079602174516630164901566970609627984888446282425013113396844822528.00000000\n",
      "Iteration 54, loss = 19731545951372464821597574064295702141989975523583881641348022350500626893629393324919566385763279005810688.00000000\n",
      "Iteration 55, loss = 19731543629728531870163137619590950408151118301666827549295894369488229402094111690650876045531789742047232.00000000\n",
      "Iteration 56, loss = 19731541308084871881549529996021758646034507936423354354008517623933417153379043543476353992418929053532160.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 57, loss = 19731539719105608165917935095579710836358077637251410951657030829930385376763135368520719058359732706213888.00000000\n",
      "Iteration 58, loss = 19731539225270048273400785606538515411370651395810793889561753820076815539578103273895044490071631405776896.00000000\n",
      "Iteration 59, loss = 19731538760927913393768173697171465193695870978213645009061314076811167540134517557260217229257054986698752.00000000\n",
      "Iteration 60, loss = 19731538296599279988586706761584202234043867919067945261074092159014735879589252679880221364131062125428736.00000000\n",
      "Iteration 61, loss = 19731537832270658805621097832913456885065995316191211821897232236835808035886683928788024079025306364542976.00000000\n",
      "Iteration 62, loss = 19731537367942033548583536235270538999196745894558156279783584982784378919903216468933227267246138236862464.00000000\n",
      "Iteration 63, loss = 19731536903613440884121595989405001408458511022975677561164236383712959982166938679179226668854269043539968.00000000\n",
      "Iteration 64, loss = 19731536439284840071515750405595118743937522513880554636671313120896538499868863471900027017115575116627968.00000000\n",
      "Iteration 65, loss = 19731535974956267777413573504590443837656171736079686432735901181187625923537079225959024052090767757279232.00000000\n",
      "Iteration 66, loss = 19731535510627695483311396603585768931374820958278818228800489241478713347205294980018021087065960397930496.00000000\n",
      "Iteration 67, loss = 19731535046299143559568983047441956709550354274259560539549013961132307132278004277890015755408214872555520.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 68, loss = 19731534728503373112696107980591432392820879953302878338401820706156226477028976559903399207399305412345856.00000000\n",
      "Iteration 69, loss = 19731534629736294541582689968355008110332684618816596170064421425540022942295339552831580412463666559975424.00000000\n",
      "Iteration 70, loss = 19731534536867897713788617336875674839793916994093949955696559732743402757285272854347851457684002790440960.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 71, loss = 19731534444002185699411353558058043380672472929787569576671549743925121005387455230417210580683088738648064.00000000\n",
      "Iteration 72, loss = 19731534351136469610962137110268239384659652046724867094709752423234337981208738897723970177008762320060416.00000000\n",
      "Iteration 73, loss = 19731534258270765744728778669394952999320961619931130921558317098161058773872718691318528353354673001857024.00000000\n",
      "Iteration 74, loss = 19731534165405053730351514890577321540199517555624750542533307109342777021974901067387887476353758950064128.00000000\n",
      "Iteration 75, loss = 19731534072539349864118156449704035154860827128831014369381871784269497814638880860982445652699669631860736.00000000\n",
      "Iteration 76, loss = 19731533979673654146028703346775093843304890339549922402104011122941221151864658072102202882392405047246848.00000000\n",
      "Iteration 77, loss = 19731533886807942131651439567957462384183446275243542023079001134122939399966840448171562005391490995453952.00000000\n",
      "Iteration 78, loss = 19731533793942226043202223120167658388170625392180839541117203813432156375788124115478321601717164576866304.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 79, loss = 19731533730383085805672287181302940150255411711760998250872842090803444570493374181673836682804984731926528.00000000\n",
      "Iteration 80, loss = 19731533710629666832192041443677917264244671189858684134855932369182202845721927813249393302479127068016640.00000000\n",
      "Iteration 81, loss = 19731533692055989096262007984970919624893468392416683733157074963371879317632273957057687322192559260827648.00000000\n",
      "Iteration 82, loss = 19731533673482845063757774161618524318312628852052878816177358032859222458340350948766519336123011503751168.00000000\n",
      "Iteration 83, loss = 19731533654909705105325493007238301548623166130445396002134428434219066871329326649237950876726876113469440.00000000\n",
      "Iteration 84, loss = 19731533636336573295037117190802423852716457046350557393965073499323913828880099767234581470677565456777216.00000000\n",
      "Iteration 85, loss = 19731533617763421114388978029505683472352863868474108271111781905066254425026379341418214431261192966111232.00000000\n",
      "Iteration 86, loss = 19731533599190281155956696875125460702663401146866625457068852306426098838015355041889645971865057575829504.00000000\n",
      "Iteration 87, loss = 19731533580617133049380510382800892859191184787746498437152348044040940706442533324835878459122097451958272.00000000\n",
      "Iteration 88, loss = 19731533562044001239092134566365015163284475703651659828982993109145787663993306442832509053072786795266048.00000000\n",
      "Iteration 89, loss = 19731533543470853132515948074040447319812259344531532809066488846760629532420484725778741540329826671394816.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 90, loss = 19731533530759027529453132487650807194364042699701357812779688901358387934730073964275404272551438122483712.00000000\n",
      "Iteration 91, loss = 19731533526808342919942692806331368109783619231569630568988949490659639335319604948837995691151584116342784.00000000\n",
      "Iteration 92, loss = 19731533523093600039427171310440058015508900398319850703362960812127072339596056501826975347082128294674432.00000000\n",
      "Iteration 93, loss = 19731533519378979381070229883713924027975486127759733925840592089769543512299469317693940803215043476848640.00000000\n",
      "Iteration 94, loss = 19731533515664346500497430450071272429767941400930650839507861371794510868160186007273107679327721558638592.00000000\n",
      "Iteration 95, loss = 19731533511949717693996583685400793368451773492857889856111917985691979496301801405614874082113812007223296.00000000\n",
      "Iteration 96, loss = 19731533508235080739351831582785969233352851947272484666842399935844445579881619386431441431553077722218496.00000000\n",
      "Iteration 97, loss = 19731533504520464155066842825032007782710814495468689992256818545359418024865930911061006414359405271187456.00000000\n",
      "Iteration 98, loss = 19731533500805827200422090722417183647611892949883284802987300495511884108445748891877573763798670986182656.00000000\n",
      "Iteration 99, loss = 19731533497091198393921243957746704586295725041810523819591357109409352736587364290219340166584761434767360.00000000\n",
      "Iteration 100, loss = 19731533493376561439276491855131880451196803496225118630321839059561818820167182271035907516024027149762560.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 16871370627118936912965584867637285353261664435041058838531786967854193599424944250423586742143656612855808.00000000\n",
      "Iteration 2, loss = 19278802991586685636164077277870842263451396212907747432682771972974083495053131571241116735321608915255296.00000000\n",
      "Iteration 3, loss = 19276512924768339127803227665745990140768870762391051615067571688338210380465966233299362688114272860372992.00000000\n",
      "Iteration 4, loss = 19274210104006287413818980481659963880550368029511820253588604874145291500740461393292631358230412570656768.00000000\n",
      "Iteration 5, loss = 19271907558293034331357767306522791231707640758278244616637249680189117194292620386444469290807960346296320.00000000\n",
      "Iteration 6, loss = 19269605287647719870453226971601050509928983465891564301240391243480664636784576979447452797538215389036544.00000000\n",
      "Iteration 7, loss = 19267303292037433677871699519684988706672454238781831784029962697954656343116561788022605410596078730870784.00000000\n",
      "Iteration 8, loss = 19265001571429363178106389048896993698789154813530830012120793142485845362930374438193339302318348206866432.00000000\n",
      "Iteration 9, loss = 19262700125790655054930972967637727994216418739157120903259838357223972023058827468357071378307697984143360.00000000\n",
      "Iteration 10, loss = 19260398955088407103255696656641783658195057739603401139952606139848761382963948911760024224085853828284416.00000000\n",
      "Iteration 11, loss = 19258098059289814895717669551975893641358927188964097875187500252979969036849335811950809065336438307946496.00000000\n",
      "Iteration 12, loss = 19255797438362057708666190413818100746776375186308349850205775131747345489794990377427639021049424524607488.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 13, loss = 19254283975417316361824829351644205679435257711138888213538174753002345341785480399129932802260292272652288.00000000\n",
      "Iteration 14, loss = 19253811795142178858531389103829457256980218243586851012871447537492748149401529867173954386856186243710976.00000000\n",
      "Iteration 15, loss = 19253351752937807621688332995713356950906654846857607372006471880813404223868944684558608587454556565143552.00000000\n",
      "Iteration 16, loss = 19252891721773833228606119950481911714027645126252587881057522157164335934522513319364787021673714220531712.00000000\n",
      "Iteration 17, loss = 19252431701601635704601598454228066284652234043823816092404579800115359881117045398729738369010473881305088.00000000\n",
      "Iteration 18, loss = 19251971692420958383141750361704950838623682017922999521030042901698895909955922270609692449039856439394304.00000000\n",
      "Iteration 19, loss = 19251511694231552745837462865610040625568003104414489887789884217692366411904322700486078134683707520319488.00000000\n",
      "Iteration 20, loss = 19251051707033125459508143799946912989306066352843095781235415853275679782737539657451729505456336714858496.00000000\n",
      "Iteration 21, loss = 19250591730825436153908585695357388253246639456585813128096185227971260957882549324513274487626414382120960.00000000\n",
      "Iteration 22, loss = 19250131765608232236577723075566769130124359652750671546291377765684031055923631758389542427442373780832256.00000000\n",
      "Iteration 23, loss = 19249671811381216300263011105606460426868719172126157521435518239721397200355181219410767877745112134975488.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 24, loss = 19249369208293982167305658328376048830660301277431351741395995041637645134059183190389297534336226008498176.00000000\n",
      "Iteration 25, loss = 19249274795528004836129989353530454741342568299879369190574647022418100113102735602260494811084106997170176.00000000\n",
      "Iteration 26, loss = 19249182807953098760705247857997500878076812846378233397477864288099908029499413308345652817648127982960640.00000000\n",
      "Iteration 27, loss = 19249090820827465043933029937763225248281122565053321061540891125732017994282098518856214261434309626822656.00000000\n",
      "Iteration 28, loss = 19248998834141411468637936108029162587370045634614349296146665010633903251192763087567905186394631324041216.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 29, loss = 19248906847894942108891919037767485432234958873817640204231973274678065072512305723243325119202505441411072.00000000\n",
      "Iteration 30, loss = 19248814862088044742479120720061676172201731826394227476986453922246999641398030299594675479837694878547968.00000000\n",
      "Iteration 31, loss = 19248722876720735665687351830800424954835871767369399526157256280830712046973531651672354374993849102630912.00000000\n",
      "Iteration 32, loss = 19248630891792986360012943687178524021897740965448901631186869027321693383272518818138165117957081546096640.00000000\n",
      "Iteration 33, loss = 19248538907304829418031517640973353668518353970683310615569590816699953828542181469092903922114691143303168.00000000\n",
      "Iteration 34, loss = 19248446923256236321239405009379706136458073051778371758747910325857984476816228643198374100752791326687232.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 35, loss = 19248386406218208815997993903936383034896971859845483740625336354146041231405246967651742297409976941936640.00000000\n",
      "Iteration 36, loss = 19248367524596431753653993196024820860493106153450582741717666773415502358557653464292369354968262122668032.00000000\n",
      "Iteration 37, loss = 19248349127915985881796928758123990439602335199023714416377443388835978724497031161654004454074327976902656.00000000\n",
      "Iteration 38, loss = 19248330731255050740521196027957439191515149268623397055311752341665048043660325123104772792152218411204608.00000000\n",
      "Iteration 39, loss = 19248312334611691145649277243743212092827559452996631763546610992464605982620648686409899199331059198066688.00000000\n",
      "Iteration 40, loss = 19248293937985919319397030412397826754213696208412384849892381336852156358220697977857182255631087437873152.00000000\n",
      "Iteration 41, loss = 19248275541377723039548597527004765564999429078601690005538701379210195353617776871158823381032066030239744.00000000\n",
      "Iteration 42, loss = 19248257144787110454247883925508373598967511701077191436359145783283725513373682783840021628880819708755968.00000000\n",
      "Iteration 43, loss = 19248238748214077489422936938936478319226567257082567039416927217200245565207517007138177472503936106627072.00000000\n",
      "Iteration 44, loss = 19248220351658640441361567243177769873342103021643105226459195008449760598242874376103689018595064691032064.00000000\n",
      "Iteration 45, loss = 19248201955120787087847916831315730650639988538489839688675587161414766795637058764448757687133968361586688.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 46, loss = 19248189851856353438174718164108791974470208398525867910218846496886588896353465506364517588857481859170304.00000000\n",
      "Iteration 47, loss = 19248186075569254598898685239249894903852167420061254646770105088390115876146458697912794523390550759964672.00000000\n",
      "Iteration 48, loss = 19248182396266538592334755504118282019076361753464019398689168178255633165544196937095404241136507766702080.00000000\n",
      "Iteration 49, loss = 19248178716964918511126093722501081558080920332317429840604023541823992698503687833417286634030391441227776.00000000\n",
      "Iteration 50, loss = 19248175037664003244365243673069729979293668556014564090583087319335072336058655345668887141424614571245568.00000000\n",
      "Iteration 51, loss = 19248171358363788717980252686852054745823229605799100045689572178916370805928200765087606236645764789960704.00000000\n",
      "Iteration 52, loss = 19248167679064287154186978770764573468343733937940004014733840116185391924955020217961242499714079197757440.00000000\n",
      "Iteration 53, loss = 19248163999765465960409800573029905851724167002386699174221592476162125514891924034188999717242258860277760.00000000\n",
      "Iteration 54, loss = 19248160320467365877368244783369777264878296986702406553520702577571584298547899301396873155964427445469184.00000000\n",
      "Iteration 55, loss = 19248156641169970608774500725895497560240616615861837740884021092923763186799351184534464709186935486152704.00000000\n",
      "Iteration 56, loss = 19248152961873267932412710393690549127136995433596026427501186026601158362803583557313975796889545881944064.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 57, loss = 19248150541226106903200351633740444739044020485738315539170754107308810846521910200654502564047988874805248.00000000\n",
      "Iteration 58, loss = 19248149785970174986422259757406075797651226499854224878995739425446980879464717279065505092084799008407552.00000000\n",
      "Iteration 59, loss = 19248149050110968895524339767046779828445937282359692013233154363975001899935221142787191689847929191792640.00000000\n",
      "Iteration 60, loss = 19248148314251864656425236500991797281525068533773211720890252599315554727428192725573866454446368545046528.00000000\n",
      "Iteration 61, loss = 19248147578392793009901754586714195029735214335237308252041649489636117733168353978461337432432106832658432.00000000\n",
      "Iteration 62, loss = 19248146842533745807809988686269627999293621049239337400813770371191688372593907483924405570458319321038848.00000000\n",
      "Iteration 63, loss = 19248146106674731198293844137602441263983042313291943373080189907727269190266650659488269921871830743777280.00000000\n",
      "Iteration 64, loss = 19248145370815741033209415602768289750020724489882481962967333435497857641624786087627731433325816367284224.00000000\n",
      "Iteration 65, loss = 19248144634956771238484750412795000920515290760254631067538413622630952454387415059580190578146863824764928.00000000\n",
      "Iteration 66, loss = 19248143899097850332623517250487782533706378855702645407350941792234062534520828536683844043048859683782656.00000000\n",
      "Iteration 67, loss = 19248143163238941648978142095097081757571597407419626055973831957454676431496938140075296087971092643184640.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 68, loss = 19248142679109744109680144075904199004896307178212237007466695889452280211620369093469134177791333569134592.00000000\n",
      "Iteration 69, loss = 19248142528058609059631129329686699181449096297365077372435213334673430248948254239560088719483691417468928.00000000\n",
      "Iteration 70, loss = 19248142380886813471057415224103172400791458823936978352174714439351048702588420550445540737778535962247168.00000000\n",
      "Iteration 71, loss = 19248142233715046400987369801324853378373459081803134052471726867136176062194877822669189442787267074588672.00000000\n",
      "Iteration 72, loss = 19248142086543271182773419040602189282172705702156645546895164631176300877239537677367639094449173453340672.00000000\n",
      "Iteration 73, loss = 19248141939371495964559468279879525185971952322510157041318602395216425692284197532066088746111079832092672.00000000\n",
      "Iteration 74, loss = 19248141792199737042633328195045551237336706217888956947489189486746555596452452221814936504466635678023680.00000000\n",
      "Iteration 75, loss = 19248141645027953676275472096378542067353199200729824236039052587041677866935314658988187102781717323186176.00000000\n",
      "Iteration 76, loss = 19248141497856194754349332011544568118717953096108624142209639678571807771103569348737034861137273169117184.00000000\n",
      "Iteration 77, loss = 19248141350684423610207333919794076559408576535218457739569864774484433858429127912198084039472591914663936.00000000\n",
      "Iteration 78, loss = 19248141203512668762353146503932275147664707249353579748677239197887065034878281310709531324501560127389696.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 79, loss = 19248141106686820291535251028354919015968620202248193312514879854167082991884990342110579983784101105631232.00000000\n",
      "Iteration 80, loss = 19248141076476590837082276477728115529144351934824968123746510944087812235982028146071211176118525255221248.00000000\n",
      "Iteration 81, loss = 19248141047042243941583391663527927783686954896408314628504773160640839743552757534536100159797731264561152.00000000\n",
      "Iteration 82, loss = 19248141017607876675724743504466877353772673764210050618579098717831360889718993379187991510109875439927296.00000000\n",
      "Iteration 83, loss = 19248140988173517558010000683350171997641146269524430814526998938766884580447026641365081913768844348882944.00000000\n",
      "Iteration 84, loss = 19248140958739170662511115869149984252183749231107777319285261155319912088017756029829970897448050358222848.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 85, loss = 19248140929304807470724420379061106359160844917665835412296374044382934506464890583244461774433606900383744.00000000\n",
      "Iteration 86, loss = 19248140899870448353009677557944401003029317422980215608244274265318458197192923845421552178092575809339392.00000000\n",
      "Iteration 87, loss = 19248140870436101457510792743744213257571920384563562113002536481871485704763653233886441161771781818679296.00000000\n",
      "Iteration 88, loss = 19248140841001730117580191915710990290766262433608976000140074707189505578648990369775732985410513627250688.00000000\n",
      "Iteration 89, loss = 19248140811567375073937401763566457471526111757679678299024762259997530541657922340715422915742894903001088.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 90, loss = 19248140792202202935330651067067682723052068257004807750030217992130033369690724921738072931595355678572544.00000000\n",
      "Iteration 91, loss = 19248140786160166822212742562475536114226518968535335759324833806608182271984289383560438034078430188797952.00000000\n",
      "Iteration 92, loss = 19248140780273279517196373856157939402812981558324187807354621989679782175462480942697977913451256976769024.00000000\n",
      "Iteration 93, loss = 19248140774386408508467815825729032838964951423138328267131559500241387168064267336885915899517733231919104.00000000\n",
      "Iteration 94, loss = 19248140768499525277523399788383608664442790831683502418098135015185488343823357604786055305563972386684928.00000000\n",
      "Iteration 95, loss = 19248140762612658342866794426926874637486137515253964980811859857619594608706042707736592818303861008629760.00000000\n",
      "Iteration 96, loss = 19248140756725771037850425720609277926072600105042817028841648040691194512184234266874132697676687796600832.00000000\n",
      "Iteration 97, loss = 19248140750838900029121867690180371362224569969856957488618585551252799504786020661062070683743164051750912.00000000\n",
      "Iteration 98, loss = 19248140744952016798177451652834947187702409378402131639585161066196900680545110928962210089789403206516736.00000000\n",
      "Iteration 99, loss = 19248140739065141715376940953433868086963002424459949996425311244886004400865998614387548549182467094872064.00000000\n",
      "Iteration 100, loss = 19248140733178262558504477585060616449332218651761446250328674091702606848905987591050287481902118616432640.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 12292166349180906394247787980045595054149428636432363153908676037284628524770082329400769938059304275804160.00000000\n",
      "Iteration 2, loss = 15625208235748691456329172575377164525577106756481711107722945271452418564235180940551585953198538607296512.00000000\n",
      "Iteration 3, loss = 15627218421281384612007411078533240516465370351455134647362192130901931309457478432923696969455345619435520.00000000\n",
      "Iteration 4, loss = 15626081769020062961159565958057500053202707670821727621731218427703713642657514848888694262232263119863808.00000000\n",
      "Iteration 5, loss = 15624943008476946888964616094208651338370041323942489612220181229538383003879819199544940398711409746116608.00000000\n",
      "Iteration 6, loss = 15623804329396392657756417998536180473718011711703523391742491825584025225733208489229747941270249868886016.00000000\n",
      "Iteration 7, loss = 15622665733296849662050200623952398512746425912293975581773006081726082758115874980794003100768717743587328.00000000\n",
      "Iteration 8, loss = 15621527220173304756308204800198998810616108446059498518594912128851740059381957541858988305572896029278208.00000000\n",
      "Iteration 9, loss = 15620388790019748684400243793321489452546251647424990370446895455021620515205860747590401712399543703699456.00000000\n",
      "Iteration 10, loss = 15619250442830111079116840834782790470385395569470517761515831570208828427048508541714948577864234242670592.00000000\n",
      "Iteration 11, loss = 15618112178598352128788164173337115922667406405948563088014500973430227638477565183678830608633133872971776.00000000\n",
      "Iteration 12, loss = 15616973997318434058780358392224766136371838759989769797624077829638932629201144287309549274709115004780544.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 13, loss = 15616174116399226126321277142155449690187995373530751474685040920555015775955540239498048380980229721554944.00000000\n",
      "Iteration 14, loss = 15615926173856806803913554417545043176663367199860612468536931081844799226123771675719096792506635248467968.00000000\n",
      "Iteration 15, loss = 15615698549508378915468912246981795051750201356191308270272244250642118559639183757403338047152748811517952.00000000\n",
      "Iteration 16, loss = 15615470942620637120276619696282363747846499089085025275712007908499688274806901549477192899234357890252800.00000000\n",
      "Iteration 17, loss = 15615243339060244888800437750018140879424262163450559145012340818425096975742198324879575537286995194675200.00000000\n",
      "Iteration 18, loss = 15615015738817292041015499133379430458209378954534394484438058200558999839158965018587137328235078496550912.00000000\n",
      "Iteration 19, loss = 15614788141891735799166300822158420846842392865395149213152893070240133506107765188592583242007777944535040.00000000\n",
      "Iteration 20, loss = 15614560548283517089209529116258610260398340024066152838573429115317229528515567557838220141840614220103680.00000000\n",
      "Iteration 21, loss = 15614332957992603318569562663902618403746205880496828537205367680810277728135182456223251814346288388898816.00000000\n",
      "Iteration 22, loss = 15614105371018925228023206092563512149732584515829700559123324124886756476191331834783486306076790215409664.00000000\n",
      "Iteration 23, loss = 15613877787362456336102767053922170008563526608148675235238180790375407502858174086562026693654939315470336.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 24, loss = 15613717840686610680834208962557530118648596036686340905665103117147346282584731544427780969842016981614592.00000000\n",
      "Iteration 25, loss = 15613668259758834387530069585415510592307004347491321853653321683159911613691836446739522356673781654618112.00000000\n",
      "Iteration 26, loss = 15613622741239637391974765474947165412869383009005300662105130634572302364451583397303029428510973491150848.00000000\n",
      "Iteration 27, loss = 15613577225681276262093589559842178578426169646363910883687165258156824530628822275835145929459145780494336.00000000\n",
      "Iteration 28, loss = 15613531710257567284320075844011708540878192998924345269027305599780896852789284467044218513359453003186176.00000000\n",
      "Iteration 29, loss = 15613486194966548793009014217354678787027514835517511254062451362835156727680241701738575086963840547553280.00000000\n",
      "Iteration 30, loss = 15613440679808208565944546672954571706200004699874442529982240551702100338458997853630417070252071313211392.00000000\n",
      "Iteration 31, loss = 15613395164782554751270578548755732372178416229507783302660247830126730229687350340244943516570970033750016.00000000\n",
      "Iteration 32, loss = 15613349649889581237879180841299901979625684196283050417691292200300294492943951098438255135910418158977024.00000000\n",
      "Iteration 33, loss = 15613304135129277840590471878156649186313366553309438617733405332541539947526553356303853111586884771905536.00000000\n",
      "Iteration 34, loss = 15613258620501673077908120342131181750481101031881202623344098549957975499401448075179934130314256440098816.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 35, loss = 15613226632346688850840266653845868286628588204612526481563675772038670003418421463232910840325568573997056.00000000\n",
      "Iteration 36, loss = 15613216716464326026897063687497659837623121712264423226874557436785866214121748556352034153040258374041600.00000000\n",
      "Iteration 37, loss = 15613207613014494421447836341034017383162667172483899629619414414654167770377852651762464885317894240468992.00000000\n",
      "Iteration 38, loss = 15613198510135568629834041068566228322672890184254143718778267683934505957391363713388467652880607605686272.00000000\n",
      "Iteration 39, loss = 15613189407262352650061911360598249715447724683009815073844566572525377246084415105797707053230753033748480.00000000\n",
      "Iteration 40, loss = 15613180304394435000864227650940655335458111974362381298202790561304153136086237243967630892353681478385664.00000000\n",
      "Iteration 41, loss = 15613171201531829941492824280996049061823870923958969752131695311824588080379975608567337513606336223379456.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 42, loss = 15613162098674531360839772247306172089207936303665097281226099826277930170544282136452927626978598718537728.00000000\n",
      "Iteration 43, loss = 15613152995822535184833118880898851880718931294724441782549216772791678134298258118861801705797056597065728.00000000\n",
      "Iteration 44, loss = 15613143892975847524580793185232347241693921125271486410506227149174583880063251618937859040071828409155584.00000000\n",
      "Iteration 45, loss = 15613134790134462268974866156848399366795840567171748010691949957617895499417914573537200339792795604615168.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 46, loss = 15613128392550681880456752472184952976636978978113024768232648138387529499696894455295010063307682999173120.00000000\n",
      "Iteration 47, loss = 15613126409386336198242426341484084597708124389711775718935996523024255813160656376672585821929871960440832.00000000\n",
      "Iteration 48, loss = 15613124588706525723716194086023056069640167277515409200308428810369087666236178399162772059867747079684096.00000000\n",
      "Iteration 49, loss = 15613122768140051856841259967429831322783931147443624280167646458826813101632879488409190825464254062133248.00000000\n",
      "Iteration 50, loss = 15613120947573875397218870683805201768998202786583352874412339333977131413535186317325375038219863820599296.00000000\n",
      "Iteration 51, loss = 15613119127007898567122162179817026522889938544782864512559611470880012067201529875608936057972679552008192.00000000\n",
      "Iteration 52, loss = 15613117306442137662838945131353995732024645697067447606356612197025460151755504998310271991416350723538944.00000000\n",
      "Iteration 53, loss = 15613115485876592684369219538416109396402324243437102155803341512413475667197111685429382838550877335191552.00000000\n",
      "Iteration 54, loss = 15613113665311255483569080063059022442240220546379183955026224753299056068964552519441069546029434653376512.00000000\n",
      "Iteration 55, loss = 15613111844746140319690361046685338748658153471540820364304017581235955810040972981014430457208965961875456.00000000\n",
      "Iteration 56, loss = 15613110024181220711265370140975936825862173697005917714547602339052916620600531463192568648712290876522496.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 57, loss = 15613108744666358262205347942309042694942346737132686511074493829545434776818047272400390591217336441634816.00000000\n",
      "Iteration 58, loss = 15613108348033962940330578117632535059623699840812697272763530203244678005779319485766230695059632492118016.00000000\n",
      "Iteration 59, loss = 15613107983898408660027793830654800296836927975881266473010428581151021731712384637400480562655785432121344.00000000\n",
      "Iteration 60, loss = 15613107619785485849422085684095507965648384302319117487110955510801932978037776965275101101057635916972032.00000000\n",
      "Iteration 61, loss = 15613107255672589520284069885855337124253789950673062170300600097624102494189010900106618562836666785988608.00000000\n",
      "Iteration 62, loss = 15613106891559705413361912094531683893533326055295973162300606680063775827182940961225934604635934755389440.00000000\n",
      "Iteration 63, loss = 15613106527446823343475730637694116931258550569297045205769006928439699796317320376726550409771908908187648.00000000\n",
      "Iteration 64, loss = 15613106163333951458769430853286981311212217130188922506579375506496876946153946564133665031591413977972736.00000000\n",
      "Iteration 65, loss = 15613105799221085685171060072338104496502948919215282961794925082362806004411920814684678943421037597949952.00000000\n",
      "Iteration 66, loss = 15613105435108228059716594629333572755576434345754287622884049321973737607231692482760891908597485951516672.00000000\n",
      "Iteration 67, loss = 15613105070995370434262129186329041014649919772293292283973173561584669210051464150837104873773934305083392.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 68, loss = 15613104815092479425889178126039112926293490755445088102014298497133198286912941487930659795743190754000896.00000000\n",
      "Iteration 69, loss = 15613104735766018694838011171478587815240957060584539717567648765299302657969240120035525686542005614673920.00000000\n",
      "Iteration 70, loss = 15613104662938916801735750185821820443844631688862162184077960571000074202173830309640094618742743409623040.00000000\n",
      "Iteration 71, loss = 15613104590116351387752786100679172900996394002304446270700966416731012431159132706399236501788151630528512.00000000\n",
      "Iteration 72, loss = 15613104517293781899697869346564352821256779496990408254387184930589449387863536394395778858160147484639232.00000000\n",
      "Iteration 73, loss = 15613104444471214448678928926935619009962853401054531289541797110384136980708389436773620977868849522147328.00000000\n",
      "Iteration 74, loss = 15613104371648644960624012172820798930223238895740493273228015624242573937412793124770163334240845376258048.00000000\n",
      "Iteration 75, loss = 15613104298826089731820929760108582729603443256073582617192989799654765347100342293435804033969784514150400.00000000\n",
      "Iteration 76, loss = 15613104226003512095622107668049417576081075113246900395005633649768199759242948563907147336994955634671616.00000000\n",
      "Iteration 77, loss = 15613104153180944644603167248420683764787149017311023430160245829562887352087801606284989456703657672179712.00000000\n",
      "Iteration 78, loss = 15613104080358383304692155832250208758830288149509629619720039007166326853354002711806730866422478259879936.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 79, loss = 15613104029177803473388784552603354126402451618637459942153549061527032159813938695720402040146964602945536.00000000\n",
      "Iteration 80, loss = 15613104013312519882729651766532811431663836199053626681431472512092505705815085710542834224320893545349120.00000000\n",
      "Iteration 81, loss = 15613103998747097467073223234915371688938882715330990123265141207296409378515554394082448247424334920941568.00000000\n",
      "Iteration 82, loss = 15613103984182591717606145222036752746773713451780826725875959573813099314418232549206955772045558825353216.00000000\n",
      "Iteration 83, loss = 15613103969618073745923209202241616193934413731961697019676415944712285433478214578043664716646545629380608.00000000\n",
      "Iteration 84, loss = 15613103955053555774240273182446479641095114012142567313476872315611471552538196606880373661247532433408000.00000000\n",
      "Iteration 85, loss = 15613103940489043913665266166109601893592879520457920761682509684319409580019526698860981895858637787627520.00000000\n",
      "Iteration 86, loss = 15613103925924530016054282815286637877644956619395113158419753387091096971360407436460290367133036958449664.00000000\n",
      "Iteration 87, loss = 15613103911360016118443299464463673861697033718332305555156997089862784362701288174059598838407436129271808.00000000\n",
      "Iteration 88, loss = 15613103896795504257868292448126796114194799226647659003362634458570722390182618266040207073018541483491328.00000000\n",
      "Iteration 89, loss = 15613103882230990360257309097303832098246876325584851400099878161342409781523499003639515544292940654313472.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 90, loss = 15613103871994880505104563844832719977098374247544900618991761170023302751236834263566149069047956473118720.00000000\n",
      "Iteration 91, loss = 15613103868821816046236027216571483618057035207991121971267449929578645043103356119881696405203258764689408.00000000\n",
      "Iteration 92, loss = 15613103865908730340883155709556343908444631465619698028753147469057675395959180243960839351821923329769472.00000000\n",
      "Iteration 93, loss = 15613103862995834079876083309747227164281249795417251872799455940608014909876794325500860288754262950805504.00000000\n",
      "Iteration 94, loss = 15613103860082925596653152903021592809443737668945839408035402416540850606951712280753082645666365471457280.00000000\n",
      "Iteration 95, loss = 15613103857170037483789985841156821139063109636256037457955285551836192665431123779818302635945529826082816.00000000\n",
      "Iteration 96, loss = 15613103854257120852423150096486841710442843872271980787317657364024025817944244317545325939510807613145088.00000000\n",
      "Iteration 97, loss = 15613103851344220517344125027705552429388085383313212528427178503701864059580959690322747349769734867386368.00000000\n",
      "Iteration 98, loss = 15613103848431322219301076293410349416779015303732605321005093309315952937358124417481468523365368305025024.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 99, loss = 15613103845518411699042169552198628793495814767883031804772646119312537998292593018352391116940764642279424.00000000\n",
      "Iteration 100, loss = 15613103842605523586179002490333857123115186735193229854692529254607880056772004517417611107219928996904960.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 7410996982185209711319038704773487335814924278389439909853856502284139296876361974072698390871651800580096.00000000\n",
      "Iteration 2, loss = 11382897833835669100199292516707780547696483335637539393051461727130787885714407969248154339282736337911808.00000000\n",
      "Iteration 3, loss = 11452288060289919485317849427906305795839037829591500755841782938738083468854236324162270817000358928187392.00000000\n",
      "Iteration 4, loss = 11450502530870347026539972075458886052364876485653756660327752275055334416665029657264658200057572826808320.00000000\n",
      "Iteration 5, loss = 11447767411053931862359602416805481357480135425144663759660009431902517241464372732150528799283650586738688.00000000\n",
      "Iteration 6, loss = 11445020309261287114893529744816918432646553322833698111084986848644537002039858315767768714829177091522560.00000000\n",
      "Iteration 7, loss = 11442273698603416444045315194765308632834019111536709618078802690760455510287913737151161096799903136350208.00000000\n",
      "Iteration 8, loss = 11439527744849547050838742773193152723223914820859818875539351742001645874437947981943500176518356580433920.00000000\n",
      "Iteration 9, loss = 11436782450048782312732104061943950664483755768080563175896647259353017039866317704727920666815365245829120.00000000\n",
      "Iteration 10, loss = 11434037814072330630121651178213379977963859018966463429961160423368099026662592147826885604442483833438208.00000000\n",
      "Iteration 11, loss = 11431293836762466344395781198827761182456374831885465491479731456096386115973710921581019591576638067834880.00000000\n",
      "Iteration 12, loss = 11428550517961135834150701348353525576997619555321585927787820363851020170334267580941685333185058146615296.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 13, loss = 11425934449936317503036627362419122479186215140052396529370606682539843485815714158824669771271341216890880.00000000\n",
      "Iteration 14, loss = 11425013820149421419959734848191588715662464525011699750930152886970238767927984455813233631828378254311424.00000000\n",
      "Iteration 15, loss = 11424460476051464781285502663700472505616808366078165895616452523432681180896773358139682984601961774448640.00000000\n",
      "Iteration 16, loss = 11423912043749647379008527779346556616051368666229535973210063298932890214724495112975461967402748675620864.00000000\n",
      "Iteration 17, loss = 11423363702759334467065012152139604836493227728259042332171416352422271741103875419554950003804172015632384.00000000\n",
      "Iteration 18, loss = 11422815388953521288802255156945844324469289521790525838153896912682277081673643469138636801311306263035904.00000000\n",
      "Iteration 19, loss = 11422267101477911622393147349933841790283052214885899573285097303048620382502660123469174685411545859489792.00000000\n",
      "Iteration 20, loss = 11421718840319881955892343920826991676003442891081127587929469716576109481226276281631930258536672291454976.00000000\n",
      "Iteration 21, loss = 11421170605478024697440197739739682485659770670066923317426990991315554804793988066148767055022712831344640.00000000\n",
      "Iteration 22, loss = 11420622396951068736587476087354082709142468099870790645500013583046559401565398343088632752765009039196160.00000000\n",
      "Iteration 23, loss = 11420074214737761296208733254727137252353163412923682919086432940974982045164154167952172899689258125623296.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 24, loss = 11419551363304340007041050582367314201895513303942812694759970792070317023029542763841359260437568089489408.00000000\n",
      "Iteration 25, loss = 11419367343256792345948486088934922758817029793846542757862518217185440603209697883454504193528492214714368.00000000\n",
      "Iteration 26, loss = 11419256728010531788775735049383323643848774824103293464514046059397199491297870588983072552855348298383360.00000000\n",
      "Iteration 27, loss = 11419147090287590745130590376254912915190793621239230772050216598334568984229853881796963746399885444775936.00000000\n",
      "Iteration 28, loss = 11419037466606830908116692800680683895950179390137480260282111392984633224078404692888831197143889031987200.00000000\n",
      "Iteration 29, loss = 11418927844151244322033195547206525267557697244621974362415989879643553501050698782342057746485190787596288.00000000\n",
      "Iteration 30, loss = 11418818222750335149732854799383940662784859051369027276838800526005586131816673144129051638789652718026752.00000000\n",
      "Iteration 31, loss = 11418708602401817836850223263824136885569269488081939256012850151597517366792152162431478410272937051422720.00000000\n",
      "Iteration 32, loss = 11418598983105651642665774250805388566997160367197489270570265437694334483168148749623342794200920119836672.00000000\n",
      "Iteration 33, loss = 11418489364861830456071578756869436901731466460581194166105865386487285572523314842560745500563483373076480.00000000\n",
      "Iteration 34, loss = 11418379747670346128923731444071936815989434130720409736746075334231368090295853023718487476013802077552640.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 35, loss = 11418275191864167590222753520430791014277548890874016318001398524328013007940495578742256365791379118358528.00000000\n",
      "Iteration 36, loss = 11418238392089515297321270115556458789112449138587652569734226524822279824493279872563178694332120433688576.00000000\n",
      "Iteration 37, loss = 11418216271182551996304018356197640735870013511520736056043829803596641174027256853438480774654702695481344.00000000\n",
      "Iteration 38, loss = 11418194345586928106065458735003370775520278285330699464923174675036751862412108265393067904387827078529024.00000000\n",
      "Iteration 39, loss = 11418172422631019693155682588047015113398408252298979757886853362162462149637777582840272301532474968637440.00000000\n",
      "Iteration 40, loss = 11418150499751762906999396818030854794085694710089465755035787564463359765832202914216271295594097918607360.00000000\n",
      "Iteration 41, loss = 11418128576915051654244833124412497231421099440151672721054828511295043811451843853419127340101067306172416.00000000\n",
      "Iteration 42, loss = 11418106654120435749941221585766877098907483969912008281428976030746123699457393082181592737641316600512512.00000000\n",
      "Iteration 43, loss = 11418084731367919268160514871066166933436225118126794539095017454689100702129749309266267014888258168422400.00000000\n",
      "Iteration 44, loss = 11418062808657489986686854973393849124333192428527065185242590787506471002626216408385351591821654909517824.00000000\n",
      "Iteration 45, loss = 11418040885989156053664147230694268745381139538625464425745270692943237145508591797064045521788331557388288.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 46, loss = 11418019975407087155457810594836146033654215434401281553334004460245432563773239032207431570707180614057984.00000000\n",
      "Iteration 47, loss = 11418012615621539090160093300215021623436399051164863872669643791895431896044241792180757152205867025694720.00000000\n",
      "Iteration 48, loss = 11418008191525835163115543639356574688049127781847375008498479522407813668409308979482882232150670562033664.00000000\n",
      "Iteration 49, loss = 11418003806484661640774223494180955307137542410698324231294762879198928992661723371071843263832815506030592.00000000\n",
      "Iteration 50, loss = 11417999421964686180373821293524066708508068945214063208762806351014057848426399116399551388495750686048256.00000000\n",
      "Iteration 51, loss = 11417995037453304974757574289665144682237994646191278331383726407870620580746900996430961030722073619595264.00000000\n",
      "Iteration 52, loss = 11417990652943700064512690957673448740608213324924930334443923161137738027539239896955764302556188475654144.00000000\n",
      "Iteration 53, loss = 11417986268435777745984259911189010535117058150019610850397287977747880926342745516434172090509610817945600.00000000\n",
      "Iteration 54, loss = 11417981883929540056208257484697916334210217530853480930712214523637299913297867209247484157919046829867008.00000000\n",
      "Iteration 55, loss = 11417977499424989032220660012686252406333379876804701626857096464742245624545054329777000268121202694815744.00000000\n",
      "Iteration 56, loss = 11417973114922120599949514826181846214595168369116950835895146469190216787803408169260120894442666045997056.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 57, loss = 11417968932828871586023928007886049182929237314843959349691983516505639933418305769553446816581812215611392.00000000\n",
      "Iteration 58, loss = 11417967460878540821286430451759708434447562766841022890079552885490506748059867831637464364772386607726592.00000000\n",
      "Iteration 59, loss = 11417966576062822663724957723110167289815773950163723794440357537681301945715886500557751739095076659265536.00000000\n",
      "Iteration 60, loss = 11417965699057709513186828660553641179806375407027908909166067889749999832190960032779301282600061086203904.00000000\n",
      "Iteration 61, loss = 11417964822156562604808859098868084000842014706475838867767700296178665054920182477778172004915233748418560.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 62, loss = 11417963945256861991974087022303777418316424664418115368928835517345281937368446533499874696291796622770176.00000000\n",
      "Iteration 63, loss = 11417963068357248971686297328641180378955436225621317083230898373770676173856032827617467211146725383208960.00000000\n",
      "Iteration 64, loss = 11417962191457707247657679341991602735193542115060155598926739537964842675259346525080551442786370562555904.00000000\n",
      "Iteration 65, loss = 11417961314558228671744327724410699413247988695221986710142784346182778897016590208363928337863907427221504.00000000\n",
      "Iteration 66, loss = 11417960437658819355054171479356729218455841194241293571284213796233236747549111940611497186389454527397888.00000000\n",
      "Iteration 67, loss = 11417959560759483371659163275801864687708476430874398285287815219988717499137810430585857515036424229879808.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 68, loss = 11417958724341760012836082836414740170474378805207446195872621897258591444937156323259655064998225672929280.00000000\n",
      "Iteration 69, loss = 11417958429951958674565506808380686918717537114767795594841312342768147506123884805245427808408144393011200.00000000\n",
      "Iteration 70, loss = 11417958252988960080014727278060121003124194099157402640263102287867351838855082570978028432846162661212160.00000000\n",
      "Iteration 71, loss = 11417958077588060486880072068508426395241894316971167171899221780830629839033238282052844047084213023801344.00000000\n",
      "Iteration 72, loss = 11417957902207944771811957620494928738718435420162139835555914821359148380216156745529144985735452589817856.00000000\n",
      "Iteration 73, loss = 11417957726828112204744553666047422396145665426917498653319327427026505344921535468006113028188851648069632.00000000\n",
      "Iteration 74, loss = 11417957551448283711749102380572088590464272252429179574019527364566363581907812899245680597315663073116160.00000000\n",
      "Iteration 75, loss = 11417957376068453181717674760610668516337190668562699443251333636169971182753640976103948403105768314765312.00000000\n",
      "Iteration 76, loss = 11417957200688630799830152478593593515992862722208863518356714571518581328161266470487415262242698290003968.00000000\n",
      "Iteration 77, loss = 11417957025308812492014582865548691052539911594611349696398882838739692745849790673633481648053040632037376.00000000\n",
      "Iteration 78, loss = 11417956849928992147163036918017702320641272057635674822972657440024553527397865522398248270526676790673408.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 79, loss = 11417956682645485364267580651581482010284256946936079962401740961892790148770092692425183378581772090474496.00000000\n",
      "Iteration 80, loss = 11417956623767537318829323452891188970607019065117116151005841046612205177850134515110136507283992934875136.00000000\n",
      "Iteration 81, loss = 11417956588374935562883191212340989519042662052616876508621805369695795408255924713875356868834890405117952.00000000\n",
      "Iteration 82, loss = 11417956553294768273879313444244385461829470234324227934053069997093204952362341853254378524370078814699520.00000000\n",
      "Iteration 83, loss = 11417956518218748390123252689819423960037879909967480149133838470696909678423644512959718333439056621338624.00000000\n",
      "Iteration 84, loss = 11417956483142781469302576632032705437834188229442919702392842258643130944136630386578851989262395196309504.00000000\n",
      "Iteration 85, loss = 11417956448066824733661782246676418257858938595809164512993814376270605390551863032104484461769264688267264.00000000\n",
      "Iteration 86, loss = 11417956412990861886913058857861872272546623734040926169189605496089327928545747614486217644266015630032896.00000000\n",
      "Iteration 87, loss = 11417956377914901077200311803533412555679997281650848876853790281844301102680081551249250590099472755195904.00000000\n",
      "Iteration 88, loss = 11417956342838938230451588414718866570367682419882610533049581401663023640673966133630983772596223696961536.00000000\n",
      "Iteration 89, loss = 11417956307762981494810794029362579390392432786248855343650553519290498087089198779156616245103093188919296.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 90, loss = 11417956274306280953046093309869769835699305127860200792123727690038645665819823954914523172048794722238464.00000000\n",
      "Iteration 91, loss = 11417956262530689306922465535645624959318169142118246978376154041046278035495382965070214034452532707721216.00000000\n",
      "Iteration 92, loss = 11417956255452170585362020155124454083761848467120727891074061838411996590488900488328297917432077148487680.00000000\n",
      "Iteration 93, loss = 11417956248436142016447587804271740316588862285969784699684459562138480026047262366719221680547209670557696.00000000\n",
      "Iteration 94, loss = 11417956241420932335995641916825706464582616674839584198589110992237719190066264706392650305018227918372864.00000000\n",
      "Iteration 95, loss = 11417956234405743025903459374240535297033255157490994212177699081699464715489760589879076562856308000161792.00000000\n",
      "Iteration 96, loss = 11417956227390553715811276831655364129483893640142404225766287171161210240913256473365502820694388081950720.00000000\n",
      "Iteration 97, loss = 11417956220375362368683117954584106693488843713415653187886481594686705130196303002470629315195761980342272.00000000\n",
      "Iteration 98, loss = 11417956213360173058590935411998935525939482196067063201475069684148450655619798885957055573033842062131200.00000000\n",
      "Iteration 99, loss = 11417956206344983748498752869413764358390120678718473215063657773610196181043294769443481830871922143920128.00000000\n",
      "Iteration 100, loss = 11417956199329792401370593992342506922395070751991722177183852197135691070326341298548608325373296042311680.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 13665348337638825734311366412187807213475785241784984158823847222185401917718592046089039607208501225455616.00000000\n",
      "Iteration 2, loss = 17118792874915774455353716930188102602759825397852532559638994801518142651816288389995297031020443709997056.00000000\n",
      "Iteration 3, loss = 17119658533911106350166431294756045816861300357280101311101530144926844547146422252851050916913665143734272.00000000\n",
      "Iteration 4, loss = 17118384667419326133853051717926352306215994986285007226314253345791882809105495632304230008629959092862976.00000000\n",
      "Iteration 5, loss = 17117109918419944335899373569131823017779045367824167647509205581973511744513252626288338976364402258739200.00000000\n",
      "Iteration 6, loss = 17115835263900472785869814336066198263101341302940918289579897340383766027398472845668050910497138142085120.00000000\n",
      "Iteration 7, loss = 17114560704300102551370018222387213737941009951292438892605885474468549447063308961816240494891425767358464.00000000\n",
      "Iteration 8, loss = 17113286239611970858195714344470231048773800117854147059568909444999468846333886064134005048184133268799488.00000000\n",
      "Iteration 9, loss = 17112011869829001043365116697651553615278177622894549989269373789441814273289147031985966738657979523923968.00000000\n",
      "Iteration 10, loss = 17110737594944124592040344615211829930915361924194799086381257709005878320569832162261946787942508143837184.00000000\n",
      "Iteration 11, loss = 17109463414950289285671328106320398636712079754561334167325689732391958669940276586902164524360912206823424.00000000\n",
      "Iteration 12, loss = 17108189329840410313132375828369218078564043296750018225029499733320342824917625767746043062849085856808960.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 13, loss = 17107312347080064987552872328170102565664977007163266979583297133434976314987104222239932675461875064897536.00000000\n",
      "Iteration 14, loss = 17107040153579155759953312003889910070895979461258974554150644675752347425377035026802244348815840641024000.00000000\n",
      "Iteration 15, loss = 17106785358004732093812768724450012743484648907412423784501212202193572236868622238568178669256713160359936.00000000\n",
      "Iteration 16, loss = 17106530574170253981245976047210648761352978630867587183178593159177282856961398121813863498272344423530496.00000000\n",
      "Iteration 17, loss = 17106275794134094261231756161010016663200087154764150092737288827184660043133834008037258849738723935911936.00000000\n",
      "Iteration 18, loss = 17106021017892561824580990977059798025438576685874287252447976529729551108891699758323193557544247381458944.00000000\n",
      "Iteration 19, loss = 17105766245445579263926579784888714647132287667827878706511696961234431880897919906182276614894079791071232.00000000\n",
      "Iteration 20, loss = 17105511476793130282980711908608076380715712825599636043181300794209297270028899616564109915094571697569792.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21, loss = 17105256711935127289196404965316173683024250555928634049315860393395369922245316651072803634667357214867456.00000000\n",
      "Iteration 22, loss = 17105001950871535652962061268749539990481197899386134849952683437876389023159531985226261796888431225208832.00000000\n",
      "Iteration 23, loss = 17104747193602257596550816763576034417693511205820407974608873962712324038029976608722095761595896925519872.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 24, loss = 17104571829937327295701370202952887396301360562451606349478603765313236252024324178428978571446415540617216.00000000\n",
      "Iteration 25, loss = 17104517399699846821356192757023336019432699145603660347230997219395504570796779615199202374350273435926528.00000000\n",
      "Iteration 26, loss = 17104466447832132721869995321299733599698093674044171293026308054129698827096886954369351338706604281298944.00000000\n",
      "Iteration 27, loss = 17104415497704947716074988846099514450859374097169820996042328790729938201224250560071885244967689547415552.00000000\n",
      "Iteration 28, loss = 17104364547730259297540334668289523683146357388188105725944968846925300321671181858637302063727076307894272.00000000\n",
      "Iteration 29, loss = 17104313597907336170350528707364790924556904580339208005580902151601806814016362627178986757107244723077120.00000000\n",
      "Iteration 30, loss = 17104262648236186482649476301269661248873769311135772040823703368504460222821590283222138378455019526553600.00000000\n",
      "Iteration 31, loss = 17104211698716789864077414105143271971640067486796187316989435838270754186682371282953759294403338884349952.00000000\n",
      "Iteration 32, loss = 17104160749349176870173987136276917119541125247992869606104004549944448247705445942093345955002795547426816.00000000\n",
      "Iteration 33, loss = 17104109800133327130579432049809734008120058500944208393483472844163036044486320716827900726886327681810432.00000000\n",
      "Iteration 34, loss = 17104058851069236571221796176769550100485490426893881576191053389054016304744096898394824083380522920706048.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 35, loss = 17104023779651577270594137804362190941576263914955652912059135115414708579761876414757445689220824666472448.00000000\n",
      "Iteration 36, loss = 17104012893942572592062236999850987510421554227478560742529331300688618811388955946559344441596702730747904.00000000\n",
      "Iteration 37, loss = 17104002703858887361974341608968531983572019507339688365003015467296226504521319876185638515070288009363456.00000000\n",
      "Iteration 38, loss = 17103992514099017518828481464303653657134793177194201609207411524049957628044853674747511644705610868457472.00000000\n",
      "Iteration 39, loss = 17103982324345356561338488833229721553155838631683599729075701354495627707658019627511063424621380722753536.00000000\n",
      "Iteration 40, loss = 17103972134597786341417736315553732101785228126874541739441052334330699847214755180360907581288638935203840.00000000\n",
      "Iteration 41, loss = 17103961944856266118346697221553959934109193475203806610935591144830161323906073245671048847973261837860864.00000000\n",
      "Iteration 42, loss = 17103951755120828484700992903007785345258749226721971167053616440974022315979163493542283438062548365082624.00000000\n",
      "Iteration 43, loss = 17103941565391459181228789018512604456114076515781908047516372561208528370450880443305513008199555233087488.00000000\n",
      "Iteration 44, loss = 17103931375668152096822156564610158461338110114249134097918678507724927578899876031816838268374163891683328.00000000\n",
      "Iteration 45, loss = 17103921185950931675912811555133482582279110934661581935881258271758227575011542511651856378626848541638656.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 46, loss = 17103914171720008306912094318315979628846126232661251425789691086639294993260124666372068633568801311424512.00000000\n",
      "Iteration 47, loss = 17103911994591745104897237885045861778971215340548548840534355402828554701783859919974415567125930546561024.00000000\n",
      "Iteration 48, loss = 17103909956586600015806587833391889890635770803884053851070087612977946261251428748124107625680854843392000.00000000\n",
      "Iteration 49, loss = 17103907918645244708314852213529414379032865944332885594353468616816008558890530026675888573189912229052416.00000000\n",
      "Iteration 50, loss = 17103905880704160326607969080316412570706519532077137182933207190175405463209395437940538044480892006563840.00000000\n",
      "Iteration 51, loss = 17103903842763324463290198754405935512754159063957037050656973007757379976663082083723758642850026158555136.00000000\n",
      "Iteration 52, loss = 17103901804822731007253612232339724399838719311838102043119585071753180190830241900881651078287196134834176.00000000\n",
      "Iteration 53, loss = 17103899766882386069606138517576038037297265503854815314726224379971558014132222952558114640802520485593088.00000000\n",
      "Iteration 54, loss = 17103897728942283539239848606656617619792732411872693711071709934603761538147677175609250040385880660639744.00000000\n",
      "Iteration 55, loss = 17103895691002405082830955489206686731313924351488287768940498742223535037612560380603359407006921009397760.00000000\n",
      "Iteration 56, loss = 17103893653062781255919104182517539398546167463374013260358495791874638054633612883259939190716234282827776.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 57, loss = 17103892250218710618055200664814368200795001775629486372239128863491761724841669291116876032538301966581760.00000000\n",
      "Iteration 58, loss = 17103891814793594940335591148692684993103484309290199022256632067525281353168866156747963034805477757157376.00000000\n",
      "Iteration 59, loss = 17103891407193029959312870134292342567364215058302387548863855609833054577856742850437987534618131194445824.00000000\n",
      "Iteration 60, loss = 17103890999605186267962357985500746584949062373930342495589522924026050499650837655345034072160899948347392.00000000\n",
      "Iteration 61, loss = 17103890592017360909935632847083927018545105373961746905530733231645302146708976649683778479734024352825344.00000000\n",
      "Iteration 62, loss = 17103890184429539625980860377639279989032525192749473418408730871137055066048014352785122413980561124098048.00000000\n",
      "Iteration 63, loss = 17103889776841728527205969580625064301748387058428005188628696840310061166089298827792965164910628812357632.00000000\n",
      "Iteration 64, loss = 17103889369253925576574984121555193688247002561619181164722237473228069810692380720326006969187521234206720.00000000\n",
      "Iteration 65, loss = 17103888961666136885195833003887926953865436930457484501094533767699832908278608093528147116821356939837440.00000000\n",
      "Iteration 66, loss = 17103888554078354304924610889678919024820936527430270991872011059980347914286183529874186554465311195660288.00000000\n",
      "Iteration 67, loss = 17103888146490581909833270447900342438004878171293862739991456681942116100996005738126724808792796368470016.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 68, loss = 17103887865921851708142714725186462458417007500125192682865402332839067044024130420207662426629504661192704.00000000\n",
      "Iteration 69, loss = 17103887778836862794803195241328375126766269284410440877537916561374781656849118946939715851139603700383744.00000000\n",
      "Iteration 70, loss = 17103887697316750613413041572242741148996690797964143003446718736210836556242874027430240656436816861200384.00000000\n",
      "Iteration 71, loss = 17103887615799194912173187683195374070566066081109964722189571661041439811900568856451968449300290185723904.00000000\n",
      "Iteration 72, loss = 17103887534281647359077239132092352065918195001768430646805999249617045612120061102998895295510588243836928.00000000\n",
      "Iteration 73, loss = 17103887452764097768945314246503243792824635513048735519954033172256400776199103995164522378384180118552576.00000000\n",
      "Iteration 74, loss = 17103887371246546141777413026428049251285387614950879341633673428959505304137697532948849697921065809870848.00000000\n",
      "Iteration 75, loss = 17103887289728994514609511806352854709746139716853023163313313685662609832076291070733177017457951501189120.00000000\n",
      "Iteration 76, loss = 17103887208211444924477586920763746436652580228133328036461347608301964996155333962898804100331543375904768.00000000\n",
      "Iteration 77, loss = 17103887126693893297309685700688551895113332330035471858140987865005069524093927500683131419868429067223040.00000000\n",
      "Iteration 78, loss = 17103887045176345744213737149585529890465461250693937782757415453580675324313419747230058266078727125336064.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 79, loss = 17103886989062598481654040204351102133480474070833307140451168384198315131234775071017465931644045073842176.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 80, loss = 17103886971645598661950159973093398398704638018312195727917277563969207417659323421982576853209358698283008.00000000\n",
      "Iteration 81, loss = 17103886955341580299744081908248444140042099139779258256035825330808919669818973146843281340942213697241088.00000000\n",
      "Iteration 82, loss = 17103886939038067122460134795952884455910285787030261548316002249838789684810062758266327136178202178748416.00000000\n",
      "Iteration 83, loss = 17103886922734566167392045690573842382452602890550231149406541164486163516643848495977171511434427760640000.00000000\n",
      "Iteration 84, loss = 17103886906431052990108098578278282698320789537801234441686718083516033531634938107400217306670416242147328.00000000\n",
      "Iteration 85, loss = 17103886890127543886896104134954895551080353003808559836903682334418404818906926427585862628579817090449408.00000000\n",
      "Iteration 86, loss = 17103886873824032746648133357145422135394228060437724180652252919384525470038465393390208187152511755354112.00000000\n",
      "Iteration 87, loss = 17103886857520519569364186244849862451262414707688727472932429838414395485029555004813253982388500236861440.00000000\n",
      "Iteration 88, loss = 17103886841217020651332073473956906646250420220586858125491362418998019953003790096905398120981432002150400.00000000\n",
      "Iteration 89, loss = 17103886824913509511084102696147433230564295277216022469239933003964140604135329062709743679554126667055104.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 90, loss = 17103886813690759243757772773306113171789022477492631920191326123713168311063420385714705307332507783397376.00000000\n",
      "Iteration 91, loss = 17103886810207360502038582527746224185901268312615306268565584159229097150032599668536507349647594218323968.00000000\n",
      "Iteration 92, loss = 17103886806946558866633343249263319602614448946286879825657687378533290236604978967889948010530871401512960.00000000\n",
      "Iteration 93, loss = 17103886803685852971918991691626469636274984820732022801764292896841263221778477923164477548239339204378624.00000000\n",
      "Iteration 94, loss = 17103886800425149114240616468475705938381209104555326829339292081085486843092426232820306849284513190641664.00000000\n",
      "Iteration 95, loss = 17103886797164451367670170248783201045824498616513114011319472263138462372827722605620035440339805727096832.00000000\n",
      "Iteration 96, loss = 17103886793903747509991795025632437347930722900336418038894471447382685994141670915275864741384979713359872.00000000\n",
      "Iteration 97, loss = 17103886790643043652313419802481673650036947184159722066469470631626909615455619224931694042430153699622912.00000000\n",
      "Iteration 98, loss = 17103886787382341831671020913816996220588859877361187145512863481807383872910016888968823106812033869283328.00000000\n",
      "Iteration 99, loss = 17103886784121640011028622025152318791140772570562652224556256331987858130364414553005952171193914038943744.00000000\n",
      "Iteration 100, loss = 17103886780860932079278294133029382556355620035629634149194468184359580479397464153899181945565675658412032.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 12526671977733533971390377329760252556384551012224411153930816465897964352241565163741881907587748917673984.00000000\n",
      "Iteration 2, loss = 16338416461987251974989504065920135154178753598555336827125511512353562114978157360401312639515103112200192.00000000\n",
      "Iteration 3, loss = 16344121784722827834294850997021352706068202631447554664321095658876066872498169377295990893806417011015680.00000000\n",
      "Iteration 4, loss = 16343057928326731847211785457648545591265418662952553633662406059366490706460773508277538971974623974916096.00000000\n",
      "Iteration 5, loss = 16341985271446296868252929226108557287599713643411162728184990356597489443403921479600519314994852314218496.00000000\n",
      "Iteration 6, loss = 16340912673348922761918693496874391554999449261568797184858121592618191092995651192914270542202130975948800.00000000\n",
      "Iteration 7, loss = 16339840145635767746643207666116939997101578041858412527411365629342739942741738612983067505705531473920000.00000000\n",
      "Iteration 8, loss = 16338767688317414224323529389054367189257386703826671134149817005593190742276579750659180739693676557500416.00000000\n",
      "Iteration 9, loss = 16337695301389264604761071730589965249548135280964079840908971703251857722605981767349044393220710298812416.00000000\n",
      "Iteration 10, loss = 16336622984846707038505413414224422416935264941114018123245570042647300661752606343789994271983833486196736.00000000\n",
      "Iteration 11, loss = 16335550738685107268710393484111477977477644348960093890564022018809322340194172262525068784976478890622976.00000000\n",
      "Iteration 12, loss = 16334478562899851408889614329265733901691026262967525566952674286130231899812891849910303974559141117034496.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 13, loss = 16333722465413513391142871712612583116148369085259994873229909957344763315883492485894512266173593619005440.00000000\n",
      "Iteration 14, loss = 16333486869501468588214846090643376767160301719941803289870049197456565376079604007424991515781272948441088.00000000\n",
      "Iteration 15, loss = 16333272428833088249011229080651513229933035697194395643575788170840879225426759992370446004906134238920704.00000000\n",
      "Iteration 16, loss = 16333058018689417015323080917770662093352366151751499928132436958613921829388656912794032985733870450114560.00000000\n",
      "Iteration 17, loss = 16332843611396638741594336067205088751071273624044965773676373507239762076435491093675402586966456790941696.00000000\n",
      "Iteration 18, loss = 16332629206918490113374288007647042332944694364051754939863556819584575394287855839116167888560416421314560.00000000\n",
      "Iteration 19, loss = 16332414805254863167756191011333950611351142674729331698869122601027078067501935366907441433670321621172224.00000000\n",
      "Iteration 20, loss = 16332200406405739571416258067891037170279422871674246587477527858141014370813685487617525352265816739938304.00000000\n",
      "Iteration 21, loss = 16331986010371068398455080815166145298587324720432473318978930942520118400711872341713925560929247192678400.00000000\n",
      "Iteration 22, loss = 16331771617150810945189108897923635895806768442818951915473852201375628070527958195951946556263195494842368.00000000\n",
      "Iteration 23, loss = 16331557226744932582006744629900042398361051079404944501999599313791282565874304025849492361543656528674816.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 24, loss = 16331406032457110036862793327358265071027714045495892355717989879654028186605026226971355073415244029100032.00000000\n",
      "Iteration 25, loss = 16331358919777468690228892300884943157901293148985128417212939842932870289220489710444177932382830126432256.00000000\n",
      "Iteration 26, loss = 16331316037037119962183330701006709396998466209995519198124120029375500650488218369155910857616340459454464.00000000\n",
      "Iteration 27, loss = 16331273159950497364266916195525699785568150847524945807198115303562467836304203630777892620750566157451264.00000000\n",
      "Iteration 28, loss = 16331230282983715629874235840985949102432647611647191353339871273310609800896156498646251245646267306475520.00000000\n",
      "Iteration 29, loss = 16331187406129496429461846518601220191147269794192818939978819548396403614438533768376932330252250627702784.00000000\n",
      "Iteration 30, loss = 16331144529387851985245606235288030662386147851430794875925322124437353093774031566257734454588753221517312.00000000\n",
      "Iteration 31, loss = 16331101652758790445369420328990725589932035420873763367052953665178460783464447309813856672002599821508608.00000000\n",
      "Iteration 32, loss = 16331058776242289402437549120362356020882359999361952847209383845320969685964838100851001585790022410305536.00000000\n",
      "Iteration 33, loss = 16331015899838369226809755954263784639694005680676973831078549324227386162679697483182166829318082821881856.00000000\n",
      "Iteration 34, loss = 16330973023547001399982372147889803688127334733524571598102938778790201307642734495469155715872894488674304.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 35, loss = 16330942785697716328896572536548868605146273190755418475239341288497873287339524594286326128386291684868096.00000000\n",
      "Iteration 36, loss = 16330933363421875183399398447160090353260547687553788330866883088885863611523825211829633720155940134584320.00000000\n",
      "Iteration 37, loss = 16330924787089520214354593399570361433078057173249837204652124577746254667258382247444238514375321723076608.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 38, loss = 16330916211869863554806880024827390675058174929369673864983085088536627265205580397953769770960085193850880.00000000\n",
      "Iteration 39, loss = 16330907636656167262525921356372841389122578525988747121833912128796361210107589468146408550747141285347328.00000000\n",
      "Iteration 40, loss = 16330899061446960597536803895252427757484236392074777815024378892552497208559975594723725724634625584660480.00000000\n",
      "Iteration 41, loss = 16330890486242278189451125327729616343719851487056503819517177700721296074950377802167817269346543209545728.00000000\n",
      "Iteration 42, loss = 16330881911042089482729240636513113121144097670261509363286403564258998267172055774759186734832301409042432.00000000\n",
      "Iteration 43, loss = 16330873335846394477371149821602918089756974941689794446332056483165603785225009512497834121091900183150592.00000000\n",
      "Iteration 44, loss = 16330864760655217617808568896832066470906744213879291686274860448676120262794631267959356588165813732638720.00000000\n",
      "Iteration 45, loss = 16330856185468534459609781848367523043245144574292068465494091469555540066195528788568156976013567856738304.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 46, loss = 16330850137939003831801725213354757938564097361441309340123419045076417880154575753170485929953877540470784.00000000\n",
      "Iteration 47, loss = 16330848253494243226912578551788965030898173455701427473565845406645766113775277281615898288207039444287488.00000000\n",
      "Iteration 48, loss = 16330846538236408043421689962585145695513118070616598843431001074558791178743181652821035936683109656821760.00000000\n",
      "Iteration 49, loss = 16330844823200375522213981890884666068910517826597859349340472211122332396539037831850904502411959092641792.00000000\n",
      "Iteration 50, loss = 16330843108164825778532665092386632063936070605203289053259242174577274379621390999248816978940173993639936.00000000\n",
      "Iteration 51, loss = 16330841393129445108837384056233758339953761362196086029054686410741019162361040580294609812415002116620288.00000000\n",
      "Iteration 52, loss = 16330839678094262031631807465231252655203227828870504997284316242721075650724277536326479689550330029146112.00000000\n",
      "Iteration 53, loss = 16330837963059241917304337633115648446107767045797808082985439349601183030323462842862330633622152613462016.00000000\n",
      "Iteration 54, loss = 16330836248024407173250714239233894665569951516137766852310386056680098298703539398096460041334237886939136.00000000\n",
      "Iteration 55, loss = 16330834532989765947614842621530336387372534877403025511132731027702824000426304619554066966033410583166976.00000000\n",
      "Iteration 56, loss = 16330832817955291758929030431685852121721567807677490390363356605498101865665916900278254484342490317979648.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 57, loss = 16330831608450984706608841676261019830650759726963763967978249880559026798710469482519034494444906221666304.00000000\n",
      "Iteration 58, loss = 16330831231562458733557261518437108607955590187727079561854690066736529526016614724776027456134472169160704.00000000\n",
      "Iteration 59, loss = 16330830888511235141124693794950489600821644931868067113398893277170991792289956747704195084398348732465152.00000000\n",
      "Iteration 60, loss = 16330830545504351710788998830103675850987306610440661977468022921973050927724395588384311262745719306452992.00000000\n",
      "Iteration 61, loss = 16330830202497517169316735892922932543849490114089122076778600549245125330529618934215621761174038281977856.00000000\n",
      "Iteration 62, loss = 16330829859490709109312165304061310726505622939653675845178295833688458003160683887003829182979537641668608.00000000\n",
      "Iteration 63, loss = 16330829516483905123379547384171861446053132583974551716514778450004291948072647548554636131458449368154112.00000000\n",
      "Iteration 64, loss = 16330829173477099100410953129796325897154953818917266536382867400383875256844161855724143316600654911242240.00000000\n",
      "Iteration 65, loss = 16330828830470309373730169551309480495822282328885269767998105678253463654739270997944048608436509921509376.00000000\n",
      "Iteration 66, loss = 16330828487463515572977433303850462557598234020096950896676556624250550780353481431401354373598952564981760.00000000\n",
      "Iteration 67, loss = 16330828144456746216656413070224479840722446623846564642975731561482645539653084117434257298801869409222656.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 68, loss = 16330827902555949583936422755797056719081176425929340795193628793267600755528284103207745774929609221996544.00000000\n",
      "Iteration 69, loss = 16330827827178254574505988396662705816770584564972809171310885160184354481691759923565643183951053328482304.00000000\n",
      "Iteration 70, loss = 16330827758568026967121676061648506670287578152577559513954232596135752278526202904954194721632160581681152.00000000\n",
      "Iteration 71, loss = 16330827689966658429198442406623488994103464125804722692641633188841166650174888090615417010648459430068224.00000000\n",
      "Iteration 72, loss = 16330827621365302113491066758514988928593480555300852180139395777164084838666269402564437879684995378839552.00000000\n",
      "Iteration 73, loss = 16330827552763945797783691110406488863083496984796981667637158365487003027157650714513458748721531327610880.00000000\n",
      "Iteration 74, loss = 16330827484162587445040339127811902529127825004914950103666527287873670579508582672081179854421361092984832.00000000\n",
      "Iteration 75, loss = 16330827415561247425620774155592092611183348709436368002911439203686593857123558819080598830151546508935168.00000000\n",
      "Iteration 76, loss = 16330827346959897221021327510941851351010430367066980644814382789818263954036288194173518989198201007898624.00000000\n",
      "Iteration 77, loss = 16330827278358549053457857200777696359283200434075754338185720041886184687089466923647738911581561690259456.00000000\n",
      "Iteration 78, loss = 16330827209757186626642552549210937488436151635437400671278301632400350967159500172452860490607979088838656.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 79, loss = 16330827161377032188984897689092059908377549778361542425246025877004343537071618620122677617841621891547136.00000000\n",
      "Iteration 80, loss = 16330827146301495224134787151751275996361119815548397151937870816323944918444763138575556862982616896241664.00000000\n",
      "Iteration 81, loss = 16330827132579450517472315218542870674442793896820611641053897769888724732267831476605787075853520820240384.00000000\n",
      "Iteration 82, loss = 16330827118859171921001325284771260094936318908958457753267233090182806079860490063222912101648685749764096.00000000\n",
      "Iteration 83, loss = 16330827105138909620818146026888339662995351196121592277227717737966892516576743484890435234137500146466816.00000000\n",
      "Iteration 84, loss = 16330827091418645283598990434519332962608695073906565749719808719814728317152547552176658603289608359772160.00000000\n",
      "Iteration 85, loss = 16330827077698370761199953169719894919993596904800733964869931371981310937026104847556383155758185656090624.00000000\n",
      "Iteration 86, loss = 16330827063978108461016773911836974488052629191963868488830416019765397373742358269223906288247000052793344.00000000\n",
      "Iteration 87, loss = 16330827050257840049725665650495795250774596250992519858385719669740731902037263627747530130725695899303936.00000000\n",
      "Iteration 88, loss = 16330827036537561453254675716724184671268121263130365970599054990034813249629922214364655156520860828827648.00000000\n",
      "Iteration 89, loss = 16330827022817297116035520124355177970881465140915339443091145971882649050205726281650878525672969042132992.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 90, loss = 16330827013141264191468012817845316186424056360122006742416297154867196928047700616803542187782991419277312.00000000\n",
      "Iteration 91, loss = 16330827010126161687384333913143766448290422550066964211278810940978118731059407971009237468819285260369920.00000000\n",
      "Iteration 92, loss = 16330827007381750709015863192015999115461068956943246057633622665754824057683572284233983748056759861772288.00000000\n",
      "Iteration 93, loss = 16330827004637704360087156343897673834409940642510356116830900593120393253448171031711387664564641291304960.00000000\n",
      "Iteration 94, loss = 16330827001893639677834662485404572137347616643674016712812635527059706723948725589757093711042167070261248.00000000\n",
      "Iteration 95, loss = 16330826999149589254834002968314074319405111510484804669073126122552774647432425628471898100876636132999168.00000000\n",
      "Iteration 96, loss = 16330826996405544942941272454681835306799671605430075779738797715854594479337473730330601780721223745929216.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 97, loss = 16330826993661486371796707599646992415074412834728219530125713647602659858259376351520207117208868075077632.00000000\n",
      "Iteration 98, loss = 16330826990917421689544213741153890718012088835891880126107448581541973328759930909565913163686393854033920.00000000\n",
      "Iteration 99, loss = 16330826988173383488759412230979910510743714158971634391178301172652545069086327074568516133541100017156096.00000000\n",
      "Iteration 100, loss = 16330826985429320843542894706972895082127078569513456038628429772528109175727330986995521943355331979509760.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 18688724380829443121702457736423288531321548619212204838548542289547767067593161176412120962837927545536512.00000000\n",
      "Iteration 2, loss = 19722355454812211665949441119379641390013828468945017001375603845437490731720697150337236983454145194229760.00000000\n",
      "Iteration 3, loss = 19716830998371157691715101039531383122631939087778593138067475032781619411315165815285043349875913759129600.00000000\n",
      "Iteration 4, loss = 19711308089393107569512568591992136254247698331279229756955612752063779488559716611475852975910788102356992.00000000\n",
      "Iteration 5, loss = 19705786727444738932384019227518704481131286458271736398402611712176149894786786840061409350746023567294464.00000000\n",
      "Iteration 6, loss = 19700266912092790524450918431450479552923536008925754146822876600098428645542294433632448863670060999245824.00000000\n",
      "Iteration 7, loss = 19694748642903980719474968344265990534808395429629313571946875443447807394967663780966710270604279409541120.00000000\n",
      "Iteration 8, loss = 19689231919445239742959409892996738410321407742099194596218017527211543955811052126497107714487500882837504.00000000\n",
      "Iteration 9, loss = 19683716741283599672206300728978537585282536436960229715499395433189427947843084433721543505093856673660928.00000000\n",
      "Iteration 10, loss = 19678203107986157769668941207101963055773774104938405072642699053141269347328765006339512378972075905253376.00000000\n",
      "Iteration 11, loss = 19672691019120243519901933813671424420685592005870066677896496195559450650544324547518682093055392608157696.00000000\n",
      "Iteration 12, loss = 19667180474253202703747691710880021426473968674616848953256503996426359442889588595477118510970690280095744.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 13, loss = 19663742915882939214051563887853225322112593578224667116145152368385808261011626745089659363496354161623040.00000000\n",
      "Iteration 14, loss = 19662636569869331463070428857398943716307322167838497495971406663804878750599257639587341535390054653886464.00000000\n",
      "Iteration 15, loss = 19661534904517842376078226700973257948331337792017315721796505708082748419528546630461255370674813188177920.00000000\n",
      "Iteration 16, loss = 19660433300890833000458352920842442272113283837108648126173953658981388725760663328745205393774262353920000.00000000\n",
      "Iteration 17, loss = 19659331758984901486130328925242428383353516641583538756886328402962237314745185917668586832389075877494784.00000000\n",
      "Iteration 18, loss = 19658230278796552279358764736049179629250725712517623294170099193419202569471022278921005800731443049005056.00000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "\n",
    "pred = pd.DataFrame({'c':test2_c,'target':test2_c})\n",
    "\n",
    "for c in range(clusters):\n",
    "    xc_train=x_train[train_c==c]\n",
    "    yc_train=y_train[train_c==c]\n",
    "    xc_test=x_test[test2_c==c]\n",
    "    \n",
    "    \n",
    "    model = MLPRegressor(hidden_layer_sizes=[200],\n",
    "                      learning_rate_init=.1,\n",
    "                      max_iter=200,\n",
    "                      learning_rate='adaptive',\n",
    "                      solver='sgd',\n",
    "                      activation='relu',\n",
    "                      verbose=True)\n",
    "    model.fit(xc_train, yc_train)\n",
    "\n",
    "    \n",
    "\n",
    "    predc = model.predict(x_test)\n",
    "    predx=predc[:,0]\n",
    "    predy=predc[:,1]\n",
    "    \n",
    "    target = [within_measure(x, y) for x,y in zip(predx,predy)]\n",
    "    \n",
    "    def add(row,c):\n",
    "        if row['c']==c:\n",
    "            return target.pop(0)\n",
    "        else:\n",
    "            return row.target\n",
    "        \n",
    "    pred['target'] = pred.apply(lambda x: add(x,c), axis=1)\n",
    "    \n",
    "    \n",
    "pred[:20]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = test2['x_entry'].values \n",
    "Y = test2['y_entry'].values\n",
    "\n",
    "city1 = [within_measure(x, y) for x,y in zip(X,Y)]\n",
    "city2 = pred.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(16)\n",
    "\n",
    "\n",
    "\n",
    "leg=['Outside','City']\n",
    "# Plot\n",
    "for data in [test2[city2==0],test2[city2==1]]:\n",
    "    ax.scatter(data.x_entry.values, data.y_entry.values,s=.05, alpha=0.5,label=leg.pop(0))\n",
    "    \n",
    "# Create a Rectangle patch\n",
    "rect = patches.Rectangle((3750901.5068,-19268905.6133),3770901.5068-3750901.5068,19268905.6133-19208905.6133,linewidth=2,edgecolor='y',facecolor='none')\n",
    "\n",
    "# Add the patch to the Axes\n",
    "ax.add_patch(rect)    \n",
    "\n",
    "ax.set(xlabel='x', ylabel='y',\n",
    "       title='LGBM on each cluster')\n",
    "ax.legend(loc=\"upper left\", markerscale=20, scatterpoints=1, fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission= pd.DataFrame()\n",
    "trajectory_id = test2['trajectory_id']\n",
    "submission['id']=trajectory_id \n",
    "submission['target'] = city2\n",
    "submission.to_csv('./output/submission_cluster.csv',index=False)    \n",
    "submission[:20]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
